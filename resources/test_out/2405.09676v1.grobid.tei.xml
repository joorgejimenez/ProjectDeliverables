<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The radius of statistical efficiency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-05-15">15 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,149.04,148.54,71.28,10.86;1,220.32,146.66,1.41,7.97"><forename type="first">Joshua</forename><surname>Cutler</surname></persName>
							<email>jocutler@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">U. Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.20,148.54,59.46,10.86;1,315.60,146.66,1.88,7.97"><forename type="first">Mateo</forename><surname>Díaz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Applied Mathematics and Statistics</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.24,148.54,111.45,10.86;1,462.60,146.66,1.88,7.97"><forename type="first">Dmitriy</forename><surname>Drusvyatskiy</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">U. Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The radius of statistical efficiency</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-15">15 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">A671F74FDF752617EE86B31A1224E5C9</idno>
					<idno type="arXiv">arXiv:2405.09676v1[math.ST]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-05-19T14:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classical results in asymptotic statistics show that the Fisher information matrix controls the difficulty of estimating a statistical model from observed data. In this work, we introduce a companion measure of robustness of an estimation problem: the radius of statistical efficiency (RSE) is the size of the smallest perturbation to the problem data that renders the Fisher information matrix singular. We compute RSE up to numerical constants for a variety of test bed problems, including principal component analysis, generalized linear models, phase retrieval, bilinear sensing, and matrix completion. In all cases, the RSE quantifies the compatibility between the covariance of the population data and the latent model parameter. Interestingly, we observe a precise reciprocal relationship between RSE and the intrinsic complexity/sensitivity of the problem instance, paralleling the classical Eckart-Young theorem in numerical analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A central theme in computational mathematics is that the numerical difficulty of solving a given problem is closely linked to both (i) the sensitivity of its solution to perturbations and (ii) the shortest distance of the problem to an ill-posed instance. As a rudimentary example, consider solving an m × d linear system Ax = b. The celebrated Eckart-Young theorem asserts the equality: min</p><formula xml:id="formula_0" coords="1,169.80,443.40,365.51,35.11">B∈R m×d { A − B F | B is singular} distance to ill−posedness = σ min (A) difficulty/sensitivity . (<label>1</label></formula><formula xml:id="formula_1" coords="1,535.31,443.40,4.61,12.21">)</formula><p>Although the proof is elementary, the conclusion is intriguing since it equates two conceptually distinct quantities. Namely, the reciprocal of the minimal singular value 1/σ min (A) is classically known to control both the numerical difficulty of solving the linear system Ax = b and the Lipschitz stability of the solution to perturbations in b. In contrast, the left side of the equation ( <ref type="formula" coords="1,518.39,523.68,4.64,12.21" target="#formula_0">1</ref>) is geometric; it measures the smallest perturbation to the data that renders the problem ill-posed. The exact equality in <ref type="bibr" coords="1,195.74,550.80,13.95,12.21" target="#b0">(1)</ref> is somewhat misleading because it is specific to linear systems. We would expect that for more sophisticated problems, the difficulty/sensitivity of the problem should be inversely proportional to the distance to ill-posedness. This is indeed the case for a wide class of problems in numerical analysis <ref type="bibr" coords="1,223.34,591.48,17.05,12.21" target="#b12">[13,</ref><ref type="bibr" coords="1,244.20,591.48,13.95,12.21" target="#b28">29]</ref> and optimization <ref type="bibr" coords="1,347.18,591.48,17.05,12.21" target="#b30">[31,</ref><ref type="bibr" coords="1,368.04,591.48,13.95,12.21" target="#b35">36,</ref><ref type="bibr" coords="1,385.80,591.48,13.95,12.21" target="#b48">49,</ref><ref type="bibr" coords="1,403.56,591.48,13.95,12.21" target="#b54">55,</ref><ref type="bibr" coords="1,421.32,591.48,12.72,12.21" target="#b55">56]</ref>, including computing eigenvalues and eigenvectors, finding zeros of polynomials, pole assignment in control systems, conic optimization, nonlinear programming, and variational inequalities. Despite this impressive body of work, this line of research is largely unexplored in statistical contexts. Therefore, here we ask:</p><p>Is there a succinct relationship between complexity, sensitivity, and distance to ill-posedness for problems in statistical inference and learning?</p><p>We will see that in a certain precise sense the answer is indeed yes for a wide class of problems. The starting point for our development is that the statistical difficulty of estimation is tightly controlled by (quantities akin to) the Fisher information matrix for maximum likelihood estimation. This connection is made precise for example by the Cramér-Rao lower bound <ref type="bibr" coords="2,440.90,162.48,16.93,12.21" target="#b18">[19,</ref><ref type="bibr" coords="2,461.16,162.48,13.95,12.21" target="#b52">53]</ref> and the local asymptotic minimax theory of Hájek and Le Cam <ref type="bibr" coords="2,319.34,176.04,17.06,12.21" target="#b34">[35,</ref><ref type="bibr" coords="2,340.68,176.04,13.95,12.21" target="#b38">39,</ref><ref type="bibr" coords="2,358.92,176.04,12.72,12.21" target="#b64">65]</ref>. From an optimization viewpoint, the minimal eigenvalue of the Fisher information matrix is closely related to the quadratic growth constant of the objective function modeling the learning problem at hand. In particular, (near-) singularity of this matrix signifies that the problem is ill-conditioned. Inspired by this observation, we introduce a new measure of robustness associated to an estimation task: the radius of statistical efficiency (RSE) is the size of the smallest perturbation to the problem data that renders the Fisher information matrix singular. Thus large RSE signifies existence of a large neighborhood around the problem instance comprised only of well-posed problems.</p><p>We compute RSE for a variety of test bed problems, including principal component analysis (PCA), generalized linear models, phase retrieval, bilinear sensing, and rank-one matrix completion. In all cases, the RSE exhibits a precise reciprocal relationship with the statistical difficulty of solving the target problem, thereby directly paralleling the Eckart-Young theorem and its numerous extensions in numerical analysis and optimization. Moreover, we provide gradient-based conditions for general estimation problems which ensure validity of such a reciprocal relationship. Slope-based criteria for error bounds, due to Ioffe <ref type="bibr" coords="2,248.43,365.64,16.92,12.21" target="#b36">[37]</ref> and Azé-Corvellec <ref type="bibr" coords="2,357.35,365.64,10.92,12.21" target="#b1">[2]</ref>, play a key role in this development.</p><p>Before delving into the technical details, we illustrate the main thread of our work with two examples-linear regression and PCA-where the conclusions are appealingly simple to state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear regression.</head><p>The problem of linear regression is to recover a vector β ⋆ ∈ R d from noisy linear measurements y = x, β ⋆ + ε, where x ∈ R d is drawn from a probability distribution D and ε is zero-mean noise vector that is independent of x. The standard approach to this task is to minimize the mean-squared error min β∈R d 1 2 E x,y ( x, β − y) 2 .</p><p>(</p><formula xml:id="formula_2" coords="2,530.69,498.36,9.23,12.21">)<label>2</label></formula><p>Classical results show that the asymptotic performance of estimators for this problem is tightly controlled by Σ −1 where Σ := E x∼D xx ⊤ is the second moment matrix of the population. The closer the matrix Σ is to being singular, the more challenging the problem (2) is to solve, requiring a higher number of samples. Seeking to estimate a neighborhood of well-posed problems around D, the RSE is defined to be the minimal Wasserstein-2 distance from D to distributions with a singular second moment matrix. We will see that for linear regression <ref type="bibr" coords="2,367.20,590.16,12.81,12.21" target="#b1">(2)</ref>, RSE is simple to compute:</p><formula xml:id="formula_3" coords="2,253.71,618.11,286.21,13.15">RSE(D) = λ min (Σ).<label>(3)</label></formula><p>That is, the simplest measure of ill-conditioning of the target problem 1/ λ min (Σ) has a geometric interpretation as the reciprocal of the distance to the nearest ill-posed problem. The representation of RSE in <ref type="bibr" coords="2,119.54,672.24,13.94,12.21" target="#b2">(3)</ref> is not specific to linear regression and holds much more generally for (quasi) maximum likelihood estimation <ref type="bibr" coords="2,175.10,685.68,16.94,12.21" target="#b45">[46]</ref> with strongly convex cumulant functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Principal component analysis (PCA).</head><p>Principal component analysis (PCA) seeks to find a q-dimensional subspace V ⊂ R d that captures most of the variance of a centered random vector x drawn from a probability distribution D. Analytically, this amounts to solving the problem max R∈Gr(q,d)</p><formula xml:id="formula_4" coords="3,303.00,118.10,52.59,19.73">E x∼D Rx 2 2 ,</formula><p>where the Grassmannian manifold Gr(q, d) consists of all orthogonal projection matrices R ∈ R d×d onto q-dimensional subspaces of R d . The column space of the optimal matrix R is called the top q principal subspace. Intuitively, the hardness of the problem is governed by the gap λ q − λ q+1 between the q'th and (q + 1)'th eigenvalues of Σ = E D xx ⊤ : the smaller the gap is, the higher the number of samples required for estimation, and this can indeed be made rigorous. Again, RSE by definition is the minimal Wasserstein-2 distance from D to a distribution with covariance having equal q'th and (q + 1)'st eigenvalues. We will show that RSE admits the simple form</p><formula xml:id="formula_5" coords="3,213.87,255.23,321.44,20.77">RSE(D) = 1 √ 2 λ q (Σ) − λ q+1 (Σ) . (<label>4</label></formula><formula xml:id="formula_6" coords="3,535.31,256.08,4.61,12.21">)</formula><p>In particular, the expression (4) endows the gap λ q (Σ) − λ q+1 (Σ)-the reciprocal of the problem's complexity-with a geometric meaning as the distance to a nearest ill-conditioned problem instance.</p><p>The two examples of linear regression and PCA can be understood within the broader context of stochastic optimization:</p><formula xml:id="formula_7" coords="3,213.00,365.40,185.91,18.67">min β∈M f (β) where f (β) = E z∼D ℓ(β; z).</formula><p>Here z is data drawn from a distribution D, the function ℓ(β; •) is a loss parameterized by β, and M ⊂ R d is a smooth manifold of allowable model parameters. For example, SP(D) may correspond to least-squares regression or maximum likelihood estimation. In both cases, the asymptotic efficiency of estimators for finding the minimizer β ⋆ of SP(D) is tightly controlled by the following matrix akin to Fisher information:</p><formula xml:id="formula_8" coords="3,226.92,464.30,81.03,20.89">I(β ⋆ , D) = P T ∇ 2</formula><p>ββ L(β ⋆ , z, λ ⋆ )P T . Here, P T is the projection onto the tangent space of M at β ⋆ and L(β ⋆ , z, λ ⋆ ) is the Lagrangian function for SP(D) with optimal multiplier λ ⋆ . For simplicity, we will abuse notation and call I(β ⋆ , D) the Fisher information matrix. The matrix I(β ⋆ , D) plays a central role in estimation, as highlighted by the lower bounds of Cramér-Rao and Hájek-Le Cam <ref type="bibr" coords="3,422.27,524.40,16.96,12.21" target="#b38">[39,</ref><ref type="bibr" coords="3,443.88,524.40,12.72,12.21" target="#b64">65]</ref>, as well as their recent extensions to stochastic optimization of Duchi-Ruan <ref type="bibr" coords="3,361.93,537.96,15.96,12.21" target="#b34">[35]</ref>. Moreover, the minimal nonzero eigenvalue of I(β ⋆ , D) controls both the coefficient of quadratic growth of the objective function in SP(D) and the Lipschitz stability of the solution under linear perturbations. In particular, the problem SP(D) becomes ill-conditioned when the minimal eigenvalue of I(β ⋆ , D) is small.</p><p>In summary, the matrix I(β ⋆ , D) tightly controls the difficulty of solving SP(D). Consequently, it is appealing to consider as a measure of robustness of SP(D) the size of the smallest perturbation to the data D, say in the Wasserstein-2 distance W 2 (•, •), that renders the Fisher information matrix singular. This is the viewpoint we explore in the current work, and call this quantity the radius of statistical efficiency (RSE). We choose to use the Wasserstein-2 distance in the definition of RSE, as opposed to other metrics on measures, because it leads to concise and easily interpretable estimates in examples. In the rest of the paper, we study basic properties of RSE and compute it up to numerical constants for a variety of test bed problems: generalized linear models, PCA, phase retrieval, blind deconvolution, and matrix completion. In all cases, the RSE translates intuitive measures of "well-posedness" into quantified neighborhoods of stable problems. Moreover, in all cases we show a reciprocal relationship between the minimal eigenvalue of I(β ⋆ , D) and RSE, thereby paralleling the Eckart-Young theorem in numerical analysis and optimization.</p><p>Outline The remainder of this section covers related work and basic notation we use. Section 2 formally describes the radius of statistical efficiency and establishes a few general-purpose results relating RSE to the minimal eigenvalue of the Fisher information matrix. The subsequent sections characterize RSE for several problems: PCA (Section 3), generalized linear models and (quasi) maximum likelihood estimation (Section 4), rank-one matrix regression (Section 5). Section 6 closes the paper with conclusions. All proofs appear in the appendix in order to streamline the reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Our work is closely related to a number of topics in statistics and computational mathematics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local minimax lower bounds in estimation.</head><p>There is a rich literature on minimax lower bounds in statistical estimation problems; we refer the reader to <ref type="bibr" coords="4,391.45,302.88,17.07,12.21" target="#b67">[68,</ref><ref type="bibr" coords="4,413.16,302.88,57.16,12.21">Chapter 15]</ref> for a detailed treatment. Typical results of this type lower-bound the performance of any statistical procedure on a worst-case instance for that procedure. Minimax lower bounds can be quite loose as they do not consider the complexity of the particular problem that one is trying to solve but rather that of an entire problem class to which it belongs. More precise local minimax lower bounds, as developed by Hájek and Le Cam <ref type="bibr" coords="4,167.40,370.56,16.95,12.21" target="#b38">[39,</ref><ref type="bibr" coords="4,188.40,370.56,12.63,12.21" target="#b64">65]</ref>, provide much finer problem-specific guarantees. Simply put, a single object akin to the Fisher information matrix controls both the difficulty of estimation from finitely many samples and the stability of the model parameters to perturbation of the density. Extensions of this theory to stochastic nonlinear programming were developed by Duchi and Ruan <ref type="bibr" coords="4,501.16,411.24,16.92,12.21" target="#b34">[35]</ref> and extended to decision-dependent problems in <ref type="bibr" coords="4,281.06,424.80,16.93,12.21" target="#b20">[21]</ref> and to a wider class of (partly smooth) problems in <ref type="bibr" coords="4,72.00,438.36,15.96,12.21" target="#b25">[26]</ref>. In particular, it is known that popular algorithms such as sample average approximation <ref type="bibr" coords="4,523.09,438.36,16.94,12.21" target="#b64">[65]</ref> and stochastic gradient descent with iterate averaging <ref type="bibr" coords="4,329.54,451.92,16.94,12.21" target="#b25">[26,</ref><ref type="bibr" coords="4,349.56,451.92,13.95,12.21" target="#b50">51]</ref> match asymptotic local lower-bound, and are therefore asymptotically optimal. Weaker ad hoc results, based on the Cramér-Rao lower bound, have been established for a handful of problems <ref type="bibr" coords="4,340.15,479.04,11.48,12.21" target="#b2">[3,</ref><ref type="bibr" coords="4,355.32,479.04,8.43,12.21" target="#b3">4,</ref><ref type="bibr" coords="4,367.44,479.04,13.95,12.21" target="#b43">44,</ref><ref type="bibr" coords="4,384.96,479.04,13.95,12.21" target="#b47">48,</ref><ref type="bibr" coords="4,402.60,479.04,12.63,12.21" target="#b61">62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Radius theorems.</head><p>Classical numerical analysis literature emphasizes the close interplay between efficiency of numerical algorithms and their sensitivity to perturbation. Namely, problems with solutions that change rapidly due to small perturbation are typically difficult to solve. Examples of this phenomenon abound in computational mathematics; e.g. eigenvalue problems and polynomial equations <ref type="bibr" coords="4,121.69,561.96,16.95,12.21" target="#b12">[13,</ref><ref type="bibr" coords="4,143.16,561.96,13.95,12.21" target="#b28">29]</ref> and optimization <ref type="bibr" coords="4,248.18,561.96,16.93,12.21" target="#b30">[31,</ref><ref type="bibr" coords="4,269.64,561.96,13.95,12.21" target="#b31">32,</ref><ref type="bibr" coords="4,288.12,561.96,12.72,12.21" target="#b56">57]</ref>. Motivated by this observation, Demmel in <ref type="bibr" coords="4,523.11,561.96,16.92,12.21" target="#b27">[28]</ref> introduced a new robustness measure, called the radius of regularity, which measures the size of a neighborhood around a problem instance within which all other problem instances are stable. A larger neighborhood thereby signifies a more robust problem instance. Estimates on the radius of regularity have now been computed for a wealth of computational problems; e.g. solving polynomial systems <ref type="bibr" coords="4,111.46,629.76,17.09,12.21" target="#b11">[12,</ref><ref type="bibr" coords="4,131.16,629.76,12.72,12.21" target="#b12">13]</ref>, linear and conic programming <ref type="bibr" coords="4,294.03,629.76,16.92,12.21" target="#b35">[36,</ref><ref type="bibr" coords="4,313.68,629.76,13.95,12.21" target="#b48">49,</ref><ref type="bibr" coords="4,330.24,629.76,13.95,12.21" target="#b54">55,</ref><ref type="bibr" coords="4,346.92,629.76,12.72,12.21" target="#b55">56]</ref>, and nonlinear optimization <ref type="bibr" coords="4,496.72,629.76,15.93,12.21" target="#b30">[31]</ref>. The radius of statistical efficiency, introduced here, serves as a direct analogue for statistical estimation.</p><p>Conditioning and radius theorems in recovery problems. Several condition numberscontrolling the convergence of first-order methods-are closely related to notions of strong identifiability, e.g., the restricted isometry property (RIP), in the context of statistical recovery problems <ref type="bibr" coords="5,72.00,73.44,11.55,12.21" target="#b7">[8,</ref><ref type="bibr" coords="5,86.64,73.44,13.95,12.21" target="#b13">14,</ref><ref type="bibr" coords="5,103.68,73.44,13.95,12.21" target="#b14">15,</ref><ref type="bibr" coords="5,120.72,73.44,13.95,12.21" target="#b15">16,</ref><ref type="bibr" coords="5,137.76,73.44,13.95,12.21" target="#b29">30,</ref><ref type="bibr" coords="5,154.80,73.44,12.72,12.21" target="#b44">45]</ref>. A few works <ref type="bibr" coords="5,237.48,73.44,11.43,12.21" target="#b4">[5,</ref><ref type="bibr" coords="5,252.12,73.44,13.83,12.21" target="#b57">58,</ref><ref type="bibr" coords="5,269.16,73.44,13.95,12.21" target="#b70">71]</ref> have established connections between these notions of strong identifiability and a suitably-defined radius to ill-posed instances. In particular, <ref type="bibr" coords="5,492.50,87.00,16.93,12.21" target="#b57">[58]</ref> established a connection between Renegar's conic distance to infeasibility <ref type="bibr" coords="5,392.77,100.56,16.94,12.21" target="#b55">[56]</ref> and the null space property <ref type="bibr" coords="5,72.00,114.12,16.95,12.21" target="#b17">[18]</ref> in compressed sensing. In a similar vein, <ref type="bibr" coords="5,291.01,114.12,17.06,12.21" target="#b70">[71]</ref> linked the ℓ 1 -distance to ill-posed problems and the RIP for generalized rank-one matrix completion. Finally, <ref type="bibr" coords="5,362.41,127.68,11.55,12.21" target="#b4">[5]</ref> defined a condition number for the LASSO variable selection problem via the reciprocal of the distance to ill-posedness, designed an algorithm whose complexity depends solely on this condition number, and proved an impossibility result for instances with infinite condition number. The radius of statistical efficiency, defined in this work, is distinct from and complementary to these metrics.</p><p>Error bounds. The basic question we explore is the relationship between the minimal eigenvalue of the Fisher information matrix (complexity) and the distance to the set where this eigenvalue is zero (RSE). The theory of error bounds exactly addresses questions of this type; namely when does the function's value polynomially bound the distance to the set of minimizers. See for example the authoritative monographs on the subject, <ref type="bibr" coords="5,275.32,264.84,16.91,12.21" target="#b19">[20,</ref><ref type="bibr" coords="5,296.28,264.84,51.28,12.21">Chapter 8]</ref> and <ref type="bibr" coords="5,373.22,264.84,16.93,12.21" target="#b37">[38,</ref><ref type="bibr" coords="5,394.20,264.84,50.47,12.21">Chapter 3]</ref>. Indeed, Demmel's original work <ref type="bibr" coords="5,138.50,278.40,16.93,12.21" target="#b27">[28]</ref> makes heavy use of this interpretation. We explore this path here as well when developing infinitesimal characterizations of RSE in Theorem 2.2. The added complication is that the functions we deal with are defined over a complete metric space, and therefore the techniques we use rely on variational principles (á la Ekeland) and computations of the slope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Notation</head><p>Linear algebra. Throughout, we let R d denote the standard d-dimensional Euclidean space with dot product x, y = x ⊤ y and norm x 2 =</p><p>x, x . The unit sphere in R d will be denoted by S d−1 , while the set of nonnegative vectors will be written as R d + . The symbol R m×n will denote the Euclidean space of real m × n matrices, endowed with the trace inner product X, Y = tr(X ⊤ Y ). The symbol ⊗ denotes Kronecker product. The Frobenius and operator norms will be written as</p><p>• F and • op , respectively. The singular values of a matrix A ∈ R m×n will be arranged in nonincreasing order:</p><formula xml:id="formula_9" coords="5,227.88,465.72,156.15,19.95">σ 1 (A) ≥ σ 2 (A) ≥ . . . ≥ σ m∧n (A).</formula><p>The space of real symmetric d × d matrices is denoted by S d and is equipped with the trace product as well. The cone of d × d positive semidefinite matrices will be written as S d + . The eigenvalues of a matrix A ∈ S d will be arranged in nonincreasing order:</p><formula xml:id="formula_10" coords="5,234.60,526.92,142.71,19.95">λ 1 (A) ≥ λ 2 (A) ≥ . . . ≥ λ d (A).</formula><p>For any subspace K ⊂ R d , the symbol P K : R d → K will denote the orthogonal projection onto K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The compression of any matrix</head><formula xml:id="formula_11" coords="5,223.92,558.50,315.87,20.41">A ∈ R d×d to K, denoted A| K : K → K, is the map A| K := P K AP K .</formula><p>Probability theory. We will require some background on the Wasserstein geometry on the space of probability measures on R d . In order to streamline the reading, we record here only the most essential notation that we will need. A detailed review of Wasserstein geometry appears in Section A. To this end, we let P p (R d ) be the space of measures µ on R d with a finite p'th moment E µ x p p &lt; ∞. The subset of measures of P p (R d ) that are centered, meaning E µ [x] = 0, will be written as P • p (R d ). When the space R d is clear from context, we use the shorthand P p and P • p . A convenient metric on P p is furnished by the Wasserstein distance W p (µ, ν); see Section A for details. The distance function to a set of measures Q ⊂ P p is defined by W p (µ, Q) = inf ν∈Q W p (µ, ν). The symbol Σ µ = E µ xx ⊤ will denote the second moment matrix of any measure µ ∈ P 2 . In the rest of the paper, we will use the symbol D to denote a distinguished measure associated with the problem of interest, while we use µ as a placeholder for arbitrary measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The distance to ill-conditioned problems</head><p>In this section, we formally define the radius of statistical efficiency (RSE), and develop some techniques for computing it. Throughout, we will focus on the stochastic optimization problem</p><formula xml:id="formula_12" coords="6,213.00,177.84,185.91,18.67">min β∈M f (β) where f (β) = E z∼D ℓ(β; z).</formula><p>Here, the set M ⊂ R d is a C 2 manifold and z is drawn from a distribution D ∈ P 2 (Z), where Z is a finite-dimensional Euclidean space equipped with its Borel σ-algebra. We assume that the function ℓ(β; z) is measurable and twice differentiable in β for every z and that f is C 2 -smooth. We also make the blanket assumption that the gradient and Hessian of ℓ(•; z) are D-integrable.</p><p>The difficulty of solving the problem SP(D) from finitely many samples z 1 , z 2 , . . . , z n iid ∼ D is tightly controlled by a matrix akin to Fisher Information. This object, which we now describe, plays a central role in our work. Let β ⋆ be a minimizer of SP(D) and define the solution map:</p><formula xml:id="formula_13" coords="6,227.40,307.44,156.99,21.19">σ(v) = argmin β∈M∩Bε(β⋆) f (β) − v, β .</formula><p>Thus, the set σ(v) is comprised of all solutions to a problem obtained from SP(D) by subtracting a linear/tilt perturbation v, β . Clearly, a desirable property is for σ to be single-valued and smooth. With this in mind, we introduce the following notion due to Poliquin and Rockafellar <ref type="bibr" coords="6,483.39,361.44,15.94,12.21" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition: (Tilt-stable minimizer)</head><p>The point β ⋆ is a tilt-stable minimizer of SP(D) if the map σ(•) satisfies σ(0) = β ⋆ and is single-valued and C 1 -smooth on some neighborhood of the origin. Then the regularity modulus of the problem is defined to be REG(D) = ∇σ(0) op .</p><p>If β ⋆ is not tilt-stable, we call β ⋆ unstable and set REG(D) = +∞.</p><p>In particular, we will regard REG(D) as the measure of difficulty of solving the problem SP(D). When M is the whole space, β ⋆ is a tilt-stable minimizer if and only if the Hessian ∇ 2 f (β ⋆ ) is nonsingular, in which case equality ∇σ(0) = [∇ 2 f (β ⋆ )] −1 holds [50, Proposition 1.2]. In particular, when SP(D) corresponds to maximum likelihood estimation, the matrix ∇σ(0) reduces to the inverse of the Fisher information. More generally, tilt-stability can be characterized either in terms of definiteness of the covariant Hessian or the Hessian of the Lagrangian on the tangent space to M. Since we will use both of these viewpoints, we review them now. The reader may safely skip this discussion during the first reading since it will not be used until the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lagrangian characterization.</head><p>Let G = 0 be the local defining equations for M around β ⋆ . That is G : R d → R m is a C 2 -smooth map with surjective Jacobian ∇G(β ⋆ ) and such that the two sets M and {β : G(β) = 0} coincide near β ⋆ . Then the tangent space to M at β ⋆ is T = Null(∇G(β ⋆ )). Define the Lagrangian function</p><formula xml:id="formula_14" coords="6,239.40,675.84,132.87,19.95">L(β, λ) := f (β) + λ, G(β) .</formula><p>First order optimality conditions at β ⋆ ensure that there exists a unique vector λ ⋆ ∈ R m satisfying ∇ β L(β ⋆ , λ ⋆ ) = 0. Define the matrix</p><formula xml:id="formula_15" coords="7,222.72,104.54,312.59,21.01">I(β ⋆ , D) := P T • ∇ 2 ββ L(β ⋆ , λ ⋆ ) • P T , (<label>5</label></formula><formula xml:id="formula_16" coords="7,535.31,105.60,4.61,12.21">)</formula><p>where P T is the orthogonal projection onto the tangent space T . If β ⋆ is a local minimizer of the problem, then I(β ⋆ , D) is positive semidefinite. Conversely:</p><formula xml:id="formula_17" coords="7,127.08,160.20,359.91,19.95">I(β ⋆ , D) is positive definite on T if and only if β ⋆ is a tilt-stable minimizer.</formula><p>Moreover, in this case equality ∇σ(0) = I(β ⋆ , D) † holds, where † denotes the Moore-Penrose inverse.</p><p>In particular, one may regard ∇σ(0) as akin to the inverse of the Fisher information matrix for MLE. Note that when β ⋆ is tilt-stable, the reciprocal of the regularity modulus 1/REG(D) coincides the minimal nonzero eigenvalue of I(β ⋆ , D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intrinsic characterization.</head><p>Often, the defining equations of the manifold are either unknown or difficult to work with. In this case, tilt-stability can be characterized through second-order expansions along curves. Namely, for any tangent vector u ∈ T , there exists a C 2 -smooth curve γ u : (−ǫ, ǫ) → M for some ǫ &gt; 0 satisfying γ u (0) = β ⋆ and γu (0) = u. The covariant Hessian of f at β ⋆ is the unique symmetric bilinear form</p><formula xml:id="formula_18" coords="7,72.00,305.90,370.86,75.97">∇ 2 M f (β ⋆ ) : T × T → R satisfying ∇ 2 M f (β ⋆ )[u, u] = (f • γ u ) ′′ (0). It is classically known that equality holds ∇ 2 M f (β ⋆ )[u, u] = u ⊤ • I(β ⋆ , D) • u ∀u ∈ T ,</formula><p>where the matrix I(β ⋆ , D) is defined in <ref type="bibr" coords="7,261.98,380.40,12.79,12.21" target="#b4">(5)</ref>. Consequently, ∇ 2 M f (β ⋆ ) is positive semidefinite when β ⋆ is a local minimizer. Conversely:</p><formula xml:id="formula_19" coords="7,123.72,416.06,40.83,20.67">∇ 2 M f (β ⋆</formula><p>) is positive definite on T if and only if β ⋆ is a tilt-stable minimizer. In this case, identifying ∇ 2 M f (β ⋆ ) with a matrix, equality ∇σ(0) = (P T ∇ 2 M f (β ⋆ )P T ) † holds. In particular, the equality REG(D) −1 = λ min (∇ 2 M f (β ⋆ )) holds.</p><p>The sensitivity matrix ∇σ(0) figures prominently in the asymptotic performance of estimation procedures. Notably, building on classical ideas due to Hájek and Le Cam, the recent paper of Duchi and Ruan <ref type="bibr" coords="7,155.31,505.20,16.93,12.21" target="#b34">[35]</ref> established a lower bound on asymptotic covariance of arbitrary estimators β n of β ⋆ using n samples z 1 . . . , z n . The precise lower bound is quite technical, and we refer the interested reader to their paper. In summary, their result shows that if β ⋆ is a tilt-stable minimizer, then the asymptotic covariance of √ n( β n −β ⋆ ) is lower bounded in the Loewner order by the matrix</p><formula xml:id="formula_20" coords="7,217.92,564.36,322.00,19.95">Σ := ∇σ(0) • Cov (∇ℓ(β ⋆ , z)) • ∇σ(0),<label>(6)</label></formula><p>Moreover, in typical settings the expression in (6) simplifies to the equality Σ = ∇σ(0); this is the case for example for (quasi) maximum likelihood estimation (Section 4) and rank-one matrix regression problems (Section 5). Thus, asymptotically the best error that any estimator can achieve in the direction u is on the order n −1/2 • u ⊤ Σu. The direction u with the worst error matches the top eigenvector of Σ and the number of samples necessary to find an accurate approximation of β ⋆ grows with λ max (Σ). Reassuringly, typical algorithms such as sample average approximation <ref type="bibr" coords="7,72.00,664.20,16.95,12.21" target="#b64">[65]</ref> and the stochastic projected gradient method <ref type="bibr" coords="7,312.02,664.20,16.93,12.21" target="#b25">[26,</ref><ref type="bibr" coords="7,332.40,664.20,13.95,12.21" target="#b50">51]</ref> match the lower-bound <ref type="bibr" coords="7,463.22,664.20,13.95,12.21" target="#b5">(6)</ref> and thus are asymptotically optimal.</p><p>In the rest of the paper, we analyze data distributions D ′ , nearest to a fixed measure D, for which the problem SP(D ′ ) admits unstable minimizers. The formal definition will depend on whether the learning problem is of supervised or unsupervised type. We now describe these two settings in turn. The goal of unsupervised learning is to learn some property of a distribution D from finitely many samples z 1 , . . . , z n iid ∼ D. Dimension reduction with principal component analysis (PCA) is a primary example. In this case, the solution β ⋆ of SP(D) strongly depends on the distribution D. Therefore a natural measure of robustness of the problem is the size of the smallest perturbation in the Wasserstein-2 distance W 2 (D ′ , D) so that the problem SP(D ′ ) has an unstable minimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition: Radius of statistical efficiency (unsupervised)</head><p>Consider the problem SP(D) and let Q ⊂ P 2 (Z) be a distinguished set of distributions. Define the set of ill-conditioned distributions as</p><formula xml:id="formula_21" coords="8,143.55,216.61,324.87,21.01">E = D ′ ∈ Q | There exists a minimizer of SP(D ′ ) that is unstable .</formula><p>The radius of statistical efficiency of D is defined to be</p><formula xml:id="formula_22" coords="8,255.27,254.63,101.31,19.95">RSE(D) := W 2 (D, E).</formula><p>Problems of supervised learning are distinctly different. The data consists of pairs z = (x, y) ∼ D, where x ∼ D x are called feature vectors and y ∼ D y|x are the labels. We will assume that the conditional distribution D y|x depends on the features x and a latent parameter β ⋆ . A typical example is the setting of regression under a model y = g(x, β ⋆ ) + ε where ε is a noise vector that is independent of x. The goal of the corresponding optimization problem SP(D) is to recover β ⋆ . In contrast to unsupervised learning, the latent parameter β ⋆ is fixed a priori and is not a function of the data distribution. Therefore a natural measure of robustness of the problem is the size of the smallest perturbation to the feature vectors W 2 (D ′</p><p>x , D x ) so that the problem SP(D ′ x × D y|x ) has an unstable minimizer. Notice that the conditional distributions of y given x coincide for the two measures D and D ′ = D ′</p><p>x × D y|x . This type of shift exclusively only in the feature data appears often in the literature under the name of covariate shift <ref type="bibr" coords="8,340.92,415.32,11.55,12.21" target="#b0">[1,</ref><ref type="bibr" coords="8,356.16,415.32,13.83,12.21" target="#b51">52,</ref><ref type="bibr" coords="8,373.68,415.32,13.95,12.21" target="#b53">54,</ref><ref type="bibr" coords="8,391.20,415.32,13.95,12.21" target="#b60">61,</ref><ref type="bibr" coords="8,408.84,415.32,12.72,12.21" target="#b68">69]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition: Radius of statistical efficiency (supervised)</head><p>Consider the problem SP(D) with D = D x × D y|x and let β ⋆ be its minimizer. Let Q ⊂ P 2 be a distinguished set of distributions. Define the set of ill-conditioned distributions as</p><formula xml:id="formula_23" coords="8,86.31,489.97,379.35,59.29">E = D ′ x ∈ Q | β ⋆ is not a tilt-stable minimizer of SP(D ′ x × D y|x ) . The radius of statistical efficiency of D is defined to be RSE(D) := W 2 (D x , E).</formula><p>Evidently, the quantity RSE(D) measures the robustness of the problem because it quantifies the size of a neighborhood around D for which all problem instances are stable. There is a small nuance in formalizing this statement due to a lack of compactness in the Wasserstein space. Namely, we have to impose the minor assumption that any sequence of measures ν i for which the problem SP(ν i ) becomes progressively harder (REG(ν i ) → ∞) must approach the set of ill-conditioned distributions (RSE(ν i ) → 0). In all examples we consider, this holds at least on bounded sets Q ′ ⊂ P 2 . The proof of this elementary observation appears in Appendix B.1. Proposition 2.1 (RSE as a robustness measure). Fix a set Q ′ ⊂ Q and suppose that for any sequence of measures ν i ∈ Q ′ \ E the implication holds:</p><formula xml:id="formula_24" coords="8,205.20,690.24,330.11,19.95">REG(ν i ) → ∞ =⇒ RSE(ν i ) → 0. (<label>7</label></formula><formula xml:id="formula_25" coords="8,535.31,690.24,4.61,12.21">)</formula><p>Then for any measure µ ∈ Q ′ \ E and any radius 0 &lt; r &lt; RSE(µ), we have</p><formula xml:id="formula_26" coords="9,234.96,92.04,300.35,21.56">sup ν∈Q ′ : W 2 (ν,µ)≤r REG(ν) &lt; +∞. (<label>8</label></formula><formula xml:id="formula_27" coords="9,535.31,92.04,4.61,12.21">)</formula><p>Moreover, if for some c, q &gt; 0, the inequality RSE(ν</p><formula xml:id="formula_28" coords="9,71.16,119.42,468.73,33.85">) q ≤ c • REG(ν) −1 holds for all ν ∈ Q ′ \ E, then the supremum in (8) is upper bounded by c • (RSE(µ) − r) −1/q .</formula><p>At first sight, it appears that computing RSE(D) in concrete problems is difficult. Indeed, the set of ill-conditioned distributions E may be quite exotic and computing RSE(D) amounts to estimating the Wasserstein-2 distance W 2 (D, E). In contrast, computing the regularity modulus REG(D) should be relatively straightforward. The key observation now is that the two quantities, RSE(D) and REG(D), are closely related since E is the set of minimizers of the function J (µ) := 1/REG(µ). Thus it would be ideal if there were a quantitative relationship of the form:</p><formula xml:id="formula_29" coords="9,189.36,241.22,345.95,20.89">(W 2 (µ, E)) ℓ 1 J (µ) (W 2 (µ, E)) ℓ 2 ∀µ ∈ P 2 . (<label>9</label></formula><formula xml:id="formula_30" coords="9,535.31,242.16,4.61,12.21">)</formula><p>The upper should be elementary to establish because it amounts to upper bounding the growth of the functional J (µ). The lower bound is more substantial because it requires lower-bounding the growth of J (µ) by a nonnegative function of the distance. Such lower-estimates are called error bounds in nonlinear analysis and can be checked by various "slope"-based conditions. See for example the authoritative monographs on the subject, <ref type="bibr" coords="9,332.80,314.88,16.92,12.21" target="#b19">[20,</ref><ref type="bibr" coords="9,353.04,314.88,50.68,12.21">Chapter 8]</ref> and <ref type="bibr" coords="9,427.94,314.88,16.93,12.21" target="#b37">[38,</ref><ref type="bibr" coords="9,448.32,314.88,49.75,12.21">Chapter 3]</ref>. Indeed, Demmel's original work <ref type="bibr" coords="9,187.68,328.44,17.07,12.21" target="#b27">[28]</ref> relies on verifying an error bound property as well, albeit in the much simpler Euclidean setting. The following theorem provides a sufficient condition <ref type="bibr" coords="9,458.05,342.00,19.35,12.21" target="#b9">(10)</ref> ensuring the relationship <ref type="bibr" coords="9,130.58,355.56,12.70,12.21" target="#b8">(9)</ref>. We state the theorem loosely by compressing all multiplicative numerical constants via the symbol . More precise and sharper guarantees appear in Appendix B.2.<ref type="foot" coords="9,458.88,368.66,4.23,7.97" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2.2: (Infinitesimal characterization of RSE)</head><p>Consider the problem SP(D) of supervised learning and let β ⋆ be its minimizer. Set Q = P 2 and suppose that the Hessian ∇ 2 M f (β ⋆ ) corresponding to a measure µ ∈ P 2 can be written as</p><formula xml:id="formula_31" coords="9,86.67,442.45,440.39,39.49">∇ 2 M f (β ⋆ ) = E µ F (x), for some C 1 -smooth map F : R d → S k + satisfying DF (x) op 1 + x 2 for all x ∈ R d .</formula><p>Suppose that there exists q 1 , q 2 ∈ [0, 1) such that for all measures ν ∈ P 2 \ E, the estimate</p><formula xml:id="formula_32" coords="9,160.35,495.97,360.03,16.01">λ min (E ν F (x)) q 1 E ν DF (x) * [uu ⊤ ] 2 2 λ min (E ν F (x)) q 2 , (<label>10</label></formula><formula xml:id="formula_33" coords="9,520.38,497.51,4.81,12.21">)</formula><p>holds for some eigenvector u ∈ S d−1 of the matrix E ν F (x) corresponding to its minimal eigenvalue. Then for every µ ∈ P 2 the inequality holds:</p><formula xml:id="formula_34" coords="9,213.27,549.13,307.11,13.59">REG(µ) q 2 −1 RSE(µ) REG(µ) q 1 −1 . (<label>11</label></formula><formula xml:id="formula_35" coords="9,520.39,550.07,4.81,12.21">)</formula><p>The expression <ref type="bibr" coords="9,168.48,575.28,19.37,12.21" target="#b9">(10)</ref> becomes particularly enlightening when F (x) decomposes as F (x) = g(x)g(x) ⊤ for some C 1 -smooth map g : R d → R k . This situation is typical for regression problems (see Section 4). A simple computation then shows that the sufficient condition <ref type="bibr" coords="9,481.44,602.40,19.36,12.21" target="#b9">(10)</ref> reduces to</p><formula xml:id="formula_36" coords="9,156.84,626.42,304.23,24.97">E ν u, g(x) 2 q 1 E ν u, g(x) 2 ∇g(x)u] 2 2 E ν u, g(x) 2 q 2 ,</formula><p>Observe that all three terms would match exactly with q 1 = q 2 = 1 2 were it not for the term ∇g(x)u] 2  2 that reweighs the middle integral. It is this reweighing that may impact the values of q 1 and q 2 . The salient feature of Theorem 2.2 is that it completely circumvents the need for explicitly estimating the distance to the exceptional set E. One could apply this theorem to a number of examples that will appear in the rest of the paper. That being said, in all the upcoming examples, we will be able to compute the distance to E explicitly, thereby obtaining sharper estimates than would follow from Theorem 2.2. Nonetheless, we believe that Theorem 2.2 is interesting in its own right and may be useful for analyzing RSE in more complex situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Principal component analysis</head><p>Principal component analysis (PCA) is a common technique for dimension reduction. The goal of PCA is to find a low-dimensional subspace that captures the majority of the variance of the distribution. In this section, we compute the radius of statistical efficiency for PCA. Setting the stage, let x be a random vector in R d drawn from a zero-mean distribution D ∈ P • 2 (R d ). A unit vector v for which the random variable v, x has maximal variance is called the first principal component of D. Thus, the first principal component is the maximizer of the problem max</p><formula xml:id="formula_37" coords="10,256.92,271.70,278.07,20.57">v∈S d−1 1 2 E x∼D v, x 2 , (<label>12</label></formula><formula xml:id="formula_38" coords="10,534.99,272.64,4.81,12.21">)</formula><p>Equivalently, the first principal component is the eigenvector corresponding to the maximal eigenvalue of the covariance matrix Σ D = E D xx ⊤ . Intuitively, the problem ( <ref type="formula" coords="10,427.50,309.24,9.67,12.21" target="#formula_37">12</ref>) is more challenging when the gap between the top two eigenvalues of Σ D is small. This is the content of the following lemma, whose proof appears in Appendix C.1.</p><p>Lemma 3.1. The set of ill-conditioned distributions for <ref type="bibr" coords="10,345.00,358.92,19.36,12.21" target="#b11">(12)</ref> is given by</p><formula xml:id="formula_39" coords="10,71.40,376.46,362.27,39.49">E = {µ ∈ P • 2 : λ 1 (Σ µ ) = λ 2 (Σ µ )}. Moreover, for any µ ∈ P • 2 \ E, equality REG(µ) −1 = λ 1 (Σ µ ) − λ 2 (Σ µ ) holds.</formula><p>Therefore, estimating RSE amounts to computing the Wasserstein-2 distance of a base measure D to the set E. The end result is the following theorem; we defer its proof to Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.2: (RSE for top principal component)</head><p>Consider the problem <ref type="bibr" coords="10,190.71,475.59,19.48,12.21" target="#b11">(12)</ref> and define the covariance</p><formula xml:id="formula_40" coords="10,338.79,475.13,74.99,13.73">Σ D := E D [xx ⊤ ].</formula><p>Then, equality holds:</p><formula xml:id="formula_41" coords="10,211.59,498.17,188.79,20.77">RSE(D) = 1 √ 2 λ 1 (Σ D ) − λ 2 (Σ D ) .</formula><p>In particular, we have</p><formula xml:id="formula_42" coords="10,188.07,536.67,332.31,31.03">RSE(D) • REG(D) = 1 √ 2 λ 1 (Σ D ) + λ 2 (Σ D ) . (<label>13</label></formula><formula xml:id="formula_43" coords="10,520.38,544.11,4.81,12.21">)</formula><p>Thus treating λ 1 (Σ D ) in ( <ref type="formula" coords="10,196.73,583.08,9.68,12.21" target="#formula_42">13</ref>) as being of constant order, we see that the hardness of the problem is inversely proportional to the distance to the nearest ill-posed problem, REG(D) ∝ RSE(D) −1 .</p><p>More generally still, we may be interested in finding a q-dimensional subspace V ⊂ R d that captures most of the variance. Analytically, this amounts to solving the problem max</p><formula xml:id="formula_44" coords="10,238.32,641.30,296.67,20.09">R∈Gr(q,d) f (R) = E x∼D Rx 2 2 , (<label>14</label></formula><formula xml:id="formula_45" coords="10,534.99,642.24,4.81,12.21">)</formula><p>where the Grassmannian manifold Gr(q, d) consists of all orthogonal projections R ∈ S d onto q dimensional subspaces of R d . The column space of the optimal matrix R is called the top q principal subspace. Equivalently, the top q principal subspace is the span of the eigenspaces of Σ D corresponding to its top q eigenvalues. The following lemma is a direction extension of Lemma 3.1; see Appendix C.3 for a proof.</p><p>Lemma 3.3. The set of ill-conditioned distributions for ( <ref type="formula" coords="11,349.84,104.88,9.68,12.21" target="#formula_44">14</ref>) is given by</p><formula xml:id="formula_46" coords="11,71.40,122.30,341.68,39.49">E = {µ ∈ P • 2 : λ q (Σ µ ) = λ q+1 (Σ µ )}. Moreover, for any µ ∈ P • 2 \ E, equality REG(µ) −1 = λ q (Σ µ ) − λ q+1 (Σ µ )</formula><p>holds. Thus, estimating RSE amounts to computing the Wasserstein-2 distance of a base measure D to the set E. The end result is the following theorem; the proof appears in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.4: (RSE for PCA)</head><p>Consider the problem <ref type="bibr" coords="11,189.03,216.85,19.36,12.21" target="#b13">(14)</ref> and define the covariance</p><formula xml:id="formula_47" coords="11,334.11,216.40,68.27,13.73">Σ := E D [xx ⊤ ].</formula><p>Then, the estimate holds:</p><formula xml:id="formula_48" coords="11,212.91,239.44,186.15,20.77">RSE(D) = 1 √ 2 λ q (Σ) − λ q+1 (Σ) .</formula><p>In particular, we have</p><formula xml:id="formula_49" coords="11,182.91,277.93,337.47,31.63">RSE(D) • REG(D) = 1 √ 2 λ q (Σ D ) + λ q+1 (Σ D ) . (<label>15</label></formula><formula xml:id="formula_50" coords="11,520.38,285.25,4.81,12.21">)</formula><p>Thus similarly to the case q = 1, treating λ q (Σ D ) in <ref type="bibr" coords="11,317.77,324.96,19.47,12.21" target="#b14">(15)</ref> as being of constant order, the hardness of the problem is inversely proportional to the distance to ill-posed problems, REG(D) ∝ RSE(D) −1 . The proofs of Theorems 3.2 and 3.4 rely on estimating the distance to the exceptional sets E. Notice that these two are defined purely in terms of the spectrum of the second-moment matrix. Although such "spectral" sets in P 2 are quite complicated, their distance can be readily computed. Geometric properties of such sets are explored in Appendix A.2 and may be of independent interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generalized linear models and (quasi) maximum likelihood estimation</head><p>In this section, we compute the RSE for a large class of supervised learning problems arising from (quasi) maximum likelihood estimation (QMLE). The goal is to estimate a parameter β ⋆ ∈ M where the constraint set M is a C 2 -smooth embedded submanifold of R d . Setting the stage, suppose that we have an L 2 random vector x (the predictor) and an L 2 random variable y (the response) satisfying the GLM conditions</p><formula xml:id="formula_51" coords="11,157.56,540.14,382.25,21.01">E D y | x = h ′ ( x, β ⋆ ) and Var D y | x = σ 2 • h ′′ ( x, β ⋆ )<label>(16)</label></formula><p>for some known C 2 -smooth convex function h : R → R with h ′′ &gt; 0 and parameter σ 2 &gt; 0. The function h is called the cumulant function of the model ( <ref type="formula" coords="11,337.23,573.24,8.94,12.21" target="#formula_51">16</ref>), and σ 2 the dispersion parameter. Here and from now on, we make the blanket assumption that Q ⊂ P 2 is the space of probability measures where sufficient regularity conditions are met to take expectations and to interchange differentiation and expectation as necessary. More precisely, we assume that the function φ : M → R given by</p><formula xml:id="formula_52" coords="11,72.00,627.02,467.95,33.97">φ(β) = E[h( x, β )] is well defined and has a C 2 -smooth local extension to a neighborhood of β ⋆ in R d with ∇φ(β ⋆ ) = E[h ′ ( x, β ⋆ )x] and ∇ 2 φ(β ⋆ ) = E[h ′′ ( x, β ⋆ )xx ⊤ ].</formula><p>Following the seminal work of McCullagh <ref type="bibr" coords="11,290.67,654.48,16.04,12.21" target="#b45">[46]</ref>, we consider the QMLE problem</p><formula xml:id="formula_53" coords="11,219.12,673.08,315.87,19.95">min β∈M f (β) := E D h( x, β ) − y x, β . (<label>17</label></formula><formula xml:id="formula_54" coords="11,534.99,673.08,4.81,12.21">)</formula><p>The function f given in ( <ref type="formula" coords="11,191.33,696.96,9.68,12.21" target="#formula_53">17</ref>) is called the negative log quasi-likelihood of the GLM <ref type="bibr" coords="11,465.64,696.96,17.85,12.21" target="#b15">(16)</ref>. In general f is not the negative of the log likelihood function, yet it shares many of its properties and hence the name. The motivation for this loss function comes from the canonical example of ( <ref type="formula" coords="12,471.30,73.44,8.98,12.21" target="#formula_51">16</ref>), where the conditional density of y given x admits an exponential-family formulation. In this case, standard maximum likelihood estimation of β ⋆ coincides with <ref type="bibr" coords="12,315.84,100.56,17.98,12.21" target="#b16">(17)</ref>. As illustration, Table <ref type="table" coords="12,444.48,100.56,5.45,12.21" target="#tab_0">1</ref> lists some common examples of QMLE. Henceforth, we let T := T M (β ⋆ ) denote the tangent space of M at β ⋆ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Response variable</head><p>Cumulant h(θ) Second derivative h ′′ (θ) </p><formula xml:id="formula_55" coords="12,104.04,167.18,324.29,75.61">Linear y = x, β ⋆ + ε 1 2 θ 2 1 Logistic y | x ∼ Ber exp x,β⋆ 1+exp x,β⋆ log(1 + exp θ) exp(θ) (1+exp(θ)) 2 Poisson y | x ∼ Poi(exp x, β ⋆ ) exp θ exp(θ) Gamma y | x ∼ Γ(σ −2 , −σ −2 x, β ⋆ ) − log(−θ) θ −2</formula><formula xml:id="formula_56" coords="12,120.60,275.30,143.79,20.41">E[ε | x] = 0 and Var[ε | x] = σ 2 .</formula><p>We begin with the following lemma that characterizes the set of ill-conditioned problem instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.1. The set of ill-conditioned distributions for (17) is given by</head><formula xml:id="formula_57" coords="12,71.40,356.52,468.41,51.99">E = {µ ∈ Q : T ∩ ker (Σ µ ) = {0}} . Moreover, for any µ ∈ Q \ E for which there exist c lb , c ub &gt; 0 satisfying c lb ≤ h ′′ ( x, β ⋆ ) ≤ c ub for µ-almost every x, we have c lb ≤ REG(µ) • λ min (Σ µ | T ) ≤ c ub .</formula><p>The proof of this lemma is deferred to Appendix D.1. The RSE for the problem follows quickly by computing the distance to the set E in Lemma 4.1; see Appendix D.2 for a proof. </p><formula xml:id="formula_58" coords="12,248.55,501.71,276.64,18.77">RSE(D) = λ min (Σ| T ),<label>(18)</label></formula><p>In particular, if for some c lb , c ub &gt; 0 the inequality c lb ≤ h ′′ ( x, β ⋆ ) ≤ c ub holds for D xalmost every x, then we have</p><formula xml:id="formula_59" coords="12,226.71,552.49,158.67,20.89">c lb ≤ (RSE(D)) 2 • REG(D) ≤ c ub .</formula><p>Thus we see that under mild conditions, the hardness of the problem is inversely proportional to the square of the distance to the nearest ill-posed problem, REG(D) ∝ RSE(D) −2 . Note that this scaling is different from the one exhibited by PCA in the previous section, REG(D) ∝ RSE(D) −1 .</p><p>Aside from the examples in Table <ref type="table" coords="12,254.16,619.32,4.22,12.21" target="#tab_0">1</ref>, an interesting problem instance occurs in sparse recovery. Namely, let M be the submanifold of R d comprised of k-sparse vectors, i.e., those which have precisely k nonzero components. Then the tangent space of M at β ⋆ is the k-dimensional subspace of R d in which β ⋆ is supported:</p><formula xml:id="formula_60" coords="12,241.08,678.48,129.75,19.95">T = span{e i | e i , β ⋆ = 0 ,</formula><p>where {e 1 , . . . , e d } denotes the standard basis of R d . Any regression problem from Table <ref type="table" coords="12,511.92,696.96,5.45,12.21" target="#tab_0">1</ref> constrained to M is ill-conditioned precisely when supp(x) ⊂ v ⊥ for some v ∈ T . The right-side of ( <ref type="formula" coords="13,75.55,73.44,9.74,12.21" target="#formula_58">18</ref>) is then the square root of the minimum eigenvalue of the submatrix of Σ D whose columns and rows are indexed by the nonzero coordinates of β ⋆ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Rank-one matrix regression problems</head><p>In this section, we compute the RSE for a number of tasks in low-rank matrix recovery-a large class of problems with numerous applications in control, system identification, recommendation systems, and machine learning. See for example <ref type="bibr" coords="13,303.97,172.80,17.06,12.21" target="#b13">[14,</ref><ref type="bibr" coords="13,324.60,172.80,13.95,12.21" target="#b16">17,</ref><ref type="bibr" coords="13,342.12,172.80,13.83,12.21" target="#b23">24,</ref><ref type="bibr" coords="13,359.64,172.80,13.83,12.21" target="#b69">70]</ref> for an overview. Setting the stage, consider the measurement model</p><formula xml:id="formula_61" coords="13,267.72,201.96,272.08,13.15">y = X, M ⋆ + ε (19)</formula><p>where the matrix X ∈ R d 1 ×d 2 is drawn from some distribution D x , the noise ε is independent of X and is mean zero with variance σ 2 , and M ⋆ is a rank r matrix. The goal of low-rank matrix recovery is to estimate the latent parameter M ⋆ for finitely many i.i.d samples (X 1 , y 1 ), . . . , (X n , y n ). For the rest of the section, we focus on the simplest case of rank r = 1 matrix recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Phase retrieval</head><p>The problem of phase retrieval corresponds to <ref type="bibr" coords="13,298.58,307.92,17.96,12.21" target="#b18">(19)</ref>, where the ground truth M ⋆ = β ⋆ β ⊤ ⋆ and the data X = xx ⊤ matrices are symmetric and have rank one. We will let D x ∈ P 4 (R d ) denote the distribution of x ∈ R d and let D denote the joint distribution of (x, y). There are two standard ways to write the phase retrieval problem as a problem of stochastic optimization. The first is simply to minimize the mean square error over rank one matrices:</p><formula xml:id="formula_62" coords="13,200.28,378.02,334.71,22.57">min M 0: rank M =1 f (M ) := 1 2 E x,y∼D x ⊤ M x − y 2 . (<label>20</label></formula><formula xml:id="formula_63" coords="13,534.99,380.64,4.81,12.21">)</formula><p>Alternatively, one may parameterize rank one matrices as M = ββ ⊤ and then minimize the mean square error over the factors:</p><formula xml:id="formula_64" coords="13,222.48,435.38,312.51,22.57">min β∈R d f (β) := 1 8 E x,y∼D x, β 2 − y 2 . (<label>21</label></formula><formula xml:id="formula_65" coords="13,534.99,438.00,4.81,12.21">)</formula><p>From the viewpoint of RSE there is no significant distinction between these two formulations. The proof of the next lemma appears in Appendix E.1.</p><p>Lemma 5.1. The set of ill-conditioned distributions for both <ref type="bibr" coords="13,368.16,498.12,19.36,12.21" target="#b19">(20)</ref> and ( <ref type="formula" coords="13,417.52,498.12,9.68,12.21" target="#formula_64">21</ref>) is given by</p><formula xml:id="formula_66" coords="13,194.64,518.42,222.75,25.73">E = v∈S d−1 µ ∈ P 4 (R d ) : supp(µ) ⊂ β ⊥ ⋆ ∪ v ⊥ .</formula><p>Fix any measure µ ∈ P 4 \ E and define the matrix Σµ = E µ x, β ⋆ 2 xx ⊤ . Then, the estimates hold:</p><formula xml:id="formula_67" coords="13,231.36,566.28,303.63,22.71">c lb ≤ REG(µ) • λ min ( Σµ ) ≤ c ub , (<label>22</label></formula><formula xml:id="formula_68" coords="13,534.99,569.04,4.81,12.21">)</formula><p>where</p><formula xml:id="formula_69" coords="13,102.12,587.18,127.59,13.61">(c lb , c ub ) = (2 β ⋆ 2 , 4 β ⋆ 2</formula><p>) for problem <ref type="bibr" coords="13,296.64,587.64,19.36,12.21" target="#b19">(20)</ref> and (c lb , c ub ) = (1, 1) for problem <ref type="bibr" coords="13,482.64,587.64,18.14,12.21" target="#b20">(21)</ref>.</p><p>It remains to estimate the expression for the distance to the exceptional set E. The following lemma shows that minimizing the expected squared distance to {β ⋆ } ⊥ ∪ {u} ⊥ over u ∈ S d−1 yields the squared W 2 distance to E; its proof appears in Appendix E.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5.2: (RSE for phase retrieval)</head><p>Consider the problems <ref type="bibr" coords="14,195.27,95.28,19.36,12.21" target="#b19">(20)</ref> and <ref type="bibr" coords="14,239.67,95.28,19.48,12.21" target="#b20">(21)</ref> and define Σ := E Dx [xx ⊤ ]. Then, equality holds:</p><formula xml:id="formula_70" coords="14,194.91,117.14,222.15,25.93">RSE(D) = min v∈S d−1 E x∼Dx x, β⋆ β⋆ 2 ∧ x, v 2 .</formula><p>Thus, using the reasoning from Lemma 5.1 and Theorem 5.2 one easily derives the following two estimates for formulation <ref type="bibr" coords="14,216.13,170.28,17.97,12.21" target="#b20">(21)</ref>:</p><formula xml:id="formula_71" coords="14,188.04,192.14,235.95,56.45">RSE(D) = min v∈S d−1 E x∼Dx x, β⋆ β⋆ 2 ∧ x, v 2 , REG(D) −1 = min v∈S d−1 E x∼Dx x, β ⋆ 2 x, v 2 .</formula><p>A moment of thought leads one to realize that for reasonable distributions, the first equation should scale as λ min (Σ) while the scaling of the second is at least λ min (Σ). We now verify that this is indeed the case when the base distribution is Gaussian x ∼ N (0, Σ) for some covariance matrix Σ 0. To this end, define the two functions</p><formula xml:id="formula_72" coords="14,217.80,311.06,176.31,45.25">h Σ (u, v) := E x∼N(0,Σ) x, u 2 ∧ x, v 2 , g Σ (u, v) := E x∼N(0,Σ) x, u 2 • x, v 2 ,</formula><p>where u, v ∈ S d−1 vary over the unit sphere. We defer the proof of the next result to Appendix E.3.</p><p>Theorem 5.3. For any u ∈ S d−1 , the following estimates hold:</p><formula xml:id="formula_73" coords="14,187.56,416.30,347.43,20.65">1 − 2 π λ min (Σ) ≤ min v∈S d−1 h Σ (u, v) ≤ λ min (Σ). (<label>23</label></formula><formula xml:id="formula_74" coords="14,534.99,417.00,4.81,12.21">)</formula><formula xml:id="formula_75" coords="14,176.88,440.04,362.92,19.95">λ min (Σ) • Σu, u ≤ min v∈S d−1 g Σ (u, v) ≤ 3λ min (Σ) • Σu, u<label>(24)</label></formula><p>Combining Lemma 5.1, Theorem 5.2, and Theorem 5.3 directly yields the following estimate on RSE with Gaussian initial data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5.4: (RSE for phase retrieval with Gaussian data)</head><p>Consider problems <ref type="bibr" coords="14,176.55,522.95,19.36,12.21" target="#b19">(20)</ref> and <ref type="bibr" coords="14,220.47,522.95,19.36,12.21" target="#b20">(21)</ref> with Gaussian data D x ∼ N (0, Σ). Then, the estimate hold:</p><formula xml:id="formula_76" coords="14,216.27,544.09,189.39,20.77">(1 − 2 π )λ min (Σ) ≤ RSE(D) ≤ λ min (Σ).</formula><p>In particular, the following estimates hold:</p><formula xml:id="formula_77" coords="14,85.95,580.09,343.59,47.09">c lb • 1 − 2 π 3 Σβ ⋆ , β ⋆ ≤ RSE(D) 2 • REG(D) ≤ c ub • 1 Σβ ⋆ , β ⋆ where (c lb , c ub ) = (2 β ⋆ 2 , 4 β ⋆ 2</formula><p>) for problem <ref type="bibr" coords="14,311.31,614.02,19.36,12.21" target="#b19">(20)</ref> and (c lb , c ub ) = (1, 1) for problem <ref type="bibr" coords="14,497.31,614.03,18.14,12.21" target="#b20">(21)</ref>.</p><p>Interestingly, we see a different scaling between RSE(D) and REG(D), depending on whether β ⋆ aligns with the nullspace of Σ. In the regime Σβ ⋆ , β ⋆ ≫ λ min (Σ), we observe the scaling REG(D) ∝ RSE(D) −2 , while in the regime Σβ ⋆ , β ⋆ ≈ λ min (Σ), the scaling is much worse REG(D) ∝ RSE(D) −4 . Thus in the latter regime, the distance to the nearest ill-posed problem has a much stronger effect on the hardness of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bilinear sensing</head><p>The problem of bilinear sensing is an asymmetric analogue of phase retrieval, that is, the ground truth matrix M ⋆ = β 1⋆ β ⊤ 2⋆ and measurement data X = x 1 x ⊤ 2 are rank one d 1 × d 2 matrices, where the factors x 1 ∼ D x 1 and x 2 ∼ D x 2 are independent. The standard way to write this problem as stochastic optimization is to minimize the mean square error over rank one rectangular matrices:</p><formula xml:id="formula_78" coords="15,194.16,150.62,340.83,28.23">min M ∈R d 1 ×d 2 : rank M =1 f (M ) := 1 2 E D x ⊤ 1 M x 2 − y 2 , (<label>25</label></formula><formula xml:id="formula_79" coords="15,534.99,153.24,4.81,12.21">)</formula><p>where D denotes the joint distribution over (x 1 , x 2 , y). Throughout, we fix as the set of allowable data distributions all product measures Q = P 4 (R d 1 ) × P 4 (R d 1 ). We disregard the factorized formulation with M = β 1 β ⊤ 2 because it results in a continuum of minimizers that are not tilt-stable. This technical difficulty could be circumvented by introducing an additional constraint, such as β 1 = 1; however, we do not pursue this approach to simplify the exposition.</p><p>The following lemma characterizes the set of ill-conditioned problems; see Appendix F.1 for a proof. For any PSD matrix Σ, the symbol κ(Σ) = λ max (Σ)/λ min (Σ) denotes its condition number.</p><p>Lemma 5.5. The set of ill-conditioned distributions for <ref type="bibr" coords="15,345.00,281.16,19.36,12.21" target="#b24">(25)</ref> is given by</p><formula xml:id="formula_80" coords="15,71.40,301.46,463.59,68.93">E = µ × ν ∈ Q : either E µ xx ⊤ or E ν xx ⊤ is singular . Moreover, for any µ × ν ∈ Q \ E, with Σ 1 = E µ x 1 x ⊤ 1 and Σ 2 = E ν x 2 x ⊤ 2 , we have 2 • min{γ 2 λ min (Σ 1 ), γ 1 λ min (Σ 2 )} κ(Σ 1 )κ(Σ 2 ) + 1 ≤ REG(µ × ν) −1 ≤ min{γ 2 λ min (Σ 1 ), γ 1 λ min (Σ 2 )} (<label>26</label></formula><formula xml:id="formula_81" coords="15,534.99,349.68,4.81,12.21">)</formula><p>where</p><formula xml:id="formula_82" coords="15,102.12,376.22,155.63,17.43">γ i = Σ i β i⋆ β i⋆ , β i⋆ β i⋆ for i = 1, 2.</formula><p>Thus an application of Theorem A.3 immediately yields the following expression for RSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5.6: (RSE for bilinear sensing)</head><p>Consider the problem <ref type="bibr" coords="15,190.71,444.91,19.48,12.21" target="#b24">(25)</ref> with</p><formula xml:id="formula_83" coords="15,237.75,444.46,186.47,15.78">Σ 1 := E Dx 1 x 1 x ⊤ 1 and Σ 2 := E Dx 2 x 2 x ⊤ 2 .</formula><p>Then it holds:</p><formula xml:id="formula_84" coords="15,205.35,470.11,201.27,13.15">RSE(D) = min λ min (Σ 1 ), λ min (Σ 2 ) .</formula><p>In particular, the following estimate holds:</p><formula xml:id="formula_85" coords="15,85.95,509.11,428.33,51.29">min {λ min (Σ 1 ), λ min (Σ 2 )} min{γ 2 λ min (Σ 1 ), γ 1 λ min (Σ 2 )} ≤ RSE(D) 2 • REG(D) ≤ C • min {λ min (Σ 1 ), λ min (Σ 2 )} min{γ 2 λ min (Σ 1 ), γ 1 λ min (Σ 2 )} where C = κ(Σ 1 )κ(Σ 2 )+1 2 with γ i = Σ i β i⋆ β i⋆ , β i⋆ β i⋆ for i = 1, 2.</formula><p>Similar to phase retrieval, the scaling between RSE and REG for bilinear sensing depends on the simultaneous alignment between β i⋆ and the least eigenvector of Σ i for i = 1, 2. In particular, when κ(Σ 1 )κ(Σ 2 ) ≈ 1 both upper and lower bounds are off by a constant and if, further, λ min (Σ 1 ) ≈ λ min (Σ 2 ), there are two regimes: (1) when min{γ 1 , γ 2 } ≫ λ min (Σ 1 ), then REG(D) ∝ RSE(D) −2 , and (2) when min{γ 1 , γ 2 } ≈ λ min (Σ 1 ), then REG(D) ∝ RSE(D) −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Matrix completion</head><p>The problem of matrix completion corresponds to <ref type="bibr" coords="15,320.54,677.04,17.87,12.21" target="#b18">(19)</ref>, where the ground truth matrix M ⋆ has low rank and the data matrices X ∼ D x are drawn from some discrete distribution on matrices of the form X = e i e ⊤ j . We will focus on the simplified setting where M ⋆ is rank one and positive semidefinite. The standard way to write this problem as stochastic optimization is min</p><formula xml:id="formula_86" coords="16,222.12,91.46,312.87,26.91">M ∈M f (M ) := 1 2 E D ( X, M − y 2 , (<label>27</label></formula><formula xml:id="formula_87" coords="16,534.99,94.08,4.81,12.21">)</formula><p>where M = {M 0 : rank M = 1} is the set of rank one PSD matrices. In this section, we compute the RSE of the problem in terms of the graph induced by the support of the distribution D x . We begin with a few observations. First, it is straightforward to verify ∇f (M ⋆ ) = 0 and</p><formula xml:id="formula_88" coords="16,209.40,161.54,325.59,20.89">∇f (M ⋆ )[∆, ∆] = E X, ∆ 2 ∀∆ ∈ S d . (<label>28</label></formula><formula xml:id="formula_89" coords="16,534.99,162.48,4.81,12.21">)</formula><p>In particular, the optimal Lagrange multipliers for M ⋆ are zero. Forming the factorization M ⋆ = β ⋆ β ⊤ ⋆ , the tangent space to M at M ⋆ can be written as</p><formula xml:id="formula_90" coords="16,72.00,212.18,355.23,39.37">T = {β ⋆ v ⊤ + vβ ⊤ ⋆ : v ∈ R d }. Moreover, an elementary computation shows that ∆ 2 F / β ⋆ 2 v 2 ∈ [2, 4].</formula><p>In particular, ∆ is zero if and only if v is zero. Consequently, the set of ill-conditioned distributions takes the form:</p><formula xml:id="formula_91" coords="16,192.36,262.70,347.44,31.47">E = v∈R d \{0} {µ ∈ P 2 (S d ) : supp(X) ⊂ (vβ ⊤ ⋆ ) ⊥ }.<label>(29)</label></formula><p>Indeed, estimating REG(µ) at any µ ∈ P 2 (S d ) \ E is straightforward, and is the content of the following lemma; see Appendix G.1 for a proof.</p><p>Lemma 5.7. Consider any measure µ ∈ P 2 (S d ) \ E and define the matrices</p><formula xml:id="formula_92" coords="16,154.08,345.14,303.87,21.01">Φ β⋆ = (I ⊗ β ⋆ ) + (β ⋆ ⊗ I) and Σ µ = E µ vec(X)vec(X) ⊤ .</formula><p>Then the estimate holds:</p><formula xml:id="formula_93" coords="16,195.60,382.22,339.39,21.01">2 β ⋆ 2 ≤ REG(µ) • λ min (Φ ⊤ β⋆ Σ µ Φ β⋆ ) ≤ 4 β ⋆ 2 . (<label>30</label></formula><formula xml:id="formula_94" coords="16,534.99,383.28,4.81,12.21">)</formula><p>Observe now that most measures µ ∈ E do not correspond to a matrix completion problem, since they do not even need to be discrete. With this in mind, we now focus on the setting where the set of admissible distributions Q encodes only matrix completion problems. To this end, for a matrix completion problem, X is equal to some matrix X = e ℓ e ⊤ k , whose distribution is induced by a random pair (ℓ, k) in ⌊ ⌈d⌋ ⌉×⌊ ⌈d⌋ ⌉ and can be represented by a matrix of probabilities</p><formula xml:id="formula_95" coords="16,146.52,474.48,318.87,13.15">P = (p ij ) where p ij = P (ℓ, k) = (i, j) = P X ij = 1 = E X ij .</formula><p>Since M ⋆ is symmetric, we assume without loss of generality that P is symmetric as well. We denote the set of all such symmetric distributions on ⌊ ⌈d⌋ ⌉×⌊ ⌈d⌋ ⌉ by</p><formula xml:id="formula_96" coords="16,72.00,524.06,375.27,40.81">Q = (p ij ) ∈ S d | ij p ij = 1 and p ij ≥ 0 for all i, j ∈ ⌊ ⌈d⌋ ⌉ , For each Q ∈ Q, we let µ Q ∈ P 2 (R d×d</formula><p>) be the distribution over canonical matrices e i e ⊤ j where (i, j) ∼ Q. 2 Thus, the set of ill-conditioned distributions encoding matrix completion problems is</p><formula xml:id="formula_97" coords="16,244.56,576.02,122.79,20.89">E mc := µ Q | Q ∈ Q ∩ E,</formula><p>where E is defined in <ref type="bibr" coords="16,172.33,595.56,17.97,12.21" target="#b28">(29)</ref>. We are now ready to study the Wasserstein distance between µ and E mc . We will see that this distance relies on the combinatorial structure of the observations.</p><p>Consider the undirected graph G = (V, E) with vertices V = ⌊ ⌈d⌋ ⌉, and edges E = {(i, j) | p ij &gt; 0}. Thus, E corresponds to the tuples of indices that are "observed" in the problem. Let G * = (V * , E * ) be the induced graph given by V * = supp(β ⋆ ), <ref type="bibr" coords="16,520.56,654.72,19.24,12.21" target="#b30">(31)</ref> meaning that E * consists of all the edges in E between elements of V * . Further, define</p><formula xml:id="formula_98" coords="16,208.68,690.74,326.31,21.01">V 0 = {i / ∈ V * | for all j ∈ V * , (i, j) / ∈ E}. (<label>32</label></formula><formula xml:id="formula_99" coords="16,534.99,691.80,4.81,12.21">)</formula><p>2</p><p>With a slight abuse of notation we use Q to denote both the matrix and distribution over indices.</p><p>Thus, V 0 corresponds to nodes i with β i = 0 that are not connected to any j for which β j = 0. We will see shortly that the following assumption characterizes the set of well-posed instances of matrix completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 1. Consider a graph G = ([d],</head><p>E) with G * and V 0 defined in <ref type="bibr" coords="17,437.42,123.12,19.35,12.21" target="#b30">(31)</ref> and <ref type="bibr" coords="17,482.29,123.12,17.87,12.21" target="#b31">(32)</ref>, respectively. Suppose the following two hold.</p><p>1. (Non-bipartite) G * has no connected components that are bipartite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">(No isolated zeros)</head><p>The set V 0 is empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given a set of edges A ⊂ ⌊</head><p>⌈d⌋ ⌉×⌊ ⌈d⌋ ⌉ , we let G A be the graph induced by A and define</p><formula xml:id="formula_100" coords="17,165.00,222.72,281.91,19.95">Ω β⋆ = A ⊂ ⌊ ⌈d⌋ ⌉×⌊ ⌈d⌋ ⌉ | G A does not satisfy Assumption 1 .</formula><p>We are now ready to show the main result of this section; see Appendix G.2 for a proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5.8: (RSE for matrix completion)</head><p>Let P = (p ij ) and M ⋆ = β ⋆ β ⊤ ⋆ be the data of the matrix completion problem <ref type="bibr" coords="17,454.83,284.87,18.23,12.21" target="#b26">(27)</ref>, and let µ be the distribution of X induced by P. Then, µ is well-posed, i.e., µ / ∈ E mc , if, and only if, the graph G induced by the problem satisfies Assumption 1. Additionally, the identity holds:</p><formula xml:id="formula_101" coords="17,221.31,330.47,299.07,28.39">RSE(µ) = min A∈Ω β⋆ A⊂supp(P ) ij∈supp(P )\A p ij . (<label>33</label></formula><formula xml:id="formula_102" coords="17,520.38,330.47,4.81,12.21">)</formula><p>Moreover, computing RSE(µ) is NP-hard in general.</p><p>Thus the theorem shows that one can compute RSE(µ) by enumerating over all exceptional edge sets A ⊂ supp(P ), meaning that G * A either has a connected bipartite component or V 0 is nonempty. Then, the set A for which the mass ij∈supp(P )\A p ij is smallest yields RSE(µ). Interestingly, computing RSE(µ) is NP-hard in general, as we show by a reduction from MAXCUT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduced a new measure of robustness-radius of statistical efficiency (RSE)for problems of statistical inference and estimation. We computed RSE for a number of test-bed problems, including principal component analysis, generalized linear models, phase retrieval, bilinear sensing, and matrix completion. In all cases, we verified a precise reciprocal relationship between RSE and the intrinsic complexity/sensitivity of the problem instance, thereby paralleling the classical Eckart-Young theorem and its numerous extensions in numerical analysis and optimization. More generally, we obtained sufficient conditions for such a relationship to hold that depend only on local information (gradients, Hessians), rather than an explicit description of the set of ill-conditioned distributions. We believe that this work provides an intriguing new perspective on the interplay between problem difficulty, solution sensitivity, and robustness in statistical inference and learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Geometry of the Wasserstein space and distance estimation</head><p>In this section, we introduce the necessary background of the Wasserstein geometry and prove a number of results that may be on independent interest. In the following section, we will use many of these results to prove estimates on RSE announced in the paper. We follow standard notation of optimal transport, as set out for example in the monographs of Villani <ref type="bibr" coords="22,431.77,284.04,16.94,12.21" target="#b65">[66]</ref> and Santambrogio <ref type="bibr" coords="22,72.00,297.60,15.96,12.21" target="#b58">[59]</ref>.</p><p>Let (X , d) be a separable complete metric space equipped with its Borel σ-algebra B X . The primary example for us will be R d equipped with the ℓ p norm. The distance of a point x ∈ X to a set K ⊂ X will be denoted by dist(x, K) = inf x ′ ∈K d(x, x ′ ). The set of Borel probability measures on X is denoted by P(X ), and will be abbreviated as P if the space X is clear from context. The support of a measure µ ∈ P, written as supp(µ), is the smallest closed set C ⊂ X such that the complement X \ C is of zero µ-measure. For any measurable map T : (Ω, F, P) → (X , B X ), the pushforward measure T # µ is defined to be</p><formula xml:id="formula_103" coords="22,202.92,409.94,206.19,21.01">(T # µ)(B) = µ(T −1 (B)) for all B ∈ B X .</formula><p>The support of a random variable on X is the support of its distribution. For any p ≥ 1, the symbol P p denotes the set of all distributions µ on X with finite p th moment, meaning E x∼µ d(x, x 0 ) p &lt; ∞ for some (and hence any) x 0 ∈ X . The Wasserstein-p distance between two measures µ, ν ∈ P p is defined by:</p><formula xml:id="formula_104" coords="22,209.28,488.78,193.47,28.25">W p (µ, ν) = min π∈Π(µ,ν) E (x,y)∼π d(x, y) p 1/p .</formula><p>Here, the set Π(µ, ν) consists of couplings between µ and ν, i.e., distributions in P p (X × X ) having µ and ν as its first and second marginals. An important fact is that the pair (P p , W p ) is a separable complete metric space in its own right and is called the Wasserstein-p space on X .</p><p>We will need a few basic estimates on the W p distance. First, consider any measures µ, ν ∈ P p (X ) and a measurable map T : X → X satisfying ν = T # µ. Then the law of the random variable (x, T (x)) is a coupling between µ and ν and therefore the estimate holds:</p><formula xml:id="formula_105" coords="22,240.48,607.94,294.51,21.01">W p p (µ, ν) ≤ E x∼µ d(x, T (x)) p . (<label>34</label></formula><formula xml:id="formula_106" coords="22,534.99,609.00,4.81,12.21">)</formula><p>Another useful observation is that for any measures µ, ν ∈ P p such that the support of ν is contained in a set C ⊂ K, the estimate holds:</p><formula xml:id="formula_107" coords="22,241.56,662.06,293.43,21.01">W p p (µ, ν) ≥ E x∼µ dist p (x, C). (<label>35</label></formula><formula xml:id="formula_108" coords="22,534.99,663.12,4.81,12.21">)</formula><p>Consequently, equality holds in <ref type="bibr" coords="22,224.55,685.80,19.33,12.21" target="#b33">(34)</ref> when T is a projection, the content of the following lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma A.1 (W p metric &amp; projections)</head><p>. Consider a measure µ ∈ P p and a set K ⊂ X . Suppose that the metric projection P K admits a measurable selection s : X → X . Then equality holds:</p><formula xml:id="formula_109" coords="23,232.08,104.54,302.91,21.01">W p p (µ, s # µ) = E X∼µ dist(X, K) p . (<label>36</label></formula><formula xml:id="formula_110" coords="23,534.99,105.60,4.81,12.21">)</formula><p>Proof. We first show that ν := s # µ lies in P p . Indeed, for any x 0 ∈ K and x ∈ X we have</p><formula xml:id="formula_111" coords="23,196.92,151.20,218.07,19.95">d(s(x), x 0 ) ≤ d(s(x), x) + d(x, x 0 ) ≤ 2d(x, x 0 ),</formula><p>and therefore ν lies in P p . Next, taking into account that the support of ν is contained in K, combining <ref type="bibr" coords="23,124.45,183.24,19.35,12.21" target="#b33">(34)</ref> and ( <ref type="formula" coords="23,173.57,183.24,9.68,12.21" target="#formula_107">35</ref>) completes the proof.</p><p>For the rest of the section, we will focus exclusively on the setting where X is the Euclidean space R d equipped with the inner product •, • . The ℓ p norm in R d will be denoted by • p . Finally, we denote the second moment matrix for any measure µ ∈ P 2 , by the symbol</p><formula xml:id="formula_112" coords="23,270.00,250.46,71.91,18.41">Σ µ := E x∼µ xx ⊤ .</formula><p>Given a set of measures V ⊂ P p , the distance function W 2 (•, V) is defined in the usual way</p><formula xml:id="formula_113" coords="23,244.56,292.68,122.79,19.95">W 2 (µ, V) = inf ν∈V W 2 (µ, ν).</formula><p>Although estimating the distance function is difficult in general, we will focus on well-structured sets V for which W 2 (µ, V) can be readily computed. The following two sections study, respectively, measures constrained either by the location of its support or by the spectrum of its covariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Sets of measures constrained by their support</head><p>Consider a linear subspace K ⊆ X and a mean zero measure µ ∈ P p . Recall that the compression Σ µ | K of Σ µ to K is the positive semidefinite quadratic form on K given by:</p><formula xml:id="formula_114" coords="23,211.56,422.78,108.87,14.09">Σ µ v, v = E x∼µ [ x, v 2 ]</formula><p>for all v ∈ K.</p><p>We will need the following lemma that provides a convenient interpretation of the trace of Σ µ | K .</p><p>Lemma A.2 (Trace of covariance &amp; W 2 distance). Consider a measure µ ∈ P 2 and let K be a proper subspace of R d . Then, the following equalities hold:</p><formula xml:id="formula_116" coords="23,195.48,495.86,220.95,21.01">tr Σ µ | K = E x∼µ dist(x, K ⊥ ) 2 = W 2 2 (µ, (P K ⊥ ) # µ).</formula><p>Proof. We successively compute</p><formula xml:id="formula_117" coords="23,112.44,540.38,382.42,21.01">tr Σ µ | K = E x∼µ tr(P K xx ⊤ P K ) = E x∼µ P K x 2 = E x∼µ dist 2 (x, K ⊥ ) = W 2 2 (µ, (P K ⊥ ) # µ)</formula><p>, where the last equality follows from Lemma A.1.</p><p>In light of (37), the matrix Σ µ | K is singular if and only if the inclusion supp(µ) ⊂ v ⊥ holds for some v ∈ K ∩ S d−1 . Define the set of measures for which Σ ν | K is indeed singular:</p><formula xml:id="formula_118" coords="23,177.48,617.06,357.51,20.89">V K = ν ∈ P 2 | supp(ν) ⊂ v ⊥ for some v ∈ K ∩ S d−1 . (<label>38</label></formula><formula xml:id="formula_119" coords="23,534.99,618.00,4.81,12.21">)</formula><p>The following theorem, the main result of the section, shows that the W 2 distance to V K is simply the minimal eigenvalue of Σ µ | K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem A.3: (Distance to V K )</head><p>Consider a measure µ ∈ P 2 and a proper linear subspace K of R d . Then, equality holds:</p><formula xml:id="formula_120" coords="24,245.67,112.73,117.02,21.01">W 2 2 (µ, V K ) = λ min (Σ µ | K )</formula><p>, where V K is defined in <ref type="bibr" coords="24,200.19,132.27,18.13,12.21" target="#b37">(38)</ref>. Moreover the distance of µ to V K is attained by the measure (P v ⊥ ) # µ, where v ∈ K is an eigenvector of Σ µ | K corresponding to its minimal eigenvalue. Proof. For any vector v ∈ K ∩ S d−1 , applying Lemma A.2 with span(v) in place of K yields:</p><formula xml:id="formula_121" coords="24,207.72,191.42,327.27,21.01">Σ µ v, v = tr Σ µ | span(v) = W 2 2 (µ, (P K ⊥ ) # µ). (<label>39</label></formula><formula xml:id="formula_122" coords="24,534.99,192.48,4.81,12.21">)</formula><p>Let us now decompose V K into a union of simpler sets</p><formula xml:id="formula_123" coords="24,154.32,228.50,303.27,25.85">V K = v∈K∩S d−1 L v where L v := {ν ∈ P 2 : supp(ν) ⊂ v ⊥ }.</formula><p>We now estimate</p><formula xml:id="formula_124" coords="24,161.04,275.30,289.83,21.01">W 2 (µ, L v ) ≤ W 2 (µ, (P v ⊥ ) # µ) = E x∼µ dist(x, v ⊥ ) 2 ≤ W 2 (µ, L v ).</formula><p>where the first inequality holds trivially, the equality follows from Lemma A.1, and the last inequality follows from <ref type="bibr" coords="24,178.10,311.88,19.35,12.21" target="#b34">(35)</ref> with C = v ⊥ . Thus equality holds throughout. Using (39), we then conclude</p><formula xml:id="formula_125" coords="24,147.84,341.04,316.23,19.95">W 2 (µ, V K ) = inf v∈K∩S d−1 W 2 (µ, L v ) = inf v∈K∩S d−1 Σ µ v, v = λ min (Σ µ | K ),</formula><p>as claimed. It also follows immediately that for any minimal eigenvector v of Σ µ | K , the pushforward measure ν = (P v ⊥ ) # µ attains the minimal W 2 distance of µ to V K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Spectral sets and functions of measures</head><p>In this section, we investigate a special class of functions on P 2 -called spectral-that depend on the measure only through the eigenvalues of its second moment matrix. This function class has close analogues in existing literature in matrix analysis and eigenvalue optimization. We postpone a detailed discussion on the related literature until the end of the section.</p><p>The following is the key definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition A.4 (Spectral functions of measures). A function</head><formula xml:id="formula_126" coords="24,72.00,496.80,469.24,39.33">F : P 2 → R ∪ {+∞} is called spectral if for any µ, ν ∈ P 2 with the same second moment matrix λ (Σ µ ) = λ (Σ ν ) equality F (µ) = F (ν) holds.</formula><p>A good example to keep in mind is the Schatten norm F (µ) = λ(Σ µ ) q for any q ∈ (0, ∞]. Notice that in this example F factors as a composition of the eigenvalue map λ(•) and the permutationinvariant function f (v) = v q on R d . Evidently, all spectral functions arise in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition A.5 (Symmetric functions). A function</head><formula xml:id="formula_127" coords="24,72.00,587.06,467.98,33.85">f : R d + → R ∪ {+∞} is called symmetric if equality f (s(x)) = f (x) holds for all x ∈ R d</formula><p>+ and all permutations of coordinates s(•). An elementary observation is that a function F : P 2 → R ∪ {+∞} is spectral if and only if there exists a symmetric function f :</p><formula xml:id="formula_128" coords="24,222.72,632.30,155.55,39.01">R d + → R ∪ {+∞} satisfying F (µ) = f (λ(Σ µ )) ∀µ ∈ P 2 .</formula><p>Concretely, the symmetric function f can be obtained from F by restricting to Gaussian measures with diagonal covariance f (v) = F (N (0, Diag(v))). The definition of spectral and symmetric functions easily extends to sets through indicator functions. Namely, a set</p><formula xml:id="formula_129" coords="24,72.00,696.96,470.06,33.51">G ⊂ P 2 is spectral if the indicator function δ G is spectral, while a set G ⊂ R d + is symmetric if the indicator δ G is symmetric.</formula><p>An interesting example of a spectral set is given by</p><formula xml:id="formula_130" coords="25,218.52,92.04,316.47,19.95">G q := {µ ∈ P 2 : λ q (Σ µ ) = λ q+1 (Σ µ )}. (<label>40</label></formula><formula xml:id="formula_131" coords="25,534.99,92.04,4.81,12.21">)</formula><p>Although complicated, we may identify it with the symmetric set</p><formula xml:id="formula_132" coords="25,71.64,128.06,451.96,39.49">G q = {v ∈ R d + : v (q) = v (q+1) }, where v (i) is the i'th largest coordinate of v. Indeed, equality G = {µ ∈ P 2 : λ(Σ µ ) ∈ G} holds.</formula><p>In this section, we will show that one can express the W 2 -distance function to G purely in terms of the ℓ 2 -distance function to the much simpler set G. Indeed, we will prove the following theorem, which specialized to example (40) yields the expression</p><formula xml:id="formula_133" coords="25,171.84,210.86,268.35,20.77">W 2 (µ, G q ) = dist 2 λ(Σ µ ), G q = 1 √ 2 λ q − λ q+1 .</formula><p>Theorem A.6: (Distance to spectral sets in P 2 )</p><formula xml:id="formula_134" coords="25,86.07,259.89,307.71,39.01">Let G ⊂ R d + be a symmetric set. Define now the set of measures G = {ν ∈ P 2 : λ(Σ ν ) ∈ G}.</formula><p>Then for any µ ∈ P 2 , equality holds:</p><formula xml:id="formula_135" coords="25,221.31,313.12,169.47,28.29">W 2 (µ, G) = min v∈G λ(Σ µ ) − √ v 2 .</formula><p>Indeed, we will prove a more general statement that applies to functions, with the distance replaced by the so-called Moreau envelope. We need some further notation to proceed. Let (Y, d) be a metric space and consider a function f : Y → R ∪ {+∞}. Then for any parameter ρ &gt; 0, the Moreau envelope and the proximal map of f <ref type="bibr" coords="25,284.88,393.00,16.95,12.21" target="#b26">[27,</ref><ref type="bibr" coords="25,305.40,393.00,12.72,12.21" target="#b46">47]</ref>, respectively, are defined as:</p><formula xml:id="formula_136" coords="25,210.60,410.54,190.71,50.91">f ρ (y) := inf y ′ ∈Y f (y ′ ) + 1 2ρ d 2 (y, y ′ ), prox ρf (y) := argmin y ′ ∈W f (y ′ ) + 1 2ρ d 2 (y, y ′ ).</formula><p>In particular, if f is an indicator function of a set Q, then f ρ reduces to the squared distance function to Q, while prox ρf (w) is the nearest point projection.</p><p>We will be interested in three metric spaces and it is important to keep the metric in mind in all results that follow.</p><p>• (P 2 , W 2 ) The space P 2 (R d ) equipped with the Wasserstein-2 distance W 2 (•, •).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• (S d</head><p>+ , W 2 ) The cone of PSD matrices S d + equipped with the Bures-Wasserstein distance</p><formula xml:id="formula_137" coords="25,85.32,565.10,415.52,62.53">W 2 2 (A, B) = tr A + tr B − 2 tr(A 1/2 BA 1/2 ). • (R d + , W 2 ) The cone of nonnegative vectors R d + equipped with the Hellinger distance W 2 (x, y) = √ x − √ y 2 ,</formula><p>where the square root is applied elementwise.</p><p>Notice that we are abusing notation by using the same symbol W 2 to denote the metric in all three spaces. The reason we are justified in doing so is that the three metric spaces are related by isometric embedding. Namely, the Wasserstein-2 distance between two Gaussian distributions µ = N (0, Σ µ ) and ν = N (0, Σ ν ) coincides with the Bures-Wasserstein distance between their covariance matrices:</p><formula xml:id="formula_138" coords="25,249.24,707.88,113.43,13.15">W 2 (µ, ν) = W 2 (Σ µ , Σ ν ).</formula><p>Similarly, the Bures-Wasserstein metric restricted to diagonal PSD matrices is the Hellinger distance:</p><p>W 2 (Diag(x), Diag(y)) = W 2 (x, y). The following is the main result of this section. </p><formula xml:id="formula_139" coords="26,218.43,192.75,175.11,21.01">F ρ (µ) = f ρ (λ(Σ µ )) ∀µ ∈ P 2 (R d ).</formula><p>Theorem A.6 follows immediately by applying Theorem A.7 to indicator functions δ G and δ G . The rest of the section is devoted to proving Theorem A.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Proof of Theorem A.7</head><p>We begin with some notation. The symbol O(d) will denote the set of orthogonal d × d matrices. The singular values for any matrix A ∈ R m×n (with m ≤ n) in nonincreasing order we written as</p><formula xml:id="formula_140" coords="26,245.88,314.40,126.39,19.95">1 (A) ≥ σ 2 (A) . . . ≥ σ m (A).</formula><p>We say that two matrices A and B admit a simultaneous ordered singular-value decomposition (SVD) if there exist matrices</p><formula xml:id="formula_141" coords="26,72.00,345.98,467.97,26.23">U ∈ O(m), V ∈ O(n) satisfying U ⊤ AV = Diag(σ(A)) and U ⊤ BV = Diag(σ(B)).</formula><p>The following trace inequality, essentially due to <ref type="bibr" coords="26,375.03,360.00,17.04,12.21" target="#b63">[64,</ref><ref type="bibr" coords="26,396.24,360.00,12.72,12.21" target="#b66">67]</ref>, will play a central role in the section. The theorem as stated, along with a proof, may be found in <ref type="bibr" coords="26,421.95,373.56,16.93,12.21" target="#b41">[42,</ref><ref type="bibr" coords="26,442.56,373.56,61.94,12.21">Theorem 4.6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem A.8 (von Neumann-Theobald). Any two matrices A, B ∈ R m×n satisfy σ(A), σ(B) ≥ A, B .</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moreover, equality holds if and only if A and B admit a simultaneous ordered SVD.</head><p>The following lemma shows that the Burer-Wasserstein distance can be written in terms of a Procrustes problem <ref type="bibr" coords="26,168.49,469.20,11.43,12.21" target="#b6">[7,</ref><ref type="bibr" coords="26,183.60,469.20,53.70,12.21">Theorem 1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma A.9 (Procrustes distance). For any two matrices A, B ∈ S d</head><p>+ equality holds:</p><formula xml:id="formula_142" coords="26,218.52,510.74,173.24,25.71">W 2 (A, B) = min U ∈O(d) A 1/2 − B 1/2 U F</formula><p>We will also need the following variational form of the Burer-Wasserstein distance; see for example <ref type="bibr" coords="26,114.72,553.44,11.56,12.21" target="#b6">[7,</ref><ref type="bibr" coords="26,129.84,553.44,45.91,12.21">Section 3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma A.10 (Variational form). For any two matrices A, B ∈ S d</head><p>+ equality holds:</p><formula xml:id="formula_143" coords="26,199.20,593.42,213.15,21.01">W 2 2 (A, B) = min x,y: E[xx ⊤ ]=A, E[yy ⊤ ]=B E x − y 2 2</formula><p>With these results in place, we are ready to start proving Theorem A.7. To this end, we will first establish the theorem in the Gaussian setting and then deduce the general case by a reduction. As the first step, we will estimate the W 2 -distance of a matrix to an orbit of B under conjugation:</p><formula xml:id="formula_144" coords="26,233.40,667.58,145.11,21.01">O(B) := {V BV ⊤ : V ∈ O(d)}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma A.11 (Distance to orbit: Gaussian case). For any two matrices A, B ∈ S d</head><p>+ it holds:</p><formula xml:id="formula_145" coords="27,216.96,97.68,318.03,21.19">W 2 (A, O(B)) = λ(A) − λ(B) 2 . (<label>41</label></formula><formula xml:id="formula_146" coords="27,534.99,97.68,4.81,12.21">)</formula><p>Moreover, the set of nearest points of O(B) to A is given by</p><formula xml:id="formula_147" coords="27,175.92,139.10,359.07,20.89">{U Diag(λ(B))U ⊤ : A = U Diag(λ(A))U ⊤ , U ∈ O(d)}. (<label>42</label></formula><formula xml:id="formula_148" coords="27,534.99,140.04,4.81,12.21">)</formula><p>Proof. To see the inequality ≤, consider an eigenvalue decomposition A = U Diag(λ(A))U ⊤ for some U ∈ O(d). Then we have</p><formula xml:id="formula_149" coords="27,126.36,193.70,358.95,20.89">W 2 (A, O(B)) ≤ W 2 (U Diag(λ(A))U ⊤ , U Diag(λ(B))U ⊤ ) = W 2 (λ(A), λ(B)),</formula><p>as claimed. Next, we show the reverse inequality ≥. To this end, set Ā := A 1/2 and B := B 1/2 . Then for any V ∈ O(d), we successively compute</p><formula xml:id="formula_150" coords="27,171.60,243.82,363.39,26.23">W 2 (A, V BV ⊤ ) = inf U ∈O(d) Ā − (V BV ⊤ )U 2 F (<label>43</label></formula><formula xml:id="formula_151" coords="27,244.68,245.28,295.12,159.07">) = Ā 2 + B 2 − 2 sup U ∈O(d) Ā, V B(V ⊤ U ) (44) = Ā 2 + B 2 − 2 sup Z∈O(d) Ā, V BZ ⊤ (45) ≥ Ā 2 + B 2 − 2 σ( Ā), σ( B) (46) = λ( Ā) 2 + λ( B) 2 − 2 λ( Ā), λ( B) (47) = λ( Ā) − λ( B) 2 2 = λ(A) − λ(B) 2 2 , (<label>48</label></formula><formula xml:id="formula_152" coords="27,534.99,383.16,4.81,12.21">)</formula><p>where <ref type="bibr" coords="27,102.84,407.04,19.37,12.21" target="#b42">(43)</ref> follows from Lemma A.9, the estimate (44) follows from expanding the Frobenius norm, (45) uses the variable substitution Z = V ⊤ U , the estimate <ref type="bibr" coords="27,354.23,420.60,19.37,12.21" target="#b45">(46)</ref> follows from von Neumann's trace inequality (Lemma A.8), and (47) uses the fact that eigenvalues and singular values coincide for PSD matrices. Taking the infimum over V ∈ O(d) shows the claimed inequality ≥ in <ref type="bibr" coords="27,480.85,447.72,17.88,12.21" target="#b40">(41)</ref>.</p><p>Next, the fact that any matrix in ( <ref type="formula" coords="27,254.58,461.28,9.67,12.21" target="#formula_147">42</ref>) is a nearest point of O(B) to A follows directly from the expression <ref type="bibr" coords="27,123.84,474.84,17.98,12.21" target="#b40">(41)</ref>. To see the converse, observe that V BV ∈ O(B) is the closest point to A if and only if the chain of inequalities ( <ref type="formula" coords="27,205.38,488.40,9.42,12.21" target="#formula_150">43</ref>)-( <ref type="formula" coords="27,228.92,488.40,9.42,12.21" target="#formula_151">48</ref>) holds as equalities. Since the only inequality appears in <ref type="bibr" coords="27,519.17,488.40,17.84,12.21" target="#b45">(46)</ref>, applying Lemma A.8 we see that equality holds if and only if there exist matrices</p><formula xml:id="formula_153" coords="27,72.00,503.26,469.24,41.85">M 1 , M 2 , Z ∈ O(d) such that Ā = M 1 Diag(λ( Ā))M ⊤ 2 and V BZ ⊤ = M 1 Diag(λ( B))M ⊤ 2 .</formula><p>In particular, multiplying each equation by its transpose yields the expressions</p><formula xml:id="formula_154" coords="27,72.00,564.02,391.71,31.75">A = M 1 Diag(λ(A))M ⊤ 1 and V BV ⊤ = M 1 Diag(λ(B))M ⊤ 1 , thereby concluding the proof.</formula><p>We are now ready to complete the proof of Theorem A.7 in the Gaussian setting. In the proof, we will use the basic fact that for any vectors v, w ∈ R d , the inequality holds:</p><formula xml:id="formula_155" coords="27,255.12,637.22,279.87,20.89">v ↑ − w ↑ 2 ≤ v − w 2 , (<label>49</label></formula><formula xml:id="formula_156" coords="27,534.99,638.16,4.81,12.21">)</formula><p>where v ↑ and w ↑ are the vectors obtained by permuting the coordinates of v and w to be nonincreasing.</p><p>Theorem A.12 (Envelope and prox-map in Gaussian space). Consider a symmetric function f : R d + → R ∪ {+∞} and define the function on PSD matrices F : S d + → R ∪ {+∞} by setting</p><formula xml:id="formula_157" coords="28,72.00,74.74,81.40,10.91">F (A) = f (λ (A))</formula><p>. Then, equality holds:</p><formula xml:id="formula_158" coords="28,229.80,90.98,152.31,21.01">F ρ (A) = f ρ (λ(A)) ∀A ∈ S d + .</formula><p>Moreover, the following expression holds:</p><formula xml:id="formula_159" coords="28,109.68,128.06,392.55,21.01">prox ρF (A) = {U Diag(v)U ⊤ : v ∈ prox ρf (λ(A)), A = U Diag(λ(A))U ⊤ , U ∈ O(d)}.</formula><p>Proof. For any matrix A ∈ S d + , we successively compute</p><formula xml:id="formula_160" coords="28,191.16,165.50,348.65,157.85">F ρ (A) = inf B 0 F (A) + 1 2ρ W 2 2 (A, B) = inf v≥0 inf B∈O(Diag(v)) f (λ(B)) + 1 2ρ W 2 2 (A, B) = inf v≥0 f (v) + 1 2ρ inf B∈O(Diag(v)) W 2 2 (A, B) = inf v≥0 f (v) + 1 2ρ W 2 2 (A, O(Diag(v))) = inf v≥0 f (v) + 1 2ρ λ(A) − √ v ↑ 2 2 (50) = inf v≥0 f (v) + 1 2ρ W 2 2 (λ(A), v) (51) = f ρ (λ(A)),</formula><p>where We now move on to establishing Theorem A.7 in full generality, i.e. outside the Gaussian setting. We begin by extending Lemma A.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma A.13 (Distance to orbit: general case). Fix a matrix B ∈ S d</head><p>+ and define the set of measures M := {ν ∈ P 2 : Σ ν ∈ O(B)} . Then any zero-mean measure µ ∈ P 2 satisfies:</p><formula xml:id="formula_162" coords="28,237.60,485.42,302.20,21.01">W 2 2 (µ, M) = W 2 2 (Σ µ , O(B)).<label>(52)</label></formula><p>Proof. We suppose first that Σ µ is positive definite. Observe now</p><formula xml:id="formula_163" coords="28,157.20,521.78,297.51,45.13">W 2 (µ, M) = inf π∈Π(µ,ν), ν∈M E π x − y 2 2 ≥ inf E[xx ⊤ ]=Σµ, E[yy ⊤ ]∈O(B) E x − y 2 2 = d 2 (Σ µ , O(B)),</formula><p>where the inequality follows from Lemma A.10. To see the reverse inequality, consider an eigenvalue decomposition</p><formula xml:id="formula_164" coords="28,72.00,583.78,467.97,137.25">Σ µ = V Diag(λ(Σ µ ))V ⊤ for some V ∈ O(d). Define now the matrix B := V Diag(λ(B))V ⊤ and set T := B1/2 Σ −1/2 µ . Then, clearly the measure ν := T # µ satisfies E z∼ν [zz ⊤ ] = T E µ [xx ⊤ ]T ⊤ = T Σ µ T ⊤ = B and therefore T # µ lies in M. Thus, we conclude W 2 2 (µ, M) ≤ W 2 2 (µ, ν) = E µ x − T x 2 2 = E µ x 2 − 2 E µ tr(T xx ⊤ ) + E µ tr(T ⊤ T xx ⊤ ) = tr(Σ µ ) − 2 tr( B1/2 Σ 1/2 µ ) + tr( B) = d i=1 λ i (Σ µ ) − λ i (B) 2 ,</formula><p>Using Lemma A.11 yields the claimed expression <ref type="bibr" coords="29,309.36,73.44,17.98,12.21" target="#b51">(52)</ref>. Finally, if Σ µ is not positive definite, then µ is almost surely supported on some subspace L. We now define the perturbed distribution µ t = µ × g t where g t is a zero-mean Gaussian distribution supported on L ⊥ with covariance t</p><formula xml:id="formula_165" coords="29,71.64,114.12,469.83,33.51">• I. Then, by Lemma A.2, W 2 (µ t , µ) = W 2 (µ t , (P L ) # µ t ) → 0, which in turn implies E µt xx ⊤ → E µ xx ⊤ . From (52) we have W 2 2 (µ t , M) = W 2 2 (E µt xx ⊤ , O(B)</formula><p>). Letting t go to zero, we deduce the desired equality</p><formula xml:id="formula_166" coords="29,321.72,140.78,125.42,20.41">W 2 2 (µ, M) = W 2 2 (Σ µ , O(B)</formula><p>). The proof of Theorem A.7 now proceeds in exactly the same way as that of Theorem A.12, with Lemma A.13 being used instead of Lemma A.11.</p><p>Proof of Theorem A.7. The argument is essentially the same as in Theorem A.12. We detail it here for completeness. Define the matrix A := Σ µ . We successively compute</p><formula xml:id="formula_167" coords="29,195.24,230.90,344.56,92.29">F ρ (µ) = inf ν∈P 2 F (ν) + 1 2ρ W 2 2 (µ, ν) = inf u≥0 inf ν: Σν ∈O(Diag(u)) f (u) + 1 2ρ W 2 2 (µ, ν) = inf u≥0 f (u) + 1 2ρ inf ν: Σν ∈O(Diag(u)) W 2 2 (µ, ν) = inf u≥0 f (u) + 1 2ρ W 2 2 (A, O(Diag(u)))<label>(53)</label></formula><formula xml:id="formula_168" coords="29,225.12,320.50,314.68,50.49">= inf v≥0 f (v) + 1 2ρ λ(A) − √ u ↑ 2 2 (54) = inf u≥0 f (u) + 1 2ρ W 2 2 (λ(A), u)<label>(55)</label></formula><formula xml:id="formula_169" coords="29,225.12,376.66,56.07,11.85">= f ρ (λ(A)),</formula><p>where (53) follows from Lemma A.13, the estimate (54) follows from Lemma A.11, and (55) follows from <ref type="bibr" coords="29,97.80,407.40,17.98,12.21" target="#b48">(49)</ref>. This completes the proof.</p><p>Connection to existing literature. The results presented in this section have close analogues in the existing literature in matrix analysis and optimization. Namely a function F : S d → R ∪ {+∞} is called orthogonally invariant (or spectral) if the equality holds:</p><formula xml:id="formula_170" coords="29,200.40,480.86,211.11,21.01">F (U XU ⊤ ) = F (X) ∀X ∈ S d , U ∈ O(d).</formula><p>Evidently such functions are fully described by their restriction to diagonal matrices. More precisely, a function F is orthogonally invariant if and only if there exists a symmetric function f :</p><formula xml:id="formula_171" coords="29,72.00,513.50,467.99,33.97">R d → R ∪ {+∞} satisfying F (X) = f (λ(X)).</formula><p>A pervasive theme in the study of such functions is that various variational properties of the permutation-invariant function f are inherited by the induced spectral function F = f • λ; see e.g. <ref type="bibr" coords="29,250.07,554.64,16.97,12.21" target="#b21">[22,</ref><ref type="bibr" coords="29,270.84,554.64,13.95,12.21" target="#b22">23,</ref><ref type="bibr" coords="29,288.72,554.64,13.95,12.21" target="#b24">25,</ref><ref type="bibr" coords="29,306.48,554.64,13.95,12.21" target="#b39">40,</ref><ref type="bibr" coords="29,324.24,554.64,13.95,12.21" target="#b40">41,</ref><ref type="bibr" coords="29,342.00,554.64,13.95,12.21" target="#b41">42,</ref><ref type="bibr" coords="29,359.88,554.64,13.83,12.21" target="#b59">60,</ref><ref type="bibr" coords="29,377.64,554.64,12.72,12.21" target="#b71">72]</ref>. For example, f convex if and only if F is convex <ref type="bibr" coords="29,163.32,568.20,16.95,12.21" target="#b24">[25,</ref><ref type="bibr" coords="29,183.60,568.20,12.72,12.21" target="#b39">40]</ref>, <ref type="bibr" coords="29,423.83,568.20,16.96,12.21" target="#b59">[60,</ref><ref type="bibr" coords="29,444.12,568.20,13.95,12.21" target="#b62">63,</ref><ref type="bibr" coords="29,461.40,568.20,12.72,12.21" target="#b71">72]</ref>, and so forth.</p><formula xml:id="formula_172" coords="29,204.00,567.74,216.41,12.67">f is C p -smooth if and only of F is C p -smooth</formula><p>A useful result in this area is the expression for the Moreau envelope obtained in <ref type="bibr" coords="29,462.15,581.76,16.93,12.21" target="#b21">[22,</ref><ref type="bibr" coords="29,482.76,581.76,13.95,12.21" target="#b32">33]</ref> </p><formula xml:id="formula_173" coords="29,228.36,599.30,306.63,20.89">F ρ (X) = f ρ (λ(X)) ∀X ∈ S d . (<label>56</label></formula><formula xml:id="formula_174" coords="29,534.99,600.24,4.81,12.21">)</formula><p>For example, as explained in <ref type="bibr" coords="29,216.14,618.84,15.95,12.21" target="#b32">[33]</ref>, it readily yields expressions relating generalized derivatives of f and F . Crucially, in (56) the Moreau envelope F ρ is computed with respect to the Frobenius norm on S d and the Moreau envelope f ρ is computed with respect to the ℓ 2 norm on R d . Thus the results presented in the section extend this circle of ideas to functions defined on the Wasserstein space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs from Section 2 B.1 Proof of Proposition 2.1</head><p>Suppose otherwise that there exists a sequence</p><formula xml:id="formula_175" coords="30,294.60,118.34,247.47,20.41">ν i ∈ Q ′ with W 2 (ν i , µ) ≤ r satisfying REG(ν i ) → ∞.</formula><p>Then from <ref type="bibr" coords="30,127.33,132.36,13.96,12.21" target="#b6">(7)</ref> we deduce W 2 (ν i , E) = RSE(ν i ) → 0. Subsequently, using the triangle inequality yields</p><formula xml:id="formula_176" coords="30,101.76,147.22,208.59,18.65">RSE(µ) = W 2 (µ, E) ≤ W 2 (ν i , µ) + W 2 (ν i , E).</formula><p>Letting i tend to infinity, we deduce W 2 (µ, E) ≤ r, which is a contradiction. Thus no such sequence ν i exists and (8) holds for some M &gt; 0. Suppose now that for some q &gt; 0, the inequality c ≥ REG(ν) • RSE(ν) q holds. Then, similarly as above, the triangle inequality for any ν ∈ Q ′ \ E yields</p><formula xml:id="formula_177" coords="30,117.36,204.14,377.31,20.89">RSE(µ) = W 2 (µ, E) ≤ W 2 (ν, µ) + W 2 (ν, E) ≤ r + RSE(ν) ≤ r + (c/REG(ν)) 1/q .</formula><p>Rearranging yields the estimate REG(ν) ≤ c(RSE(µ) − r) −1/q , thereby completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Theorem 2.2</head><p>In this section, we prove Theorem 2.2, or rather a stronger version thereof. To this end, we fix a C 1 -smooth map F : R d → S k + satisfying that it is integrable with respect to any measure µ ∈ P 2 . We define the function J : P 2 → R by setting</p><formula xml:id="formula_178" coords="30,251.28,320.38,109.35,18.65">J (µ) = λ min (E µ F (x)).</formula><p>The differential of F will be denoted by DF (x) : R d → S k , while the symbol DF (x) * : S k → R d will denote the adjoint linear map of DF (x). We further assume that there exists a constant L &gt; 0 such that DF (x) op ≤ L(1 + x ) for all x.</p><p>In order to simplify notation, for any matrix A ∈ S k , we let E k (A) denote the set of all unit eigenvectors of A corresponding to the minimal eigenvalue λ min (A). We will also use the elementary fact that λ min is a concave function on S k and its supdifferential at any matrix A is the set</p><formula xml:id="formula_179" coords="30,219.72,422.90,172.35,20.89">∂λ min (A) = conv{uu ⊤ : u ∈ E k (A)}.</formula><p>See for example [9, Corollary 5.2.3, Corollary 5.2.4 (iii)]. Abusing notation, we will set E k (µ) := E k (E µ F (x)) for any measure µ ∈ P 2 .</p><p>The proof of Theorem 2.2 will be subdivided into two parts, corresponding to the two inequalities in <ref type="bibr" coords="30,84.01,483.00,17.88,12.21" target="#b10">(11)</ref>. We begin by establishing the first inequality REG(µ) q 2 −1 RSE(µ). The proof amounts to simply applying the fundamental theorem of calculus to the function J along a geodesic µ t joining a measure µ to its nearest point in E. This is the content of the following theorem.</p><p>Theorem B.1 (Small distance implies small value). Fix a measure µ ∈ P 2 , constants c, ε &gt; 0, and a power q ∈ [0, 1). Suppose that for all measures ν satisfying W 2 (ν, [J = 0]) ≤ W 2 (µ, [J = 0]) + ε, the estimate holds:</p><formula xml:id="formula_180" coords="30,207.96,574.34,192.18,21.03">min u∈E k (ν) E ν DF (x) * [uu ⊤ ] 2 2 ≤ c • J (ν) 2q</formula><p>. Then, the inequality holds:</p><formula xml:id="formula_181" coords="30,220.80,610.94,170.43,21.01">W 2 (µ, [J = 0]) ≥ 1 (1−q) √ c • J (µ) 1−q .</formula><p>Proof. Fix a measure ν ∈ [J = 0] satisfying W 2 (µ, ν) ≤ W 2 (µ, [J = 0]) + ε and let π ∈ Π(ν, µ) be an optimal transport plan between ν and µ. Define the functions π t (x, y) = (1 − t)x + ty. Then the curve µ t = (π t ) # µ is a constant speed geodesic between µ and ν [59, Theorem 5.27]:</p><formula xml:id="formula_182" coords="30,195.36,683.38,221.07,18.65">W 2 (µ t , µ s ) = |t − s| • W 2 (µ, ν) ∀t, s ∈ [0, 1].</formula><p>Define the curve of matrices γ(t) = E µt F (x). We would like to now compute γ(t) by exchanging differentiation and integration in the expression:</p><formula xml:id="formula_183" coords="31,228.00,104.02,306.99,25.97">γ(t) = d dt E (x,y)∼π F ((1 − t)x + ty). (<label>57</label></formula><formula xml:id="formula_184" coords="31,534.99,110.04,4.81,12.21">)</formula><p>To this end, we bound the derivative of the integrand:</p><formula xml:id="formula_185" coords="31,167.16,154.06,282.99,18.65">DF ((1 − t)x + ty)[y − x] op ≤ L(1 + x 2 + y 2 ) y − x 2 .</formula><p>Applying Hölder's inequality, we see that the right side is π-integrable. Therefore, exchanging integration and differentiation in (57) yields the expression γ(t) = E (x,y)∼π DF ((1</p><formula xml:id="formula_186" coords="31,459.36,186.10,82.83,18.65">− t)x + ty)[y − x].</formula><p>It is straightforward to see that γ is absolutely continuous and therefore using the subdifferential chain rule for concave functions <ref type="bibr" coords="31,227.07,211.92,16.92,12.21" target="#b10">[11,</ref><ref type="bibr" coords="31,247.68,211.92,55.82,12.21">Lemma 3.3,</ref><ref type="bibr" coords="31,307.44,211.92,4.58,12.21">p</ref>. 73], we deduce that for almost every t ∈ (0, 1) we have</p><formula xml:id="formula_187" coords="31,161.04,237.34,378.76,25.97">d dt J (µ t ) = d dt (λ min • γ)(t) = U t , γ(t) ∀U t ∈ ∂λ min (γ(t)).<label>(58)</label></formula><p>In particular, for each such t we may choose U t satisfying the running assumption <ref type="bibr" coords="31,210.51,276.48,17.95,12.21" target="#b57">(58)</ref>, we successively compute</p><formula xml:id="formula_188" coords="31,72.00,262.46,478.05,33.97">E µt DF (x) * [U t ] 2 2 ≤ c • J (µ t ) 2q . Continuing with</formula><formula xml:id="formula_189" coords="31,151.68,294.10,388.12,77.21">d dt J (µ t ) = E (x,y)∼π DF ((1 − t)x + ty) * [U t ], y − x ≤ E (x,y)∼π DF ((1 − t)x + ty) * [U t ] 2 • y − x 2 ≤ E (x,y)∼π DF ((1 − t)x + ty) * [U t ] 2 2 • E (x,y)∼π y − x 2<label>(59)</label></formula><formula xml:id="formula_190" coords="31,193.56,369.94,117.63,26.93">≤ √ c • W 2 (µ, ν) • J (µ t ) q ,</formula><p>where (59) follows from Hölder's inequality. Raising J (µ t ) to power 1 − q, we deduce</p><formula xml:id="formula_191" coords="31,172.44,412.06,163.96,27.06">d dt J (µ t ) 1−q ≤ √ c(1 − q) • W 2 (µ, ν)</formula><p>for a.e. t ∈ (0, 1).</p><p>Integrating both sides from t = 0 to t = 1, we conclude</p><formula xml:id="formula_192" coords="31,203.28,451.54,205.35,27.05">J (µ) 1−q − J (ν) 1−q ≤ √ c(1 − q) • W 2 (µ, ν).</formula><p>Taking into account the equality J (ν) = 0 and the estimate W 2 (µ, ν) ≤ W 2 (µ, [J = 0]) + ε, we may now let ε tend to zero 0 thereby completing the proof.</p><p>Next, we pass to the reverse inequality RSE(µ) REG(µ) q 1 −1 , which is a more substantive conclusion. The main tool we will use is the characterization of an "error bound property" using the slope. In what follows, for any real number r, the symbol r + = max{0, r} denotes its positive part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition B.2 (Slope)</head><p>. Consider a function f : X → R ∪ {+∞} defined on a metric space (X , d).</p><p>The slope of f at any point x with f (x) finite is defined by</p><formula xml:id="formula_193" coords="31,223.08,601.58,165.87,34.23">|∇f |(x) = lim sup x ′ →x (f (x) − f (x ′ )) + d(x, x ′ ) .</formula><p>Importantly, if a slope is large on a neighborhood, then the function must decrease significantly. This is the content of the following theorem; see <ref type="bibr" coords="31,301.93,646.56,16.95,12.21" target="#b36">[37,</ref><ref type="bibr" coords="31,322.20,646.56,66.85,12.21">Basic Lemma,</ref><ref type="bibr" coords="31,392.38,646.56,50.44,12.21">Chapter 1]</ref>) or <ref type="bibr" coords="31,463.32,646.56,16.96,12.21" target="#b33">[34,</ref><ref type="bibr" coords="31,483.60,646.56,50.41,12.21">Lemma 2.5</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem B.3 (Decrease principle)</head><p>. Consider a lower semicontinuous function f : X → R ∪ {+∞} on a complete metric space (X , d). Fix a point x with f (x) finite, and suppose that there are constants α &lt; f (x) and r, κ &gt; 0 so that the implication holds:</p><formula xml:id="formula_194" coords="31,157.08,711.82,297.87,18.65">α &lt; f (u) ≤ f (x) and d(u, x) ≤ r =⇒ |∇f |(u) ≥ κ.</formula><p>If in addition the inequality f (x) − α &lt; κr is valid, then the estimate holds:</p><formula xml:id="formula_195" coords="32,233.64,90.98,144.63,21.01">d(x, [f ≤ α]) ≤ κ −1 (f (x) − α).</formula><p>We will apply this theorem to the function J (µ) 1−q . The key step therefore is to compute the slope of J . This is the content of the following lemma. Lemma B.4 (Slope computation). Suppose that F satisfies DF (x) op ≤ L(1 + x 2 ) for all x ∈ R d , where L is some constant. Then, for any measure µ ∈ P 2 , the estimate holds:</p><formula xml:id="formula_196" coords="32,208.32,183.02,195.39,22.47">|∇J |(µ) ≥ sup u∈E k (µ) E µ DF (x) * [uu ⊤ ] 2 2 .</formula><p>Proof. We begin by writing J (µ) = (λ min • G)(µ), where we define the map G(µ) := E µ F (x).</p><p>Next, fix a measure µ ∈ P 2 and a matrix U ∈ ∂λ min (G(µ)), and define the transport map</p><formula xml:id="formula_197" coords="32,72.00,227.98,467.97,25.39">T (x) = x−DF (x) * [U ].</formula><p>Clearly, we may assume that DF (x) * [U ] is not µ-almost surely zero, since otherwise the theorem holds trivially for U = uu ⊤ . Observe that I − T is square µ-integrable since</p><formula xml:id="formula_198" coords="32,139.20,271.34,333.63,20.89">E µ x − T (x) 2 2 = E µ DF (x) * [U ] 2 2 ≤ L 2 • U 2 F • E µ (1 + x 2 ) 2 &lt; ∞.</formula><p>Define now the curve γ : [0, 1) → P 2 by setting γ(t) = (I + t(T − I)) # µ. Note from (34), we have</p><formula xml:id="formula_199" coords="32,221.28,308.42,313.71,20.89">W 2 2 (γ(t), γ(0)) ≤ t 2 E µ x − T (x) 2 2 . (<label>60</label></formula><formula xml:id="formula_200" coords="32,534.99,309.36,4.81,12.21">)</formula><p>Next, from concavity of λ min we deduce</p><formula xml:id="formula_201" coords="32,189.36,346.44,345.63,19.95">J (γ(t)) − J (γ(0)) ≤ U, (G • γ)(t) − (G • γ)(0) . (<label>61</label></formula><formula xml:id="formula_202" coords="32,534.99,346.44,4.81,12.21">)</formula><p>We would like to compute d dt U, G•γ(t) by exchanging integration/differentiation in the expression:</p><formula xml:id="formula_203" coords="32,193.92,383.62,341.07,25.97">d dt U, G • γ(t) = d dt E µ U, F (x + t(T (x) − x)) . (<label>62</label></formula><formula xml:id="formula_204" coords="32,534.99,389.64,4.81,12.21">)</formula><p>To this end, we bound the derivative of the integrand uniformly in t:</p><formula xml:id="formula_205" coords="32,104.40,430.54,403.23,74.57">| U, DF (x + t(T (x) − x))[T (x) − x] | ≤ DF (x + t(T (x) − x))[T (x) − x] 2 ≤ L(1 + x + t(T (x) − x) 2 • T (x) − x 2 ) ≤ L(1 + x + t T (x) − x 2 ) T (x) − x 2 ≤ L T (x) − x 2 + L 2 x 2 2 + L + 2 2 x − T (x) 2 2 .</formula><p>Clearly, the right-side is µ-integrable and therefore by the dominated convergence theorem, we may exchange integration and differentiation in <ref type="bibr" coords="32,278.45,519.72,19.32,12.21" target="#b61">(62)</ref> yielding:</p><formula xml:id="formula_206" coords="32,145.68,537.22,389.31,26.09">d dt U, G • γ(t) | t=0 = E µ DF (x) * [U ], T (x) − x = − E µ T (x) − x 2 2 . (<label>63</label></formula><formula xml:id="formula_207" coords="32,534.99,543.36,4.81,12.21">)</formula><p>In particular, we deduce (J • γ)(t) &lt; (J • γ)(0) for all small t &gt; 0. Therefore, dividing (61) by W 2 (γ(t), γ(0)) and taking the limit as t → 0 yields</p><formula xml:id="formula_208" coords="32,182.76,596.74,357.04,128.57">|∇J |(µ) ≥ lim t→0 J (γ(0)) − J (γ(t)) W 2 (γ(0), γ(t)) ≥ lim t→0 J (γ(0)) − J (γ(t)) t E µ x − T (x) 2 2 (64) = V, lim t→0 (G • γ)(0) − (G • γ)(t) t E µ x − T (x) 2 2 = E µ T (x) − x 2 2 = E µ DF (x) * [U ] 2 2 , (<label>65</label></formula><formula xml:id="formula_209" coords="32,534.99,705.36,4.81,12.21">)</formula><p>where the estimate (64) follows from <ref type="bibr" coords="33,250.33,73.44,19.35,12.21" target="#b59">(60)</ref> and the estimate (65) follows from <ref type="bibr" coords="33,441.61,73.44,17.97,12.21" target="#b62">(63)</ref>.</p><p>Finally, combining the decrease principle (Theorem B.3) and the estimate on the slope of J (Lemma B.4) we arrive at the main result.</p><p>Theorem B.5 (Small value implies small distance). Suppose that F : R d → S d + satisfies DF (x) op ≤ L(1 + x 2 ) for all x ∈ R d , where L is some constant. Fix a constant c &gt; 0, a radius r &gt; 0, and a power q ∈ [0, 1). Suppose that for all measures ν ∈ [0 &lt; J ≤ J (µ)] ∩ B 2 (µ; r), the estimate holds:</p><formula xml:id="formula_210" coords="33,189.36,176.66,233.31,23.19">sup u∈E k (ν) E ν DF (x) * [uu ⊤ ] 2 2 ≥ c • λ min (E ν F (x)) 2q .</formula><p>Then, the inequality holds:</p><formula xml:id="formula_211" coords="33,220.80,218.42,170.43,21.01">W 2 (µ, [J = 0]) ≤ 1 (1−q) √ c • J (µ) 1−q ,</formula><p>as long as r is large enough so that J (µ) 1−q &lt; (1 − q)r √ c.</p><p>Proof. It follows immediately from [66, Theorem 6.9] and continuity of λ min (•) that the function J is continuous. Define the function G(ν) := J (ν) 1−q and note that the standard chain rule implies We first argue the inclusion ⊃. Observe that for any measure µ ∈ P • 2 with λ 1 (Σ µ ) = λ 2 (Σ µ ), the set of maximizers of <ref type="bibr" coords="33,172.07,451.92,19.49,12.21" target="#b11">(12)</ref> is the intersection of a sphere and the top eigenspace of Σ µ . Since none of the maximizers are isolated, they are unstable and therefore µ lies in E.</p><formula xml:id="formula_212" coords="33,71.64,291.22,333.04,51.29">|∇G|(ν) = (1 − q) • |∇J |(ν) J (ν) q ≥ (1 − q) √ c, whenever 0 &lt; G(ν) ≤ G(µ)</formula><p>To see the reverse inclusion ⊂, fix a measure µ ∈ P • 2 and suppose that the top two eigenvalues λ 1 and λ 2 of Σ µ are distinct. Then the normalized top eigenvector v of Σ µ is the unique maximizer of <ref type="bibr" coords="33,85.68,506.16,17.98,12.21" target="#b11">(12)</ref>. It remains to verify that v is a tilt-stable maximizer of <ref type="bibr" coords="33,395.64,506.16,17.98,12.21" target="#b11">(12)</ref>. To this end, define the Lagrangian function</p><formula xml:id="formula_213" coords="33,171.84,518.90,175.22,20.77">L(v, λ) = − 1 2 E µ x, v 2 + λ 2 ( v 2 − 1)</formula><p>. Then equalities hold:</p><formula xml:id="formula_214" coords="33,164.28,537.26,283.35,20.89">∇ v L(v, λ) = (λI − Σ µ )v and ∇ 2 vv L(v, λ) = λI − Σ µ .</formula><p>In particular, the first equation shows that the optimal Lagrange multiplier λ is λ 1 . Let V ∈ R d×(d−1) be a matrix with an eigenbasis for v ⊥ as its columns. Then an elementary computation yields</p><formula xml:id="formula_215" coords="33,143.88,598.94,395.92,32.93">min y∈S d−2 ∇ 2 vv L(v, λ)(V y), (V y) = min y∈S d−2 d i=2 (λ 1 − λ i )y 2 i = λ 1 − λ 2 &gt; 0,<label>(66)</label></formula><p>and therefore v is a tilt-stable maximizer of <ref type="bibr" coords="33,291.12,633.48,17.98,12.21" target="#b11">(12)</ref>. Moreover, the claimed equality REG(µ) −1 = λ 1 (Σ µ ) − λ 2 (Σ µ ) follows directly from (66), thereby completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Theorem 3.2</head><p>This follows directly by applying Theorem A.6 with G = E from Lemma 3.1 and G = {v ∈ R d + : v (1) = v (2) }, where v (i) denotes the i'th largest coordinate value of v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Lemma 3.3</head><p>Henceforth, fix a measure µ ∈ P • 2 and define the shorthand λ := λ(Σ µ ). Let's dispense first with the simple direction ⊃. To this end, suppose that equality λ q = λ q+1 holds. Then we may form two sets of orthonormal bases U := {u 1 , . . . , u q } and U ′ = {u 1 , . . . , u q−1 , u ′ q } with u q , u ′ q = 0, and which are contained in the span of the eigenspaces corresponding to the top q eigenvalues. We may further interpolate between the two bases with U t = {u 1 , . . . , tu q + (1 − t)u ′ q } for t ∈ (0, 1). The orthogonal projections onto the span of U t furnish a path of optimal solutions, which are therefore not tilt-stable. Thus µ lies in E, as claimed.</p><p>We now establish the reverse inclusion ⊂. Suppose therefore that λ q and λ q+1 are distinct. We begin by conveniently parameterizing the Grassmannian manifold Gr(q, d) as follows. Define the matrix A := I q 0 0 0 . Then using [6, Section 2.1] we may write Gr(q, d) as the orbit of A under conjugation by orthogonal matrices:</p><formula xml:id="formula_216" coords="34,230.04,262.10,151.83,21.01">Gr(q, d) = {U AU ⊤ : U ∈ O(d)}.</formula><p>Fix a skew symmetric matrix</p><formula xml:id="formula_217" coords="34,215.40,284.02,87.03,32.21">W := W 1 W 2 −W ⊤ 2 W 4</formula><p>and define the curve γ : R → Gr(q, d) by</p><formula xml:id="formula_218" coords="34,230.76,317.14,150.51,18.65">γ(t) = exp(−tW ) • A • exp(tW ).</formula><p>Differentiating the curve yields the expression</p><formula xml:id="formula_219" coords="34,226.08,354.70,161.43,26.49">γ(0) = AW − W A = 0 W 2 W ⊤ 2 0 .</formula><p>Moreover, [6, Section 2.3] shows that varying W among all skew-symmetric matrices yields the entire tangent space</p><formula xml:id="formula_220" coords="34,202.92,417.34,206.19,25.49">T Gr(q,d) (A) = 0 B B ⊤ 0 : B ∈ R q×(d−q) .</formula><p>Now without loss of generality, we may assume that Σ µ is diagonal, that is Σ µ = Diag(λ). Then clearly, R = A is the unique maximizer of the problem <ref type="bibr" coords="34,345.26,459.84,17.87,12.21" target="#b13">(14)</ref>. We now perform the second order expansion</p><formula xml:id="formula_221" coords="34,76.20,490.06,459.63,34.41">(f • γ)(t) = γ(t), Σ µ = f (A) + t AW − W A, Σ µ =0 +t 2 1 2 (AW 2 + W 2 A) − W AW, Σ µ + O(t 3 ).</formula><p>In particular, we deduce</p><formula xml:id="formula_222" coords="34,193.20,544.18,225.63,25.97">(f • γ) ′′ (0) = 1 2 (AW 2 + W 2 A) − W AW, Σ µ .</formula><p>Taking into account the definition of A, a quick computation show</p><formula xml:id="formula_223" coords="34,73.20,591.14,465.39,68.17">1 2 (AW 2 + W 2 A) − W AW = W 2 1 − W 2 W ⊤ 2 1 2 (W 2 W 4 + W 1 W 2 ) − 1 2 (W 2 W 4 + W 1 W 2 ) ⊤ 0 − W 2 1 W 1 W 2 −W ⊤ 2 W 1 −W ⊤ 2 W 2 = −W 2 W ⊤ 2 1 2 (W 2 W 4 − W 1 W 2 ) − 1 2 (W 2 W 4 − W 1 W 2 ) ⊤ W ⊤ 2 W 2 .</formula><p>Taking the trace product with the diagonal matrix Σ µ yields</p><formula xml:id="formula_224" coords="35,161.40,90.98,378.41,56.77">(f • γ) ′′ (0) = − W 2 W ⊤ 2 , Diag(λ 1:q ) + W ⊤ 2 W 2 , Diag(λ q+1:d ) ≤ − W 2 W ⊤ 2 , λ q I q + W ⊤ 2 W 2 , λ q+1 I d−q (67) = −(λ q − λ q+1 ) • W 2 2 F</formula><p>In particular, the covariant Hessian ∇ 2 M f (A) is negative definite on the tangent space T Gr(q,d) (A). Therefore, A is a tilt-stable maximizer of the problem, as we had to show. Moreover, setting W 2 = Diag(w) with w q = w q+1 = 1 and w i = 0 for i / ∈ {q, q + 1}, yields equality in <ref type="bibr" coords="35,466.94,173.40,17.96,12.21" target="#b66">(67)</ref>. Therefore we deduce REG(µ) −1 = λ q (Σ µ ) − λ q+1 (Σ µ ) as claimed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Proof of Theorem 3.4</head><p>This follows directly by applying Theorem A.6 with G = E from Lemma 3.3 and G = {v ∈ R d + : v (q) = v (q+1) }, where v (i) denotes the i'th largest coordinate value of v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proofs from Section 4 D.1 Proof of Lemma 4.1</head><p>For any µ ∈ Q, observe</p><formula xml:id="formula_225" coords="35,158.64,347.54,290.52,37.57">∇f (β) = E h ′ ( x, β ) − y x = E E h ′ ( x, β ) − y | x x = E (h ′ ( x, β ) − h ′ ( x, β ⋆ ))x</formula><p>Therefore, equality ∇f (β ⋆ ) = 0 holds for any distribution of x. Hence, β ⋆ is critical for the problem with zero Lagrange multipliers λ = 0. Differentiating again yields the expression for the Hessian</p><formula xml:id="formula_226" coords="35,207.00,414.74,327.99,20.89">H := ∇ 2 f (β ⋆ ) = σ −2 • E h ′′ ( x, β ⋆ )xx ⊤ . (<label>68</label></formula><formula xml:id="formula_227" coords="35,534.99,415.68,4.81,12.21">)</formula><p>Note that H is positive semidefinite since h ′′ &gt; 0. Consequently, the set of ill-conditioned distributions E corresponds to those distributions on x for which ker(H) nontrivially intersects T . Clearly, a vector v lies in ker(H) if and only if 0 = Hv, v , or equivalently 0 = E h ′′ ( x, β ⋆ ) x, v 2 . Taking into account the assumption h ′′ &gt; 0, this occurs precisely when v lies in the nullspace of Σ µ . Thus E consists of all measures µ ∈ Q satisfying T ∩ ker (Σ µ ) = {0}. Finally, it follows directly from (68) that if for some α, β &gt; 0 the inequality α ≤ h ′′ ( x, β ⋆ ) ≤ β holds for µ-almost every x, then</p><formula xml:id="formula_228" coords="35,72.00,516.82,159.99,18.65">λ min (Σ µ | T ) ∈ λ min (Σ µ | T ) • [α, β],</formula><p>thereby completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Proof of Theorem 4.2</head><p>This follows directly from Lemma 4.1 and Theorem A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proofs from Section 5.1 E.1 Proof of Lemma 5.1</head><p>We begin by verifying the claim for the formulation <ref type="bibr" coords="35,322.26,645.12,17.93,12.21" target="#b20">(21)</ref>. To this end, a quick computation shows</p><formula xml:id="formula_229" coords="35,72.00,660.86,331.47,42.77">∇f (β ⋆ ) = 0 and ∇ 2 f (β ⋆ ) = E µ x, β ⋆ 2 xx ⊤ . Therefore we deduce λ min (∇ 2 f (β ⋆ )) = min v∈S d−1 E µ x, β ⋆ 2 x, v 2 .</formula><p>In particular, ∇ 2 f (β ⋆ ) if and only if the support of µ is contained in β ⊥ ⋆ ∪ v ⊥ almost surely.</p><p>Next, we verify the claim for the formulation <ref type="bibr" coords="36,320.91,73.44,17.95,12.21" target="#b19">(20)</ref>. To this end, let M denote the set of symmetric PSD rank one matrices:</p><formula xml:id="formula_230" coords="36,71.64,104.54,402.87,57.97">M = {M ∈ S d + : rank(M ) = 1}. A quick computation now yields ∇f (M ) = E M − M ⋆ , xx ⊤ and ∇ 2 f (M ⋆ )[∆, ∆] = E ∆, xx ⊤ 2 .</formula><p>In particular, equality ∇f (M ⋆ ) = 0 holds and therefore the optimal Lagrange multipliers λ ⋆ are zero. Hence the Hessian of the Lagrangian ∇ 2 L(M ⋆ , λ ⋆ ) coincides with ∇ 2 f (M ⋆ ). Classically, if we form the factorization M ⋆ = β ⋆ β ⊤ ⋆ , then the tangent space to M at M ⋆ can be written as</p><formula xml:id="formula_231" coords="36,72.00,205.82,396.78,76.45">T = {β ⋆ v ⊤ + vβ ⊤ ⋆ : v ∈ R d }. Consequently, for any ∆ = β ⋆ v ⊤ + vβ ⊤ ⋆ we compute ∇ 2 f (M ⋆ )[∆, ∆] = E ∆, xx ⊤ 2 = 4 E β ⋆ , x 2 v, x 2 . Note that ∆ 2 F = 2(β ⊤ ⋆ v) 2 + 2 β ⋆ 2 v 2 and therefore 2 F / β ⋆ 2 v 2 ∈ [2, 4]</formula><p>. Therefore ∆ is nonzero as long as w is nonzero. We therefore again deduce that </p><formula xml:id="formula_232" coords="36,72.00,275.42,468.04,33.97">∇ 2 f (M ⋆ )[∆, ∆] = 0 of some nonzero ∆ ∈ T M (M ⋆ ) if and only if the support of µ is contained in β ⊥ ⋆ ∪ v ⊥ almost</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Proof of Theorem 5.2</head><p>Define the set</p><formula xml:id="formula_233" coords="36,72.00,352.10,467.99,55.45">K v = {β ⋆ } ⊥ ∪ {v} ⊥ for each unit vector v ∈ S d−1 and define the function h : S d−1 → [0, ∞). h(v) = E x∼µ dist(x, K v ) 2 = E x∼µ x, β⋆ β⋆ 2 ∧ x, v 2</formula><p>Fatou's lemma directly implies that h is lower semicontinuous and therefore admits a minimizer u ⋆ ∈ argmin u∈S d−1 h(u). Let s : R d → R d be a Borel measurable selection of the metric projection P Ku ⋆ . Define the pushforward measure ν = s # µ. Clearly ν lies in E and hence</p><formula xml:id="formula_234" coords="36,180.96,453.14,358.84,21.01">inf ν∈E W 2 2 (µ, ν) ≤ W 2 2 (µ, ν) = E x∼µ dist(x, K u⋆ ) 2 = h(u ⋆ )<label>(69)</label></formula><p>with the first equality holding by <ref type="bibr" coords="36,237.17,478.20,17.94,12.21" target="#b35">(36)</ref>. On the other hand, for any ν ∈ E there exists w ∈ S d−1 such that supp(ν) ⊂ K w and hence</p><formula xml:id="formula_235" coords="36,201.00,509.30,210.04,21.01">h(u ⋆ ) ≤ h(w) = E x∼µ dist(x, K w ) 2 ≤ W 2 2 (µ, ν)</formula><p>with the last inequality holding by <ref type="bibr" coords="36,238.25,533.04,17.84,12.21" target="#b34">(35)</ref>. Taking the infimum over ν ∈ E, we deduce that (69) holds with equality, thereby completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Proof of Theorem 5.3</head><p>We will need the following two elementary lemmas.</p><formula xml:id="formula_236" coords="36,72.00,616.46,395.79,14.69">Lemma E.1. If (y 1 , y 2 ) is a centered Gaussian vector with E y 2 1 = σ 2 1 , E y 2 2 = σ 2 2</formula><p>, and E y 1 y 2 = ρσ 1 σ 2 , then the equations hold: Proof. Taking the derivative of ψ(t) and using the fundamental theorem of calculus we get</p><formula xml:id="formula_237" coords="36,201.24,647.14,209.55,42.09">E |y 1 y 2 | = 2 π 1 − ρ 2 + ρ arcsin(ρ) σ 1 σ 2 , E(y 1 y 2 ) 2 = (1 + 2ρ 2 )σ</formula><formula xml:id="formula_238" coords="37,99.24,181.70,440.57,32.69">ψ(t) = 1 + t 0 arcsin(s)ds = 1 + t 0 ∞ n=0 2n n s 2n+1 4 n (2n + 1) ds = 1 + ∞ n=0 2n n t 2n+2 4 n (2n + 2) ,<label>(70)</label></formula><p>where the second equality follows by taking the Taylor expansion of arcsin(s). Factorizing a t 2 from the series yields</p><formula xml:id="formula_239" coords="37,92.52,249.14,426.87,32.69">ψ(t) = 1 + t 2 ∞ n=0 2n n |t| n+1 4 n (2n + 2) ≤ 1 + t 2 ∞ n=0 2n n 1 4 n (2n + 2) = 1 + π 2 − 1 t 2 = φ(t),</formula><p>where the inequality follows since |t| ≤ 1 and the second to last equality evaluates (70) at t = 1. This completes the proof.</p><p>With these two lemmas at hand, we start the proof of Theorem 5.3. We will first verify <ref type="bibr" coords="37,519.74,321.60,17.87,12.21" target="#b23">(24)</ref>. To this end, notice that we may write g Σ (u, v) = E(y 1 y 2 ) 2 , where we define the random variables y 1 = x, u and y 2 = x, v . We compute E y 2 1 = Σu, u , E y 2 2 = Σv, v , and E y 1 y 2 = Σu, v . Therefore, Lemma E.1 directly implies</p><formula xml:id="formula_240" coords="37,119.40,379.82,372.63,20.89">g Σ (u, v) = Σu, u Σv, v + 2 Σu, v 2 = Σ 1/2 u 2 2 • Σ 1/2 v 2 2 + 2 Σ 1/2 u, Σ 1/2 v 2</formula><p>Consequently, applying the Cauchy-Schwarz inequality yields the two sided bound</p><formula xml:id="formula_241" coords="37,180.84,416.78,255.75,21.01">Σ 1/2 u 2 2 • Σ 1/2 v 2 2 ≤ g Σ (u, v) ≤ 3 Σ 1/2 u 2 2 • Σ 1/2 v 2 2 .</formula><p>Taking the infimum over v ∈ S d−1 completes the proof of <ref type="bibr" coords="37,347.29,436.32,17.97,12.21" target="#b23">(24)</ref>.</p><p>Next, we verify <ref type="bibr" coords="37,165.01,449.88,17.97,12.21" target="#b22">(23)</ref>. Notice that the upper bound follows since</p><formula xml:id="formula_242" coords="37,119.04,469.34,373.83,20.89">min v∈S d−1 h Σ (u, v) = min v∈S d−1 E min{ x, u 2 , x, v 2 } ≤ min v∈S d−1 E x, v 2 = λ min (Σ).</formula><p>To prove the lower bound we will show the slightly stronger statement that for any u, v ∈ S d−1 , 1 − 2 π λ min (Σ) ≤ h Σ (u, v). Recall that min{a, b} = a+b 2 − |a−b| 2 for any a, b ∈ R. Therefore, we can write</p><formula xml:id="formula_243" coords="37,164.04,554.54,276.99,61.45">h Σ (u, v) = E x∼N (0,Σ) x, u 2 + x, v 2 2 − | x, u 2 − x, v 2 | 2 = u ⊤ Σu + v ⊤ Σv 2 − E x∼N (0,Σ) | x, u 2 − x, v 2 | 2 .</formula><p>Next, let's compute</p><formula xml:id="formula_244" coords="37,206.64,631.58,198.63,28.38">| x, u 2 − x, v 2 | = | x, u + v y 1 x, u − v y 2 |.</formula><p>Then we get σ 2 1 := E y 2 1 = Σ 1/2 (u + v) 2 and σ 2 2 := E y 2 2 = Σ 1/2 (u − v) 2 , and</p><formula xml:id="formula_245" coords="37,220.32,681.14,171.39,20.89">E y 1 y 2 = (u + v) ⊤ Σ(u − v) = ρσ 1 σ 2 ,</formula><p>where ρ := Σ 1/2 (u+v),Σ 1/2 (u−v) Σ 1/2 (u+v) Σ 1/2 (u−v) . Thus, by Lemma E.1 we get</p><formula xml:id="formula_246" coords="38,202.56,90.27,206.91,55.00">E |y 1 y 2 | = 2 π     1 − ρ 2 + ρ arcsin(ρ) =:ψ(ρ)     σ 1 σ 2 .</formula><p>Therefore, after the relabeling û = Σ 1/2 u and v = Σ 1/2 v we deduce</p><formula xml:id="formula_247" coords="38,193.80,169.46,224.31,27.73">h Σ (u, v) = û 2 + v 2 2 − ψ(ρ) π û + v û − v .</formula><p>By Lemma E.2 we have that</p><formula xml:id="formula_248" coords="38,157.92,215.06,377.07,27.73">h Σ (u, v) ≥ û 2 + v 2 2 − 1 2 − 1 π ρ 2 + 1 π û + v û − v . (<label>71</label></formula><formula xml:id="formula_249" coords="38,534.99,222.84,4.81,12.21">)</formula><p>In what follows we upper bound the second term on the right-hand-sight of this inequality. Without loss of generality we assume that v ≤ û . By definition ρ is equal to cos α where α is the angle between û + v and û − v. Thus, by the cosine law we have that</p><formula xml:id="formula_250" coords="38,77.40,290.18,462.40,21.01">2ρ û + v û − v = 2 cos(α) û + v û − v = û + v 2 + û − v 2 − 4 v 2 = 2( û 2 − v 2 ),<label>(72)</label></formula><p>where the last equality follows by the parallelogram law. Similarly, using Young's inequality</p><formula xml:id="formula_251" coords="38,182.88,326.42,352.11,27.85">û + v û − v ≤ û + v 2 + û − v 2 2 ≤ û 2 + v 2 . (<label>73</label></formula><formula xml:id="formula_252" coords="38,534.99,334.32,4.81,12.21">)</formula><p>Then, applying ( <ref type="formula" coords="38,152.58,355.92,9.67,12.21" target="#formula_250">72</ref>) and (73) yields</p><formula xml:id="formula_253" coords="38,122.88,373.30,375.51,81.77">1 2 − 1 π ρ 2 + 1 π û + v û − v ≤ 1 2 − 1 π ρ û 2 − v 2 + 1 π û 2 + v 2 ≤ 1 π + ρ 2 − ρ π û 2 + 1 π − ρ 2 + ρ π v 2 ≤ 1 2 û 2 + 2 π − 1 2 v 2</formula><p>where the last inequality follows by adding and subtracting (2/π − 1/2) to the coefficient of v and noting that v ≤ û . Combining this inequality with (71) yields</p><formula xml:id="formula_254" coords="38,154.68,489.34,302.55,26.10">h Σ (u, v) ≥ 1 − 2 π v 2 = 1 − 2 π v ⊤ Σv ≥ 1 − 2 π λ min (Σ),</formula><p>which proves the lower bound. A quick computation yields the expression ∇f (M ) = E M − M ⋆ , x 1 x ⊤ 2 . In particular, equality ∇f (M ⋆ ) = 0 holds and therefore the optimal Lagrange multipliers λ ⋆ are zero. Hence the Hessian of the Lagrangian ∇ 2 L(M ⋆ , λ ⋆ ) coincides with ∇ 2 f (M ⋆ ). We now successively compute</p><formula xml:id="formula_255" coords="39,204.96,90.98,146.07,21.01">∇ 2 f (M ⋆ )[∆, ∆] = E ∆, x 1 x ⊤ 2 2</formula><p>(74)</p><formula xml:id="formula_256" coords="39,281.28,109.10,253.71,89.45">= E x ⊤ 1 ∆(x 2 x ⊤ 2 )∆ ⊤ x 1 = E x 1 x ⊤ 1 ∆Σ 2 ∆ ⊤ x 1 = E x 1 tr(∆Σ 2 ∆ ⊤ x 1 x ⊤ 1 ) = tr(∆Σ 2 ∆ ⊤ Σ 1 ) = Σ 1/2 1 ∆Σ 1/2 2 2 F (<label>75</label></formula><formula xml:id="formula_257" coords="39,534.99,184.20,4.81,12.21">)</formula><p>≥ λ min (Σ 1 )λ min (Σ 2 ) ∆ 2 F . In particular, if both Σ 1 and Σ 2 are nonsingular, then µ × ν does not lie in E. We now leverage the tangent structure to get an upper bound and a better lower bound on the quadratic form ∇ 2 f (M ⋆ )[∆, ∆]. To this end, standard results (see e.g. [10, Section 7.5]) show that the tangent space to M at M ⋆ = β 1⋆ β ⊤ 2⋆ is given by</p><formula xml:id="formula_258" coords="39,72.00,278.54,384.99,39.49">T M (M ⋆ ) = aβ 1⋆ β ⊤ 2⋆ + uβ ⊤ 2⋆ + β 1⋆ v ⊤ : a ∈ R, u ∈ β ⊥ 1⋆ , v ∈ β ⊥ 2⋆ . Let ∆ = aβ 1⋆ β ⊤ 2⋆ + uβ ⊤ 2⋆ + β 1⋆ v ⊤ ∈ T M (M ⋆ ).</formula><p>Without loss of generality we might assume β 1⋆ = β 2⋆ = 1, since otherwise we can make the change of variables a ′ / β 1⋆ β 2⋆ ← a, v ′ / β 1 ← v, and u ′ / β 2 ← u. Thus, ∆ 2 F = a 2 + v 2 + u 2 . Observe that we may rewrite (75) as</p><formula xml:id="formula_259" coords="39,99.84,342.86,412.35,49.45">∇ 2 f (M ⋆ )[∆, ∆] = Σ 1/2 1 (aβ 1⋆ + u)β ⊤ 2⋆ + β 1⋆ v ⊤ Σ 1/2 2 2 F = Σ 1/2 2 ⊗ Σ 1/2 1 vec (aβ 1⋆ + u)β ⊤ 2⋆ + Σ 1/2 2 ⊗ Σ 1/2 1 vec β 1⋆ v ⊤ 2 F .</formula><p>To obtain a bound we leverage the following general claim. We use this constraint to define a relaxation of the problem. We introduce two new variables z and w, which intuitively play the role of x + y and x − y, respectively. Without loss of generality we can set x = y = 1 since we can divide both sides of (77) by x y and therefore from orthogonality of x and y we have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim</head><formula xml:id="formula_260" coords="40,72.00,488.02,471.03,112.65">∇ 2 f (M ⋆ )[∆, ∆] ≥ 2 κ (Σ 1 ) κ (Σ 2 ) + 1 ( β 1⋆ , Σ 1 β 1⋆ u, Σ 2 u + β 2⋆ , Σ 2 β 2⋆ aβ 1⋆ + v, Σ 1 (aβ 1⋆ + v) ) ≥ 2 κ (Σ 1 ) κ (Σ 2 ) + 1 β 1⋆ , Σ 1 β 1⋆ λ min (Σ 2 ) u 2 + β 2⋆ , Σ 2 β 2⋆ λ min (Σ 1 ) aβ 1⋆ + v 2 ≥ 2 κ (Σ 1 ) κ (Σ 2 ) + 1 min { β 1⋆ , Σ 1 β 1⋆ λ min (Σ 2 ) , β 2⋆ , Σ 2 β 2⋆ λ min (Σ 1 )}</formula><p>where the last holds since u 2 + aβ 1⋆ + v 2 = 1. This establishes the lower bound in <ref type="bibr" coords="40,488.06,605.28,17.87,12.21" target="#b25">(26)</ref>.</p><p>Next we establish the converse. For any ∆ = aβ 1⋆ β ⊤ 2⋆ + uβ ⊤ 2⋆ + β 1⋆ v ⊤ , from (74) we have</p><formula xml:id="formula_261" coords="40,123.36,636.38,411.63,21.01">∇ 2 f (M ⋆ )[∆, ∆] = E[a x 1 , β 1⋆ x 2 , β 2⋆ + x 1 , u x 2 , β 2⋆ + x 1 , β 1⋆ x 2 , v ] 2 . (<label>82</label></formula><formula xml:id="formula_262" coords="40,534.99,637.44,4.81,12.21">)</formula><p>Note the equality ∆ 2 F = a 2 β 1⋆ </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,86.19,447.78,172.29,10.90;12,85.35,466.43,214.05,13.15;12,305.19,467.72,21.40,18.65;12,331.71,467.72,193.42,10.91;12,86.31,479.53,187.20,13.59"><head>Theorem 4 . 2 :</head><label>42</label><figDesc>(RSE for QMLE) Consider a QMLE problem (17) and let D x ∈ Q be the distribution of x with covariance Σ := E[xx ⊤ ]. Then, the estimate holds:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="26,86.19,141.80,194.45,10.90;26,85.35,161.65,172.61,12.21;26,258.15,161.19,4.36,7.97;26,257.91,167.91,6.58,7.97;26,267.99,162.94,10.91,18.65;26,282.03,161.65,213.39,12.21;26,499.11,162.94,7.59,18.65;26,506.67,166.83,4.23,7.97;26,514.47,162.94,10.91,18.65;26,86.67,176.50,9.41,10.90;26,98.79,176.50,40.01,18.65;26,142.71,175.21,120.32,12.21;26,263.07,180.39,5.09,7.97;26,268.59,175.21,112.69,12.21"><head>Theorem A. 7 :</head><label>7</label><figDesc>(Diagonal reduction) Consider a symmetric function f : R d + → R∪{+∞} and define the spectral function F : P 2 → R ∪ {+∞} by setting F (µ) = f (λ (Σ µ )) . Then equality holds:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="28,128.04,328.68,411.79,12.21;28,72.00,341.78,230.29,13.59;28,302.40,348.38,9.68,7.97;28,313.80,342.24,72.85,12.21;28,402.36,342.24,137.58,12.21;28,72.00,357.10,21.25,10.91;28,93.36,361.94,8.38,7.97;28,103.20,355.80,183.51,13.15;28,288.48,357.10,58.25,18.65;28,351.24,355.80,188.75,12.21;28,72.00,369.98,444.76,13.59"><head></head><label></label><figDesc>follows from Lemma A.11 and (51) follows from (49). Next, observe from the chain of equalities that B = U Diag(v)U ⊤ lies in prox ρF (A) for some U O(d) if, and only if, v lies in prox ρf (λ(A)) and equality d 2 (A, B) = d 2 (A, O(Diag(v))) holds. Appealing to Lemma A.11, this equality holds if and only if we may write A = U Diag(λ(A))U ⊤ . Thus the proof is complete.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="33,214.32,322.56,63.16,13.15;33,282.96,323.86,8.49,18.65;33,296.88,322.56,144.78,12.21;33,446.88,323.86,6.49,18.65;33,459.12,322.56,80.95,12.21;33,72.00,337.42,30.54,10.91;33,105.00,337.42,8.49,18.65;33,115.92,337.42,9.52,10.91;33,125.40,329.62,9.09,18.65;33,134.40,336.12,105.51,12.21;33,88.92,358.68,298.11,12.21;33,72.00,392.44,11.66,13.64;33,99.85,392.44,153.99,13.64;33,72.00,418.95,150.86,11.37"><head>C Proofs from Section 3 C. 1</head><label>31</label><figDesc>and W 2 (ν, µ) ≤ r. Applying Theorem B.3 to G with α = 0 and κ = (1 − q) √ c completes the proof. Theorem 2.2 follows immediately from Theorems B.1 and B.5. Proof of Lemma 3.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="38,72.00,551.92,10.14,13.64;38,98.28,551.92,166.61,13.64;38,72.00,578.43,149.66,11.37;38,72.00,597.38,339.15,13.61;38,414.12,599.14,8.49,18.65;38,424.92,599.14,9.87,11.85;38,438.96,597.84,42.98,12.21;38,221.76,617.74,13.10,18.65;38,237.84,617.74,27.86,10.91;38,270.00,617.74,7.28,18.65;38,280.32,617.74,9.41,10.90;38,289.92,615.38,7.97,8.34;38,298.44,615.38,10.96,13.59;38,309.36,618.23,3.65,5.49;38,317.04,617.74,73.23,10.91"><head>F Proofs from Section 5. 2 F. 1 5 Set Σ 1 =</head><label>2151</label><figDesc>Proof of Lemma 5.E µ xx ⊤ and Σ 2 = E ν xx ⊤ . Define the manifold of rank one d 1 × d 2 matrices:M = M ∈ R d 1 ×d 2 : rank(A) = 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="40,247.56,207.58,6.24,10.91;40,257.04,207.58,8.49,18.65;40,268.80,207.58,5.35,10.91;40,279.96,205.82,4.23,7.97;40,289.92,206.28,78.43,12.21;40,374.16,205.82,4.23,7.97;40,384.00,206.28,155.98,12.21;40,72.00,221.14,7.91,18.65;40,84.12,221.14,8.49,10.91;40,95.88,221.14,18.15,18.65;40,115.80,221.14,12.28,10.91;40,131.64,221.14,3.03,18.65;40,143.40,221.14,5.07,10.91;40,154.32,219.38,4.23,7.97;40,162.60,221.14,24.97,10.91;40,193.20,219.38,4.23,7.97;40,201.48,219.84,44.11,12.21;40,249.36,221.14,7.78,18.65;40,257.16,221.14,25.17,11.85;40,285.72,221.14,18.88,18.65;40,306.37,221.14,10.00,10.91;40,319.80,221.14,3.03,18.65;40,331.56,221.14,14.40,10.91;40,351.36,219.38,4.23,7.97;40,358.80,221.14,29.94,10.91;40,394.56,219.38,4.23,7.97;40,402.72,221.14,34.95,10.91;40,443.52,219.38,4.23,7.97;40,450.84,221.14,32.29,10.91;40,488.76,219.38,4.23,7.97;40,500.04,219.84,39.89,12.21;40,72.00,233.40,143.48,12.21;40,180.72,259.30,20.28,10.91;40,176.16,268.70,29.62,7.97;40,210.12,251.98,3.03,18.65;40,217.32,251.98,32.71,10.91;40,254.64,251.98,3.03,18.65;40,214.32,266.74,38.71,10.91;40,263.28,259.30,8.49,18.65;40,280.44,259.30,20.28,10.91;40,274.80,269.18,31.29,7.97;40,319.20,259.30,20.28,10.91;40,308.52,269.18,41.00,7.97;40,362.16,251.98,13.23,10.91;40,381.24,250.22,4.23,7.97;40,388.56,251.98,8.49,18.65;40,404.88,251.98,15.97,10.91;40,426.48,250.22,4.23,7.97;40,366.24,266.74,49.51,10.91;40,263.28,290.50,37.44,10.91;40,274.80,300.38,31.29,7.97;40,318.36,283.18,13.23,10.91;40,337.44,281.42,4.23,7.97;40,344.76,283.18,8.49,18.65;40,361.08,283.18,15.97,10.91;40,382.68,281.42,4.23,7.97;40,318.36,297.14,68.55,11.83;40,520.56,289.20,19.24,12.21;40,263.28,321.70,8.49,10.91;40,276.00,314.26,6.36,10.91;40,282.36,312.50,4.23,7.97;40,282.36,319.22,15.75,7.97;40,298.68,314.26,16.60,10.91;40,317.76,314.26,8.49,18.65;40,328.68,314.26,6.36,10.91;40,335.04,312.50,4.23,7.97;40,335.04,319.22,14.18,7.97;40,349.68,314.26,16.60,10.91;40,276.00,328.22,10.59,11.95;40,282.36,334.10,15.75,7.97;40,298.68,329.26,36.36,10.91;40,335.04,327.62,4.23,7.97;40,335.04,334.34,14.18,7.97;40,349.68,329.26,16.60,10.91;40,520.56,320.40,19.24,12.21;40,71.64,345.96,200.44,12.21;40,275.28,347.26,10.91,18.65;40,289.20,345.96,90.49,12.21;40,379.68,345.50,4.23,7.97;40,386.28,347.26,15.00,10.91;40,401.28,345.50,4.23,7.97;40,408.96,345.96,131.01,12.21;40,70.68,361.68,192.64,12.21;40,266.40,362.98,10.91,18.65;40,281.52,359.90,6.90,13.59;40,288.36,358.79,3.65,5.49;40,292.56,359.90,10.23,13.59;40,302.76,358.79,3.65,5.49;40,306.96,359.90,2.35,13.59;40,283.92,368.63,22.49,8.48;40,312.84,361.68,122.45,12.21;40,437.88,353.98,9.09,18.65;40,447.00,362.98,93.27,11.85;40,72.00,377.28,140.73,12.21;40,215.76,369.70,9.09,18.65;40,224.88,377.28,295.71,13.15;40,88.92,401.64,178.04,12.21;40,267.00,399.38,12.63,7.97;40,267.00,408.14,4.23,7.97;40,282.72,402.94,8.49,18.65;40,293.76,402.94,7.88,10.91;40,301.68,399.38,12.63,7.97;40,301.68,408.14,4.23,7.97;40,314.88,401.18,113.38,13.61;40,421.08,407.90,8.43,7.97;40,439.92,401.18,102.15,13.61;40,72.00,416.40,186.30,12.21;40,91.68,435.26,102.22,14.09;40,186.72,442.46,8.43,7.97;40,195.72,435.26,328.74,14.09;40,71.64,457.56,451.24,13.15;40,522.84,457.10,4.23,7.97;40,531.48,458.86,8.49,10.91;40,72.00,471.12,209.47,13.15"><head>x − y 2 = 1 and x + y 2 = 2 .1</head><label>222</label><figDesc>Define the constraint sets Z = {(z, w) | z 2 = w 2 = 2} and X w,z = {(x, y) | Ax 2 + Ay 2 = Az 2 + Aw2 /2}. We now successively upper bound max x,y;x⊥y | Ax, Ay | Ax Ay ≤ max (z,w)∈Z max (x,y)∈Xw,z Az 2 − Aw 2 4 Ax Ay = max (z,w)∈ZAz 2 − Aw 2 Az 2 + Aw 2 (80) = λ 2 max (A) − λ 2 min (A) λ 2 max (A) + λ 2 min (A)(81)where (80) follows since the function(a, b) → ab constrained to a 2 + b 2 = c is minimized at (a, b) = ( c/2, c/2) and (81) follows since (a, b) → |a 2 −b 2 | a 2 +b 2 constrained to the interval √ 2[λ min (A), λ max (A)] attains a maximum at (a, b) = √ 2 (λ max (A), λ min (A)) .Reordering the terms in (81) proves (77).We instantiate the claim with A = Σ , x = vec (aβ 1⋆ + u)β ⊤ 2⋆ and y = vec β 1⋆ v ⊤ . Note that x and y are orthogonal sincex, y = (aβ 1⋆ + u)β ⊤ 2⋆ , β 1⋆ v ⊤ = tr β 2⋆ (aβ 1⋆ + u) ⊤ β 1⋆ v ⊤ = tr (aβ 1⋆ + u) ⊤ β 1⋆ v ⊤ β 2⋆= 0 where we used the cyclic invariance of the trace and the fact that v, β 2⋆ = 0. Further, κ(A) 2 = κ(Σ 1 )κ(Σ 2 ) and thus, all together we derive</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="40,235.56,655.46,4.23,7.97;40,235.56,662.18,4.23,7.97;40,245.76,657.22,14.55,11.85;40,266.40,655.46,4.23,7.97;40,266.40,662.18,4.23,7.97;40,272.76,657.22,30.15,11.85;40,308.88,655.46,4.23,7.97;40,319.08,657.22,5.29,10.91;40,330.12,655.46,4.23,7.97;40,336.60,657.22,30.15,11.85;40,372.72,655.46,4.23,7.97;40,382.92,657.22,6.24,10.91;40,394.56,655.46,4.23,7.97;40,399.48,655.92,140.35,12.21;40,70.68,669.48,62.73,12.21;40,105.36,689.26,9.09,18.65;40,114.48,687.02,4.23,7.97;40,119.28,689.26,193.35,11.85;40,312.60,687.02,4.23,7.97;40,320.40,689.26,186.15,11.85"><head>2 2 β 2⋆ 2 2 + β 1⋆ 2 v 2 + β 2⋆ 2 u 2 .</head><label>22</label><figDesc>Now setting v = 0, equation (82) becomes ∇ 2 f (M ⋆ )[∆, ∆] = E[ x 1 , aβ 1⋆ + u x 2 , β 2⋆ ] 2 = Σ 1 (aβ 1⋆ + u), aβ 1⋆ + u Σ 2 β 2⋆ , β 2⋆ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="12,71.64,262.20,468.41,25.77"><head>Table 1 :</head><label>1</label><figDesc>Examples of QGLM problems<ref type="bibr" coords="12,268.70,262.20,17.87,12.21" target="#b15">(16)</ref>. In the linear regression model, ε is random noise satisfying</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="36,72.00,289.44,468.05,25.77"><head></head><label></label><figDesc>surely. The claimed expression<ref type="bibr" coords="36,164.52,303.00,19.49,12.21" target="#b21">(22)</ref> follows immediately.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="36,72.00,675.14,470.07,55.33"><head></head><label></label><figDesc>The first equation is proved for example in<ref type="bibr" coords="36,315.15,696.96,16.92,12.21" target="#b42">[43,</ref> Corollary 3.1]. To see the second equation, standard results show that the conditional distributiony 1 | y 2 is Gaussian N (ρ(σ 1 /σ 2 )y 2 , σ 2 1 −ρ 2 σ 2 1 ). Consider the function ψ : [−1, 1] → R defined by ψ(t) = √ 1 − t 2 + t arcsin(t) and the function φ : [−1, 1] → R given by φ(t) = π 2 − 1 t 2 + 1.Then, for any t ∈ [−1, 1] we have ψ(t) ≤ φ(t).</figDesc><table coords="36,72.00,675.14,253.23,34.03"><row><cell>2 1 σ 2 2 . 1 | y 2 ] = (1 − ρ 2 )σ 2 1 + 2 ] = (1 + 2ρ 2 )σ 2 1 | y 2 ]y 2 2 = E[E[y 2 1 y 2 gives E y 2 1 σ 2 2 , as claimed. ρ 2 σ 2 1 σ 2 2 Proof. Thus, the second moment is E[y 2 Lemma E.2.</cell><cell>y 2 2 and therefore iterating expectations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="39,71.28,414.62,468.93,275.47"><head></head><label></label><figDesc>F.1. Let A ∈ S d + be a positive definite matrix. Let x, y ∈ R d be any pair of orthogonal vectors. Then the estimate holds:We will come back to the proof of (77) but first we show how it implies the result (76). Expanding the square yieldsAx + Ay 2 = Ax 2 + 2 Ax, Ay + Ay 2 ≥ Ax 2 − 2| Ax, Ay | + Ay 2 Ax 2 + Ay 2 ,where (78) follows by (77) and (79) is an application of Young's inequality. So we now focus on proving (77). This is equivalent to finding an upper bound for the following optimization problem + y) 2 − A(x − y)2  4 Ax Ay where the equality follows from the parallelogram law. Using the same law yields that 1 2 A(x + y) 2 + A(x − y) 2 = Ax 2 + Ay 2 .</figDesc><table coords="39,72.00,442.78,467.80,230.51"><row><cell>max x,y: x⊥y</cell><cell>Ax, Ay Ax Ay</cell><cell cols="3">x,y;x⊥y = max</cell><cell>A(x</cell></row><row><cell cols="3">Ax + Ay 2 ≥</cell><cell cols="2">2 κ(A) 2 + 1</cell><cell>Ax 2 + Ay 2 .</cell><cell>(76)</cell></row><row><cell cols="5">Proof. The claim will follow from the following inequality</cell></row><row><cell cols="5">| Ax, Ay | ≤ 1 −</cell><cell>2 κ(A) 2 + 1</cell><cell>Ax Ay .</cell><cell>(77)</cell></row><row><cell cols="5">≥ Ax 2 − 2 1 −</cell><cell>2 κ(A) 2 + 1</cell><cell>Ax Ay + Ay 2</cell><cell>(78)</cell></row><row><cell cols="4">≥ Ax 2 − 1 −</cell><cell>2 κ(A) 2 + 1</cell><cell>Ax 2 + Ay 2 + Ay 2</cell><cell>(79)</cell></row><row><cell>=</cell><cell>2 κ(A) 2 + 1</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In the theorem statement, the symbol DF (x) : R d → S k denotes the differential of F , while the symbol DF (x) * : S k → R d is the adjoint linear map of DF (x).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank John Duchi, Jorge Garza-Vargas, Zaid Harchaoui, and Eitan Levin for insightful conversations during the development of this work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>www.math.washington.edu/∼ddrusv. Research of Drusvyatskiy was supported by the NSF DMS-2306322, NSF CCF 1740551, and AFOSR FA9550-24-1-0092 awards.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note the equality aβ</head><p>. Therefore we deduce min</p><p>A symmetric argument shows</p><p>In particular, if Σ 1 or Σ 2 are singular then µ × ν lies in E, as claimed. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Proof of Theorem 5.6</head><p>This follows directly from Lemma 5.5 and Theorem A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Proofs from Section 5.3</head><p>G.1 Proof of Lemma 5.7</p><p>Consider any tangent vector</p><p>Then we may vectorize ∆ as follows:</p><p>Minimizing the expression (83) in ∆ with ∆ F = 1 yields the guarantee <ref type="bibr" coords="41,424.09,366.60,17.97,12.21" target="#b29">(30)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Proof of Theorem 5.8</head><p>Characterization of well-posedness. To simplify notation, we will relabel β ⋆ to β. Observe for any measure (p ij ) ∈ Q, equation (28) yields</p><p>where ∆ v := βv ⊤ + vβ ⊤ is any tangent vector to M ⋆ at M. Recall moreover that ∆ v is nonzero if and only if v is nonzero.</p><p>( =⇒ ) We prove the contrapositive. Thus, suppose that Assumption 1 does not hold. We will consider two cases separately. Case 1. Assume that there exists ī ∈ V 0 . Thus for any edge ( ī, j) ∈ E equality β j = 0 holds. Then setting v = e ī, we deduce</p><p>We conclude that µ lies in E mc , as claimed.</p><p>Case 2. Assume that one of the components of G * is bipartite and the set V 0 is empty. Without loss of generality, we may assume that G * is connected, since otherwise we can restrict the following argument to any connected component. Thus, there exists a partition V * = I ∪ J with all the edges (i, j) ∈ E * satisfying i ∈ I and j ∈ J. Define the vector</p><p>Using (84), we obtain</p><p>where we used that v is supported on V * and that V 0 is empty. Thus, µ lies in E mc , as claimed.</p><p>( ⇐= ) Assume that Assumption 1 holds. To this end, let</p><p>Our goal is to show that v is the zero vector. To this end, clearly (84) implies</p><p>Without loss of generality suppose that G * is connected, otherwise, we can repeat the argument for each connected component. Since G * is non-bipartite, it must contain an odd-size cycle</p><p>Consider the expansion</p><p>where the last equality follows since each term in the parenthesis is zero. Since β i 1 &gt; 0, we deduce v i k = 0. Next, observe from (85) that for any neighbor j of i k , i.e., satisfying (i k , j) ∈ E * , we have that</p><p>Repeating the argument, we deduce v j = 0 for all v ∈ V * . Next, consider any vertex i / ∈ V * . Then since V 0 is empty, there exists some j ∈ V * with (i, j) ∈ E. Using (85) again, we conclude v i β j = 0. Taking into account β j &gt; 0, we deduce v i = 0. Thus v is identically zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance formula.</head><p>We have now proved that a measure µ lies in E mc if and only if its support supp(µ) lies in Ω β⋆ . Given any set of indices A ⊆ [d] × [d] define P A : R d×d → R d×d to be the orthogonal projection onto the entries indexed by A. Fix now a set of entries A ⊂ supp(P ) such that A ∈ Ω M⋆ . Clearly, the pushforward measure (P A ) # µ lies E mc , and we compute</p><p>Taking the minimum over A yields the inequality ≤ in <ref type="bibr" coords="42,335.65,456.96,17.97,12.21" target="#b32">(33)</ref>.</p><p>Conversely, fix a measure ν ∈ E mc and let A be the indices that are observed with positive probability according to ν. Thus, the support of ν is contained in the subspace range(P A ). Then by Lemma A.1 and (35), we have</p><p>Taking the minimum over all entries A ⊂ supp(P ) such that A ∈ Ω M⋆ completes the proof of the reverse inequality ≥ in <ref type="bibr" coords="42,183.97,559.80,17.88,12.21" target="#b32">(33)</ref>.</p><p>Hardness. We reduce from the MAXCUT problem. Assume that we have a polynomial time algorithm Alg(M ⋆ , (p ij )) to compute <ref type="bibr" coords="42,245.16,602.16,17.88,12.21" target="#b32">(33)</ref>. Given an instance of MAXCUT G = (V, E), we split the graph into its connected components</p><p>For each ℓ ≤ k, we define an instance of matrix completion with M ⋆ = 11 ⊤ ∈ R |G ℓ |×|G ℓ | -with a slight abuse of notation we use M ⋆ for all k problems -and set the distribution P (ℓ) to p</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and p</head><p>(ℓ) ij = 0, otherwise. Since the entries of M ⋆ are strictly positive and G ℓ is connected, the output of Alg(M ⋆ , P (ℓ) ) times |E ℓ | is equal to the minimum number of edges one needs to remove from G ℓ to make it bipartite. Thus, |E ℓ | 1 − ALG M ⋆ , P (ℓ)  is equal to the number of edges of the largest bipartite graph one can construct via edge deletion, which is readily seen to be equal to </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="18,94.44,97.80,445.34,12.21;18,93.60,111.36,447.89,12.21;18,93.84,124.92,274.02,12.21" xml:id="b0">
	<analytic>
		<title level="a" type="main">Conference Organization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="DOI">10.1109/micai.2015.6</idno>
	</analytic>
	<monogr>
		<title level="m">2015 Fourteenth Mexican International Conference on Artificial Intelligence (MICAI)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct coords="18,94.44,147.48,447.54,12.21;18,94.44,161.04,446.80,12.21;18,94.08,174.60,69.39,12.21" xml:id="b1">
	<analytic>
		<title level="a" type="main">Characterizations of error bounds for lower semicontinuous functions on metric spaces</title>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Azé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Noël</forename><surname>Corvellec</surname></persName>
		</author>
		<idno type="DOI">10.1051/cocv:2004013</idno>
	</analytic>
	<monogr>
		<title level="j">ESAIM: Control, Optimisation and Calculus of Variations</title>
		<title level="j" type="abbrev">ESAIM: COCV</title>
		<idno type="ISSN">1292-8119</idno>
		<idno type="ISSNe">1262-3377</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="425" />
			<date type="published" when="2004-06-15">2004</date>
			<publisher>EDP Sciences</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,197.04,447.28,12.21;18,94.44,210.60,349.96,12.21" xml:id="b2">
	<analytic>
		<title level="a" type="main">Reconstruction of Signals from Magnitudes of Redundant Representations: The Complex Case</title>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Balan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10208-015-9261-0</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<title level="j" type="abbrev">Found Comput Math</title>
		<idno type="ISSN">1615-3375</idno>
		<idno type="ISSNe">1615-3383</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="677" to="721" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.45,233.16,445.65,12.21;18,94.44,246.72,446.44,12.21;18,94.20,260.28,24.63,12.21" xml:id="b3">
	<analytic>
		<title level="a" type="main">Saving phase: Injectivity and stability for phase retrieval</title>
		<author>
			<persName><forename type="first">Afonso</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jameson</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><forename type="middle">G</forename><surname>Mixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">A</forename><surname>Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acha.2013.10.002</idno>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<title level="j" type="abbrev">Applied and Computational Harmonic Analysis</title>
		<idno type="ISSN">1063-5203</idno>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="125" />
			<date type="published" when="2014-07">2014</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,282.72,445.93,12.21;18,94.44,296.28,445.35,12.21;18,93.60,309.84,114.75,12.21" xml:id="b4">
	<monogr>
		<title level="m" type="main">When can you trust feature selection?-i: A condition-based analysis of lasso and generalised hardness of approximation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bastounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11425</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,94.44,332.40,445.51,12.21;18,94.44,345.96,446.56,12.21;18,94.20,359.40,24.63,12.21" xml:id="b5">
	<analytic>
		<title level="a" type="main">A Grassmann manifold handbook: basic geometry and computational aspects</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bendokat</surname></persName>
			<idno type="ORCID">0000-0002-0671-6291</idno>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Zimmermann</surname></persName>
			<idno type="ORCID">0000-0003-1692-3996</idno>
		</author>
		<author>
			<persName><forename type="first">P-A</forename><surname>Absil</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10444-023-10090-8</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Computational Mathematics</title>
		<title level="j" type="abbrev">Adv Comput Math</title>
		<idno type="ISSN">1019-7168</idno>
		<idno type="ISSNe">1572-9044</idno>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2024-01-05">2024</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,381.96,445.60,12.21;18,94.44,395.52,280.96,12.21" xml:id="b6">
	<analytic>
		<title level="a" type="main">On the Bures–Wasserstein distance between positive definite matrices</title>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanvi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdo</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.exmath.2018.01.002</idno>
	</analytic>
	<monogr>
		<title level="j">Expositiones Mathematicae</title>
		<title level="j" type="abbrev">Expositiones Mathematicae</title>
		<idno type="ISSN">0723-0869</idno>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="191" />
			<date type="published" when="2019-06">2019</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,418.08,445.25,12.21;18,93.60,431.52,282.04,12.21" xml:id="b7">
	<analytic>
		<title level="a" type="main">Iterative hard thresholding for compressed sensing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acha.2009.04.002</idno>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<title level="j" type="abbrev">Applied and Computational Harmonic Analysis</title>
		<idno type="ISSN">1063-5203</idno>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="274" />
			<date type="published" when="2009-11">2009</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,454.08,283.01,12.21" xml:id="b8">
	<analytic>
		<title level="a" type="main">Convex Analysis and Nonlinear Optimization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Borwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-31256-9</idno>
	</analytic>
	<monogr>
		<title level="m">Convex Analysis</title>
				<imprint>
			<publisher>Springer New York</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,476.64,447.16,12.21;18,94.20,490.20,24.63,12.21" xml:id="b9">
	<monogr>
		<title level="m" type="main">An Introduction to Optimization on Smooth Manifolds</title>
		<author>
			<persName coords=""><forename type="first">N</forename></persName>
		</author>
		<idno type="DOI">10.1017/9781009166164</idno>
		<imprint>
			<date type="published" when="2023-03-09">2023</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,512.64,447.51,12.21;18,94.44,526.20,68.80,12.21" xml:id="b10">
	<monogr>
		<title level="m" type="main">Monotonic maximal operators and semi-groups of contractions in Hilbert spaces</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Brezis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,548.76,445.42,12.21;18,93.84,562.32,80.44,12.21" xml:id="b11">
	<analytic>
		<title level="a" type="main">On a problem posed by Steve Smale</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bürgisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Cucker</surname></persName>
		</author>
		<idno type="DOI">10.4007/annals.2011.174.3.8</idno>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<title level="j" type="abbrev">Ann. Math.</title>
		<idno type="ISSN">0003-486X</idno>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1785" to="1836" />
			<date type="published" when="2011-11-01">2011</date>
			<publisher>Annals of Mathematics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,584.76,447.51,12.21;18,94.44,598.32,199.01,12.21" xml:id="b12">
	<monogr>
		<title level="m" type="main">Condition: The geometry of numerical algorithms</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bürgisser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">349</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,620.88,445.83,12.21;18,94.44,634.44,445.34,12.21;18,93.60,648.00,262.12,12.21" xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-Rank Matrix Recovery with Composite Optimization: Good Conditioning and Rapid Convergence</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Charisopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damek</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10208-020-09490-9</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<title level="j" type="abbrev">Found Comput Math</title>
		<idno type="ISSN">1615-3375</idno>
		<idno type="ISSNe">1615-3383</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1505" to="1593" />
			<date type="published" when="2021-01-28">2021</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,94.44,670.44,445.98,12.21;18,94.44,684.00,446.44,12.21;18,94.20,697.56,24.63,12.21" xml:id="b14">
	<analytic>
		<title level="a" type="main">Composite optimization for robust rank one bilinear sensing</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Charisopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damek</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<idno type="DOI">10.1093/imaiai/iaaa027</idno>
	</analytic>
	<monogr>
		<title level="j">Information and Inference: A Journal of the IMA</title>
		<idno type="ISSNe">2049-8772</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="396" />
			<date type="published" when="2021">2021</date>
			<publisher>Oxford University Press (OUP)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,73.44,446.92,12.21;19,94.44,87.00,415.11,12.21" xml:id="b15">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1509.03025</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="19,94.44,109.56,446.81,12.21;19,94.08,123.12,380.32,12.21" xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview</title>
		<author>
			<persName><forename type="first">Yuejie</forename><surname>Chi</surname></persName>
			<idno type="ORCID">0000-0002-6766-5459</idno>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><forename type="middle">M</forename><surname>Lu</surname></persName>
			<idno type="ORCID">0000-0002-5174-2595</idno>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Chen</surname></persName>
			<idno type="ORCID">0000-0001-9256-5815</idno>
		</author>
		<idno type="DOI">10.1109/tsp.2019.2937282</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<title level="j" type="abbrev">IEEE Trans. Signal Process.</title>
		<idno type="ISSN">1053-587X</idno>
		<idno type="ISSNe">1941-0476</idno>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="5239" to="5269" />
			<date type="published" when="2019-10-15">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,145.56,447.53,12.21;19,93.60,159.12,323.44,12.21" xml:id="b17">
	<analytic>
		<title level="a" type="main">Compressed sensing and best 𝑘-term approximation</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Dahmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Devore</surname></persName>
		</author>
		<idno type="DOI">10.1090/s0894-0347-08-00610-3</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Mathematical Society</title>
		<title level="j" type="abbrev">J. Amer. Math. Soc.</title>
		<idno type="ISSN">0894-0347</idno>
		<idno type="ISSNe">1088-6834</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2009">2009</date>
			<publisher>American Mathematical Society (AMS)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,181.68,441.18,12.21" xml:id="b18">
	<monogr>
		<title level="m" type="main">Mathematical methods of statistics</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cramér</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Princeton university press</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,204.24,416.08,12.21" xml:id="b19">
	<monogr>
		<title level="m" type="main">Modern Nonconvex Nondifferentiable Optimization</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Shi</forename><surname>Pang</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611976748</idno>
		<imprint>
			<date type="published" when="2021-01">2021</date>
			<publisher>Society for Industrial and Applied Mathematics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,226.68,445.86,12.21;19,94.44,240.24,446.79,12.21;19,94.20,253.80,86.44,12.21" xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic approximation with decision-dependent distributions: asymptotic normality and optimality</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,276.36,445.66,12.21;19,94.44,289.92,301.72,12.21" xml:id="b21">
	<analytic>
		<title level="a" type="main">Spectral (isotropic) manifolds and their dimension</title>
		<author>
			<persName><forename type="first">Aris</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Malick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hristo</forename><surname>Sendov</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11854-016-0013-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal d&apos;Analyse Mathématique</title>
		<title level="j" type="abbrev">JAMA</title>
		<idno type="ISSN">0021-7670</idno>
		<idno type="ISSNe">1565-8538</idno>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="369" to="397" />
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,312.36,447.45,12.21;19,93.84,325.92,350.68,12.21" xml:id="b22">
	<analytic>
		<title level="a" type="main">Orthogonal Invariance and Identifiability</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1137/130916710</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<title level="j" type="abbrev">SIAM J. Matrix Anal. &amp; Appl.</title>
		<idno type="ISSN">0895-4798</idno>
		<idno type="ISSNe">1095-7162</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="580" to="598" />
			<date type="published" when="2014-01">2014</date>
			<publisher>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,348.48,445.49,12.21;19,94.44,362.04,425.32,12.21" xml:id="b23">
	<analytic>
		<title level="a" type="main">An Overview of Low-Rank Matrix Recovery From Incomplete Observations</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Romberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/jstsp.2016.2539100</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<title level="j" type="abbrev">IEEE J. Sel. Top. Signal Process.</title>
		<idno type="ISSN">1932-4553</idno>
		<idno type="ISSNe">1941-0484</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="608" to="622" />
			<date type="published" when="2016-06">2016</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,384.48,446.91,12.21;19,94.20,398.04,69.40,12.21" xml:id="b24">
	<analytic>
		<title level="a" type="main">All convex invariant functions of hermitian matrices</title>
		<author>
			<persName coords=""><forename type="first">Chandler</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf01898787</idno>
	</analytic>
	<monogr>
		<title level="j">Archiv der Mathematik</title>
		<title level="j" type="abbrev">Arch. Math</title>
		<idno type="ISSN">0003-889X</idno>
		<idno type="ISSNe">1420-8938</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="276" to="278" />
			<date type="published" when="1957-10">1957</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,420.60,445.66,12.21;19,94.44,434.16,314.55,12.21" xml:id="b25">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:2301.06632</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="19,94.44,456.60,446.91,12.21;19,94.44,470.16,99.27,12.21" xml:id="b26">
	<analytic>
		<title level="a" type="main">The scientific work of Ennio De Giorgi</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">De</forename><surname>Giorgi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-41496-1_2</idno>
	</analytic>
	<monogr>
		<title level="m">Springer Collected Works in Mathematics</title>
				<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="5" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,492.72,447.75,12.21;19,93.84,506.28,205.96,12.21" xml:id="b27">
	<analytic>
		<title level="a" type="main">On condition numbers and the distance to the nearest ill-posed problem</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf01400115</idno>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<title level="j" type="abbrev">Numer. Math.</title>
		<idno type="ISSN">0029-599X</idno>
		<idno type="ISSNe">0945-3245</idno>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="289" />
			<date type="published" when="1987-05">1987</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,528.72,298.00,12.21" xml:id="b28">
	<monogr>
		<title level="m" type="main">Applied Numerical Linear Algebra</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611971446</idno>
		<imprint>
			<date type="published" when="1997-01">1997</date>
			<publisher>Society for Industrial and Applied Mathematics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,551.28,445.37,12.21;19,93.72,564.84,117.27,12.21" xml:id="b29">
	<analytic>
		<title level="a" type="main">The nonsmooth landscape of blind deconvolution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Díaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Optimization for Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,587.40,444.71,12.21;19,93.60,600.96,278.32,12.21" xml:id="b30">
	<analytic>
		<title level="a" type="main">The radius of metric regularity</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dontchev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rockafellar</surname></persName>
		</author>
		<idno type="DOI">10.1090/s0002-9947-02-03088-x</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<title level="j" type="abbrev">Trans. Amer. Math. Soc.</title>
		<idno type="ISSN">0002-9947</idno>
		<idno type="ISSNe">1088-6850</idno>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="493" to="517" />
			<date type="published" when="2003">2003</date>
			<publisher>American Mathematical Society (AMS)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,623.40,447.51,12.21;19,94.44,636.96,71.33,12.21" xml:id="b31">
	<monogr>
		<title level="m" type="main">Implicit Functions and Solution Mappings</title>
		<author>
			<persName><forename type="first">Asen</forename><forename type="middle">L</forename><surname>Dontchev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Tyrrell</forename><surname>Rockafellar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-87821-8</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer New York</publisher>
			<biblScope unit="volume">543</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,659.52,445.11,12.21;19,93.60,673.08,194.92,12.21" xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficiency of minimizing compositions of convex functions and smooth maps</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
			<idno type="ORCID">0000-0001-5245-0458</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Paquette</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-018-1311-3</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<title level="j" type="abbrev">Math. Program.</title>
		<idno type="ISSN">0025-5610</idno>
		<idno type="ISSNe">1436-4646</idno>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="503" to="558" />
			<date type="published" when="2018-07-20">2018</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,94.44,695.52,445.00,12.21;19,93.60,709.08,187.12,12.21" xml:id="b33">
	<analytic>
		<title level="a" type="main">Curves of Descent</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1137/130920216</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<title level="j" type="abbrev">SIAM J. Control Optim.</title>
		<idno type="ISSN">0363-0129</idno>
		<idno type="ISSNe">1095-7138</idno>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="138" />
			<date type="published" when="2015-01">2015</date>
			<publisher>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.44,73.44,444.70,12.21;20,93.84,87.00,136.36,12.21" xml:id="b34">
	<analytic>
		<title level="a" type="main">Asymptotic optimality in stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ruan</surname></persName>
		</author>
		<idno type="DOI">10.1214/19-aos1831</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<title level="j" type="abbrev">Ann. Statist.</title>
		<idno type="ISSN">0090-5364</idno>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="48" />
			<date type="published" when="2021-02-01">2021</date>
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.44,109.56,447.28,12.21;20,94.44,123.12,446.79,12.21;20,94.44,136.68,97.36,12.21" xml:id="b35">
	<analytic>
		<title level="a" type="main">Some characterizations and properties of the “distance to ill-posedness” and the condition measure of a conic linear system</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">R</forename><surname>Vera</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107990063a</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<title level="j" type="abbrev">Math. Program.</title>
		<idno type="ISSN">0025-5610</idno>
		<idno type="ISSNe">1436-4646</idno>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="260" />
			<date type="published" when="1999-11">1999</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.44,159.12,445.73,12.21;20,93.12,172.68,64.72,12.21" xml:id="b36">
	<analytic>
		<title level="a" type="main">Metric regularity and subdifferential calculus</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="DOI">10.1070/rm2000v055n03abeh000292</idno>
	</analytic>
	<monogr>
		<title level="j">Russian Mathematical Surveys</title>
		<title level="j" type="abbrev">Russ. Math. Surv.</title>
		<idno type="ISSN">0036-0279</idno>
		<idno type="ISSNe">1468-4829</idno>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="501" to="558" />
			<date type="published" when="2000-06-30">2000</date>
			<publisher>Steklov Mathematical Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.44,195.24,447.11,12.21;20,93.84,208.80,106.83,12.21" xml:id="b37">
	<monogr>
		<title level="m" type="main">Variational Analysis of Regular Mappings</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">D</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-64277-2</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.44,231.24,445.50,12.21;20,94.44,244.80,117.03,12.21" xml:id="b38">
	<monogr>
		<title level="m" type="main">Asymptotics in Statistics</title>
		<author>
			<persName><forename type="first">Lucien</forename><surname>Le Cam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Lo Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-1166-2</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer New York</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.44,267.36,445.85,12.21;20,93.12,280.92,86.44,12.21" xml:id="b39">
	<analytic>
		<title level="a" type="main">Convex Analysis on the Hermitian Matrices</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1137/0806009</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<title level="j" type="abbrev">SIAM J. Optim.</title>
		<idno type="ISSN">1052-6234</idno>
		<idno type="ISSNe">1095-7189</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="177" />
			<date type="published" when="1996-02">1996</date>
			<publisher>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.44,303.36,447.52,12.21" xml:id="b40">
	<analytic>
		<title level="a" type="main">Nonsmooth analysis of eigenvalues</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107980004a</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<title level="j" type="abbrev">Math. Program.</title>
		<idno type="ISSN">0025-5610</idno>
		<idno type="ISSNe">1436-4646</idno>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1999-01">1999</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.44,325.92,446.70,12.21;20,92.28,339.48,165.16,12.21" xml:id="b41">
	<analytic>
		<title level="a" type="main">Nonsmooth Analysis of Singular Values. Part I: Theory</title>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hristo</forename><forename type="middle">S</forename><surname>Sendov</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11228-004-7197-7</idno>
	</analytic>
	<monogr>
		<title level="j">Set-Valued Analysis</title>
		<title level="j" type="abbrev">Set-Valued Anal</title>
		<idno type="ISSN">0927-6947</idno>
		<idno type="ISSNe">1572-932X</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="241" />
			<date type="published" when="2005-09">2005</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.45,362.04,446.70,12.21;20,93.84,375.60,445.85,12.21;20,94.44,389.04,75.03,12.21" xml:id="b42">
	<analytic>
		<title level="a" type="main">Gaussian integrals involving absolute value functions</title>
		<author>
			<persName><forename type="first">Wenbo</forename><forename type="middle">V</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1214/09-imscoll504</idno>
	</analytic>
	<monogr>
		<title level="m">Institute of Mathematical Statistics Collections</title>
				<imprint>
			<publisher>Institute of Mathematical Statistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="43" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.45,411.60,447.41,12.21;20,94.44,425.16,412.36,12.21" xml:id="b43">
	<analytic>
		<title level="a" type="main">Cramér-rao lower bounds for low-rank decomposition of multidimensional arrays</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2074" to="2086" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.45,447.72,447.40,12.21;20,94.44,461.28,445.65,12.21;20,93.84,474.72,379.96,12.21" xml:id="b44">
	<analytic>
		<title level="a" type="main">Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind Deconvolution</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaizheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10208-019-09429-9</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<title level="j" type="abbrev">Found Comput Math</title>
		<idno type="ISSN">1615-3375</idno>
		<idno type="ISSNe">1615-3383</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="632" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.45,497.28,408.89,12.21" xml:id="b45">
	<analytic>
		<title level="a" type="main">Quasi-Likelihood Functions</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Mccullagh</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176346056</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<title level="j" type="abbrev">Ann. Statist.</title>
		<idno type="ISSN">0090-5364</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1983-03-01">Mar. 1983</date>
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.45,519.84,445.00,12.21;20,93.84,533.40,171.88,12.21" xml:id="b46">
	<analytic>
		<title level="a" type="main">Proximité et dualité dans un espace hilbertien</title>
		<author>
			<persName coords=""><forename type="first">J.-J</forename><surname>Moreau</surname></persName>
		</author>
		<idno type="DOI">10.24033/bsmf.1625</idno>
	</analytic>
	<monogr>
		<title level="j">Bulletin de la Soci&amp;#233;t&amp;#233; math&amp;#233;matique de France</title>
		<title level="j" type="abbrev">Bul. Soc. Math. France</title>
		<idno type="ISSN">0037-9484</idno>
		<idno type="ISSNe">2102-622X</idno>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="273" to="299" />
			<date type="published" when="1965">1965</date>
			<publisher>Societe Mathematique de France</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.45,555.84,445.54,12.21;20,94.44,569.40,424.96,12.21" xml:id="b47">
	<analytic>
		<title level="a" type="main">On the Achievability of Cramér–Rao Bound in Noisy Compressed Sensing</title>
		<author>
			<persName><forename type="first">Rad</forename><surname>Niazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massoud</forename><surname>Babaie-Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Jutten</surname></persName>
		</author>
		<idno type="DOI">10.1109/tsp.2011.2171953</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<title level="j" type="abbrev">IEEE Trans. Signal Process.</title>
		<idno type="ISSN">1053-587X</idno>
		<idno type="ISSNe">1941-0476</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="518" to="526" />
			<date type="published" when="2011">2011</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.45,591.96,447.64,12.21;20,93.84,605.52,253.84,12.21" xml:id="b48">
	<analytic>
		<title level="a" type="main">Understanding the geometry of infeasible perturbations of a conic linear system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="534" to="550" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.45,627.96,445.45,12.21;20,93.12,641.52,160.48,12.21" xml:id="b49">
	<analytic>
		<title level="a" type="main">Tilt Stability of a Local Minimum</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Poliquin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<idno type="DOI">10.1137/s1052623496309296</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<title level="j" type="abbrev">SIAM J. Optim.</title>
		<idno type="ISSN">1052-6234</idno>
		<idno type="ISSNe">1095-7189</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="299" />
			<date type="published" when="1998-05">1998</date>
			<publisher>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,94.45,664.08,447.43,12.21;20,93.84,677.64,448.14,12.21;20,93.84,691.20,67.83,12.21" xml:id="b50">
	<analytic>
		<title level="a" type="main">Acceleration of Stochastic Approximation by Averaging</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
		<idno type="DOI">10.1137/0330046</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<title level="j" type="abbrev">SIAM J. Control Optim.</title>
		<idno type="ISSN">0363-0129</idno>
		<idno type="ISSNe">1095-7138</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992-07">1992</date>
			<publisher>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,73.44,445.56,12.21;21,93.72,87.00,289.83,12.21" xml:id="b51">
	<monogr>
		<title level="m" type="main">Dataset Shift in Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/9780262170055.001.0001</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,108.60,447.62,12.21;21,94.44,122.16,440.81,12.21" xml:id="b52">
	<analytic>
		<title level="a" type="main">Information and the Accuracy Attainable in the Estimation of Statistical Parameters</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-0919-5_16</idno>
	</analytic>
	<monogr>
		<title level="m">Springer Series in Statistics</title>
				<imprint>
			<publisher>Springer New York</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="235" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,143.64,445.33,12.21;21,93.60,157.20,316.00,12.21" xml:id="b53">
	<analytic>
		<title level="a" type="main">Doubly Robust Covariate Shift Correction</title>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v29i1.9576</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015-02-21">2015</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,178.68,446.67,12.21;21,94.44,192.24,86.44,12.21" xml:id="b54">
	<analytic>
		<title level="a" type="main">Some perturbation theory for linear programming</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Renegar</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf01581690</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<title level="j" type="abbrev">Mathematical Programming</title>
		<idno type="ISSN">0025-5610</idno>
		<idno type="ISSNe">1436-4646</idno>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="73" to="91" />
			<date type="published" when="1994-02">1994</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,213.84,447.42,12.21;21,94.44,227.40,279.88,12.21" xml:id="b55">
	<analytic>
		<title level="a" type="main">Incorporating Condition Measures into the Complexity Theory of Linear Programming</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Renegar</surname></persName>
		</author>
		<idno type="DOI">10.1137/0805026</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<title level="j" type="abbrev">SIAM J. Optim.</title>
		<idno type="ISSN">1052-6234</idno>
		<idno type="ISSNe">1095-7189</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="506" to="524" />
			<date type="published" when="1995-08">1995</date>
			<publisher>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,248.88,446.70,12.21;21,93.48,262.44,210.76,12.21" xml:id="b56">
	<analytic>
		<title level="a" type="main">Linear programming, complexity theory and elementary functional analysis</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Renegar</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf01585941</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<title level="j" type="abbrev">Mathematical Programming</title>
		<idno type="ISSN">0025-5610</idno>
		<idno type="ISSNe">1436-4646</idno>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="279" to="351" />
			<date type="published" when="1995-10">1995</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,283.92,445.22,12.21;21,94.44,297.48,447.15,12.21;21,94.44,311.04,75.52,12.21" xml:id="b57">
	<analytic>
		<title level="a" type="main">Computational complexity versus statistical performance on sparse recovery problems</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Roulet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Boumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>D’aspremont</surname></persName>
		</author>
		<idno type="DOI">10.1093/imaiai/iay020</idno>
	</analytic>
	<monogr>
		<title level="j">Information and Inference: A Journal of the IMA</title>
		<idno type="ISSN">2049-8764</idno>
		<idno type="ISSNe">2049-8772</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2020">2020</date>
			<publisher>Oxford University Press (OUP)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,332.64,446.67,12.21;21,94.44,346.20,42.15,12.21" xml:id="b58">
	<monogr>
		<title level="m" type="main">Optimal Transport for Applied Mathematicians</title>
		<author>
			<persName coords=""><forename type="first">Filippo</forename><surname>Santambrogio</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-20828-2</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">94</biblScope>
			<pubPlace>Birkäuser, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,367.68,445.58,12.21;21,93.60,381.24,165.52,12.21" xml:id="b59">
	<analytic>
		<title level="a" type="main">The higher-order derivatives of spectral functions</title>
		<author>
			<persName coords=""><forename type="first">Hristo</forename><forename type="middle">S</forename><surname>Sendov</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.laa.2006.12.013</idno>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<title level="j" type="abbrev">Linear Algebra and its Applications</title>
		<idno type="ISSN">0024-3795</idno>
		<imprint>
			<biblScope unit="volume">424</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="240" to="281" />
			<date type="published" when="2007-07">2007</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,402.72,447.30,12.21;21,94.44,416.28,410.80,12.21" xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName coords=""><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0378-3758(00)00115-4</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<title level="j" type="abbrev">Journal of Statistical Planning and Inference</title>
		<idno type="ISSN">0378-3758</idno>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000-10">2000</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,437.88,446.58,12.21;21,93.60,451.44,250.48,12.21" xml:id="b61">
	<analytic>
		<title level="a" type="main">Covariance, subspace, and intrinsic Crame/spl acute/r-Rao bounds</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1109/tsp.2005.845428</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<title level="j" type="abbrev">IEEE Trans. Signal Process.</title>
		<idno type="ISSN">1053-587X</idno>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1610" to="1630" />
			<date type="published" when="2005-05">2005</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,472.92,445.66,12.21;21,93.72,486.48,446.61,12.21;21,94.44,502.72,262.71,9.29" xml:id="b62">
	<analytic>
		<title level="a" type="main">On the differentiability of O(n) invariant functions of symmetric matrices</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Sylvester</surname></persName>
		</author>
		<idno type="DOI">10.1215/s0012-7094-85-05223-8</idno>
		<idno>S0012-7094-85-05223-8</idno>
		<ptr target="https://doi.org/10.1215/" />
	</analytic>
	<monogr>
		<title level="j">Duke Mathematical Journal</title>
		<title level="j" type="abbrev">Duke Math. J.</title>
		<idno type="ISSN">0012-7094</idno>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="475" to="483" />
			<date type="published" when="1985-06-01">1985</date>
			<publisher>Duke University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.44,521.52,445.60,12.21;21,93.72,535.08,447.99,12.21;21,94.44,548.64,164.44,12.21" xml:id="b63">
	<analytic>
		<title level="a" type="main">An inequality for the trace of the product of two symmetric matrices</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Theobald</surname></persName>
		</author>
		<idno type="DOI">10.1017/s0305004100051070</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Proceedings of the Cambridge Philosophical Society</title>
		<title level="j" type="abbrev">Math. Proc. Camb. Phil. Soc.</title>
		<idno type="ISSN">0305-0041</idno>
		<idno type="ISSNe">1469-8064</idno>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="267" />
			<date type="published" when="1975-03">1975</date>
			<publisher>Cambridge University Press (CUP)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.45,570.24,425.45,12.21" xml:id="b64">
	<monogr>
		<title level="m" type="main">Van der Vaart. Asymptotic statistics</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.45,591.72,308.70,12.21" xml:id="b65">
	<analytic>
		<title level="a" type="main">Stability of optimal transport</title>
		<author>
			<persName coords=""><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-71050-9_28</idno>
	</analytic>
	<monogr>
		<title level="m">Grundlehren der mathematischen Wissenschaften</title>
				<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="773" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.45,613.20,447.77,12.21;21,94.44,626.76,282.99,12.21" xml:id="b66">
	<analytic>
		<title level="a" type="main">Walter Von Der Vogelweide</title>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Von</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="DOI">10.1093/oseo/instance.00229703</idno>
	</analytic>
	<monogr>
		<title level="m">J. M. Synge: Collected Works, Vol. 1: Poems</title>
				<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1937">1937. 1962</date>
			<biblScope unit="volume">iv</biblScope>
			<biblScope unit="page" from="286" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.45,648.36,447.39,12.21;21,94.44,661.92,142.37,12.21" xml:id="b67">
	<monogr>
		<title level="m" type="main">High-Dimensional Statistics</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108627771</idno>
		<imprint>
			<date type="published" when="2019-02-12">2019</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,94.45,683.40,445.40,12.21;21,94.44,696.96,446.91,12.21;21,94.44,710.52,138.16,12.21" xml:id="b68">
	<analytic>
		<title level="a" type="main">Robust learning under uncertain test distributions: Relating covariate shift to model misspecification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="631" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,94.44,73.44,447.00,12.21;22,93.60,87.00,315.05,12.21" xml:id="b69">
	<monogr>
		<title level="m" type="main">High-Dimensional Data Analysis with Low-Dimensional Models</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108779302</idno>
		<imprint>
			<date type="published" when="2022-01-13">2022</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,94.44,109.56,445.69,12.21;22,94.44,123.12,417.75,12.21" xml:id="b70">
	<analytic>
		<title level="a" type="main">A new complexity metric for nonconvex rank-one generalized matrix completion</title>
		<author>
			<persName><forename type="first">Haixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baturalp</forename><surname>Yalcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javad</forename><surname>Lavaei</surname></persName>
			<idno type="ORCID">0000-0003-4294-1338</idno>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sojoudi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-023-02008-5</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<title level="j" type="abbrev">Math. Program.</title>
		<idno type="ISSN">0025-5610</idno>
		<idno type="ISSNe">1436-4646</idno>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2023-08-16">2023</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,94.44,145.56,59.22,12.21;22,175.47,145.56,255.85,12.21;22,453.00,146.86,88.03,10.91;22,93.60,159.12,197.08,12.21;22,310.81,159.12,185.90,12.21;22,517.32,159.12,23.02,12.21;22,94.44,175.36,262.71,9.29" xml:id="b71">
	<analytic>
		<title level="a" type="main">Differentiability properties of isotropic functions</title>
		<author>
			<persName coords=""><forename type="first">Miroslav</forename><surname>Šilhavý</surname></persName>
		</author>
		<idno type="DOI">10.1215/s0012-7094-00-10431-0</idno>
		<idno>S0012-7094-00-10431-0</idno>
		<ptr target="https://doi.org/10.1215/" />
	</analytic>
	<monogr>
		<title level="j">Duke Mathematical Journal</title>
		<title level="j" type="abbrev">Duke Math. J.</title>
		<idno type="ISSN">0012-7094</idno>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="373" />
			<date type="published" when="2000-09-15">2000</date>
			<publisher>Duke University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
