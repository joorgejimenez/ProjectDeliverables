<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Two-Time-Scale Stochastic Gradient Method with Applications in Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-05-15">15 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,90.00,132.00,53.05,9.81"><forename type="first">Sihan</forename><surname>Zeng</surname></persName>
							<email>sihan.zeng@jpmchase.com</email>
						</author>
						<author>
							<persName coords="1,89.64,160.81,67.83,9.81"><forename type="first">Thinh</forename><forename type="middle">T</forename><surname>Doan</surname></persName>
							<email>thinhdoan@vt.edu</email>
						</author>
						<author>
							<persName coords="1,127.36,208.09,62.56,8.64"><forename type="first">Shipra</forename><surname>Agrawal</surname></persName>
						</author>
						<author>
							<persName coords="1,209.28,208.09,46.76,8.64"><forename type="first">Aaron</forename><surname>Roth</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">JPMorgan AI Research</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Two-Time-Scale Stochastic Gradient Method with Applications in Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-15">15 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">1A7E6932AE6BB2BADB4EE7745FD8199C</idno>
					<idno type="arXiv">arXiv:2405.09660v1[math.OC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-05-19T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bi-level optimization</term>
					<term>accelerated stochastic optimization</term>
					<term>reinforcement learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Two-time-scale optimization is a framework introduced in Zeng et al. ( <ref type="formula" coords="1,416.44,256.57,18.06,8.64">2024</ref>) that abstracts a range of policy evaluation and policy optimization problems in reinforcement learning (RL). Akin to bi-level optimization under a particular type of stochastic oracle, the two-time-scale optimization framework has an upper level objective whose gradient evaluation depends on the solution of a lower level problem, which is to find the root of a strongly monotone operator. In this work, we propose a new method for solving two-time-scale optimization that achieves significantly faster convergence than the prior arts. The key idea of our approach is to leverage an averaging step to improve the estimates of the operators in both lower and upper levels before using them to update the decision variables. These additional averaging steps eliminate the direct coupling between the main variables, enabling the accelerated performance of our algorithm. We characterize the finite-time convergence rates of the proposed algorithm under various conditions of the underlying objective function, including strong convexity, convexity, Polyak-Lojasiewicz condition, and general non-convexity. These rates significantly improve over the best-known complexity of the standard two-time-scale stochastic approximation algorithm. When applied to RL, we show how the proposed algorithm specializes to novel online sample-based methods that surpass or match the performance of the existing state of the art. Finally, we support our theoretical results with numerical simulations in RL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="46" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inspired by the two-time-scale optimization framework for reinforcement learning (RL) introduced in Zeng et al. ( <ref type="formula" coords="1,154.84,513.66,18.18,9.46">2024</ref>), we study the following optimization problem</p><formula xml:id="formula_0" coords="1,253.63,533.95,269.10,13.49">θ ‹ " argmin θPR d hpθq,<label>(1)</label></formula><p>where h : R d Ñ R is a continuously differentiable (possibly nonconvex) function. The gradient of h can only be accessed through a special stochastic oracle F : R d ˆRr ˆX Ñ R. The three arguments to F are the decision variable θ P R d , an auxiliary variable ω P R r , and a random variable X over the sample space X . Given any θ, F pθ, ω, Xq returns an unbiased estimate of the gradient ∇hpθq only under a single "correct" setting of ω when the random variable X follows some distribution ξ ∇ θ hpθq " E X"ξ rF pθ, ω ‹ pθq, Xqs,</p><p>where ω ‹ pθq P R r is defined through another stochastic sampling operator G : R d ˆRr ˆX Ñ R, as the solution to the equation E X"ξ rGpθ, ω ‹ pθq, Xs " 0.</p><p>(3) ZENG DOAN</p><p>In this work, the operator G is assumed to be strongly monotone with respect to ω, implying that ω ‹ pθq exists and is unique for any θ. Given any ω ‰ ω ‹ pθq, the stochastic gradient F pθ, ω, Xq may be arbitrarily biased. By the first-order optimality condition, it is obvious that solving (1) is equivalent to finding a pair of solutions pθ ‹ , ω ‹ q that solve the following two coupled nonlinear equations # E X"ξ rF pθ ‹ , ω ‹ , Xqs " 0, E X"ξ rGpθ ‹ , ω ‹ , Xqs " 0.</p><p>(4)</p><p>As detailed in Section 4, a wide range of RL methods, such as gradient-based temporal difference learning <ref type="bibr" coords="2,128.81,211.99,81.84,9.46">(Sutton et al., 2008</ref><ref type="bibr" coords="2,218.33,211.99,25.68,9.46">(Sutton et al., , 2009) )</ref> and actor-critic algorithms for policy optimization <ref type="bibr" coords="2,470.07,211.99,53.30,9.46;2,90.00,225.54,25.35,9.46">(Yang et al., 2019;</ref><ref type="bibr" coords="2,118.26,225.54,71.26,9.46">Wu et al., 2020;</ref><ref type="bibr" coords="2,192.45,225.54,97.91,9.46" target="#b5">Chen and Zhao, 2024)</ref>, aim to solve problems that are special cases of (4). Thus, by developing and analyzing algorithms for the unified framework (4), we can apply them to various RL problems and immediately deduce performance guarantees, without having to tailor analysis for each specific problem. <ref type="bibr" coords="2,253.93,266.19,82.64,9.46" target="#b18">Hong et al. (2023)</ref>; <ref type="bibr" coords="2,344.91,266.19,82.36,9.46">Zeng et al. (2024)</ref> pioneer the effort to construct such a unified framework and study solving (4) with the classic two-time-scale stochastic approximation (SA) algorithm, in which the iterates θ k , ω k are introduced to track θ ‹ , ω ‹ and updated in the following simple manner</p><formula xml:id="formula_2" coords="2,161.03,329.13,361.71,10.77">θ k`1 " θ k ´αk F pθ k , ω k , X k q, ω k`1 " ω k ´βk Gpθ k , ω k , X k q,<label>(5)</label></formula><p>where X k are independently drawn from ξ in every iteration. The step sizes α k and β k need to be chosen carefully according to the structure of h, usually one much smaller than the other, hence leading to the name "two-time-scale". Despite the simplicity and popularity of this algorithm, its convergence rate is (at least as known so far) sub-optimal when compared to standard SGD under the most common oracle where we have access to Hpθ, Xq P R d such that E XPξ rHpθ, Xqs " ∇hpθq.</p><p>(6) This is due to the complex coupling between θ k , ω k , and the noise X k , Taking strongly convex function h as an example, standard SGD with k queries of the stochastic gradient H is able to reduce the optimality error to Op1{kq, while the two-time-scale SA algorithm can only achieve Op1{k 2{3 q.</p><p>Remark 1 Under a Lipschitz/Holder continuous gradient assumption on ω ‹ , Shen and Chen (2022); <ref type="bibr" coords="2,89.61,513.12,77.80,9.40" target="#b16">Han et al. (2024)</ref> show that the standard two-time-scale algorithm achieves Op1{kq for strongly convex functions. We do not make this assumption in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Main Contributions</head><p>First, our work proposes a new variant of the two-time-scale SA algorithm, presented in Algorithm 1, which for the first time achieves the optimal convergence rates when solving (4) under function h satisfying various common structural properties, namely, strong convexity, convexity, Polyak-Lojasiewicz (PL) condition, and general non-convexity. We summarize the rates as well as the convergence metrics and choices of step sizes in Table <ref type="table" coords="2,333.15,628.01,4.16,9.46" target="#tab_0">1</ref>. Our key idea behind the innovation is to leverage an averaging technique to estimate the unknown operators F, G through their samples before updating the decision variables. In other words, we "denoise" the estimate of the stochastic operators F, G, which leads to more stable updates of θ k and ω k . This seemingly simple modification effectively decouples the direct interaction between θ k and ω k and allows us to choose more aggressive step sizes, resulting in the accelerated convergence rates as well as simplified and elegant proofs.</p><p>Second, we show how the results derived for the general framework apply to policy evaluation and optimization problem in reinforcement learning. We discuss three specific applications, the first of which is temporal difference learning with gradient correction (TDC) under linear and non-linear function approximation. In the linear setting, the objective is strongly convex and Algorithm 1 specializes to a variant of the TDC algorithm guaranteed to converge with rate r</p><p>Op1{kq. In the non-linear setting, the objective is non-convex and our convergence rate is r Op1{k 1{2 q. These results match the best-known rates of TDC in both cases. The second application is sample-based policy optimization in the linear-quadratic regulator, which has an objective function satisfying the PL condition. The state-of-the-art online actor-critic algorithm for this problem has a complexity of r Op1{k 2{3 q while our algorithm achieves the much faster rate r</p><p>Op1{kq. The final application is samplebased policy optimization in an entropy-regularized MDP, the objective of which also observes the PL condition. The results in <ref type="bibr" coords="3,198.45,243.76,77.46,9.46">Zeng et al. (2024)</ref> imply that the classic two-time-scale SA algorithm finds the globally optimal policy with rate r Op1{k 2{3 q. Our algorithm in this context enjoys a finite-time rate of r Op1{kq, again significantly elevating the state of the art. Erhp s θ k q ´θ‹ s Opk ´1 2 q k ´1 2 Opk ´1 4 q PL Condition Erhpθ k q ´hpθ ‹ qs Opk ´1q k ´1 Opk ´2 3 q Non-Convexity min tďk Er}∇hpθ t q} 2 s Opk ´1 2 q k ´1 2 Opk ´2 5 q</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Related Work</head><p>This paper is closely related to the existing works on two-time-scale stochastic approximation, stochastic bi-level and composite optimization, as well as those that analyze abstracted or specific RL algorithms. We discuss the latest advances in these subjects to give context to our contributions. Two-time-scale stochastic approximation. The aim of two-time-scale SA, pioneered by works including <ref type="bibr" coords="3,133.81,536.52,60.36,9.46" target="#b1">Borkar (1997)</ref>; <ref type="bibr" coords="3,201.46,536.52,109.86,9.46">Konda and Borkar (1999)</ref>, is to solve a system of equations in the form of (4). Existing works in this domain primarily focus on the settings where F and G are linear operators <ref type="bibr" coords="3,89.64,563.62,123.75,9.46">(Konda and Tsitsiklis, 2004;</ref><ref type="bibr" coords="3,216.12,563.62,77.80,9.46" target="#b8">Dalal et al., 2018;</ref><ref type="bibr" coords="3,296.65,563.62,115.46,9.46" target="#b11">Doan and Romberg, 2019;</ref><ref type="bibr" coords="3,414.83,563.62,77.80,9.46" target="#b9">Dalal et al., 2020;</ref><ref type="bibr" coords="3,495.37,563.62,26.64,9.46;3,90.00,577.17,51.68,9.46" target="#b15">Gupta et al., 2019)</ref> or strongly monotone operators <ref type="bibr" coords="3,283.18,577.17,139.97,9.46">(Mokkadem and Pelletier, 2006;</ref><ref type="bibr" coords="3,425.86,577.17,53.47,9.46" target="#b10">Doan, 2022;</ref><ref type="bibr" coords="3,482.05,577.17,39.96,9.46;3,90.00,590.72,50.89,9.46">Shen and Chen, 2022)</ref>. Our work can be seen as a generalization of this line of work to the case where F is not a strongly monotone operator but the gradient map of a function h exhibiting various structure.</p><p>Stochastic bi-level and composite optimization. Bi-level optimization <ref type="bibr" coords="3,407.17,617.81,89.01,9.46" target="#b7">(Colson et al., 2007;</ref><ref type="bibr" coords="3,498.91,617.81,23.10,9.46;3,90.00,631.36,51.15,9.46" target="#b3">Chen et al., 2022a</ref><ref type="bibr" coords="3,148.78,631.36,25.45,9.46" target="#b4">Chen et al., , 2023) )</ref> studies solve an optimization program of the form</p><formula xml:id="formula_3" coords="3,180.29,651.20,342.44,12.90">min x f 1 px, y ‹ pxqq subject to y ‹ pxq P argmin y f 2 px, yq,<label>(7)</label></formula><p>or equivalently (by the first-order condition), finding a pair of stationary points px ‹ , y ‹ q that observes</p><formula xml:id="formula_4" coords="3,217.56,693.58,176.88,12.46">∇ x f 1 px ‹ , y ‹ q " 0, ∇ y f 2 px ‹ , y ‹ q " 0.</formula><p>This is a special case of our objective (4) with G being a gradient map. In RL problems G usually abstracts the Bellman backup operator associated with policy evaluation, which is well-known to not be the gradient of any function. In this sense, our framework is more general and suitable for modeling applications in RL.</p><p>As another related subject, stochastic composite optimization <ref type="bibr" coords="4,381.57,148.92,112.07,9.46" target="#b13">(Ghadimi and Lan, 2012;</ref><ref type="bibr" coords="4,496.38,148.92,25.63,9.46;4,90.00,162.47,51.51,9.46">Wang et al., 2017;</ref><ref type="bibr" coords="4,144.24,162.47,77.87,9.46" target="#b2">Chen et al., 2021)</ref> studies problems structured as</p><formula xml:id="formula_5" coords="4,272.95,184.07,66.10,15.54">min x g 1 pg 2 pxqq.</formula><p>At a first glance, this optimization objective reduces to (7) by choosing g 1 " f 1 , g 2 pxq " px, y ‹ pxqq, and y ‹ pxq to be the minimizer of f 2 px, ¨q. Nevertheless, there is a key difference in the stochastic oracle. The assumption in stochastic composite optimization is that g 2 is continuous differentiable and that we can access an oracle that returns a direct stochastic estimate of ∇g 2 . This assumption is restrictive in most RL applications, in which there is no guarantee on the differentiability of g 2 and only indirect information of g 2 is accessible. Finite-Time Analysis of RL algorithms. Last but not least, we note the connection of our paper to the vast volume of recent literature that present finite-time and finite-sample analysis for various RL algorithms including temporal difference learning <ref type="bibr" coords="4,305.08,318.03,81.06,9.46">(Wang et al., 2021;</ref><ref type="bibr" coords="4,388.70,318.03,79.41,9.46" target="#b17">Haque et al., 2023)</ref>, actor-critic algorithms for standard <ref type="bibr" coords="4,192.50,331.57,97.53,9.46">MDP (Wu et al., 2020;</ref><ref type="bibr" coords="4,292.73,331.57,93.74,9.46" target="#b5">Chen and Zhao, 2024)</ref>, and actor-critic algorithms for the linear-quadratic regulator <ref type="bibr" coords="4,222.29,345.12,82.15,9.46">(Yang et al., 2019;</ref><ref type="bibr" coords="4,307.26,345.12,65.33,9.46" target="#b19">Ju et al., 2022;</ref><ref type="bibr" coords="4,375.40,345.12,87.12,9.46">Zhou and Lu, 2023)</ref>. As samples in RL are sequentially drawn from a Markov chain, many of these works drop the i.i.d. noise assumption and realistically assume that the samples X k are Markovian. Markovian samples can either be "time-invariant" in the sense that the stationary distribution of the Markov chain does not change over time (for example in policy evaluation or off-policy learning) or "controlled" if the samples are obtained from an Markov chain whose stationary distribution changes as the policy is updated (for example in actor-critic algorithms). To focus on the main contribution which is the acceleration due to stable estimates of operators F and G, our work assumes i.i.d. noise for simplicity. We note that the techniques to handle "time-invariant" and "controlled" Markovian noise have been well-known since Srikant and Ying (2019) and <ref type="bibr" coords="4,343.43,467.07,71.65,9.46">Zou et al. (2019)</ref>. Extending the analysis of Algorithm 1 to the Markovian noise setting can be done in a straightforward way and only makes the bounds worse by a logpkq factor. We discuss this extension in more details in Remark 2.</p><p>Outline of the paper. The rest of the paper is organized as follows. In Section 2 we present the proposed fast two-time-scale optimization algorithm. In Section 3 we study the finite-time convergence rates of the algorithm under various functional structure. Section 4 discusses how the optimization framework (1)-(3) applies to RL problems and how the proposed algorithm solves these problems faster than (or as fast as) the prior arts. Finally, numerical simulations verifying the superiority of the proposed algorithm are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Faster Two-Time-Scale Stochastic Gradient Method</head><p>We formally present the fast two-time-scale stochastic gradient method in Algorithm 1, in which we maintain and iteratively update four variables: θ k and ω k are used to track θ ‹ and ω ‹ pθ ‹ q, while f k and g k estimate the stochastic operators F and G at the current iterates pθ k , ω k q. An averaging step is carried out on the estimate of the stochastic operators. Note that Algorithm 1 reduces to the standard two-time-scale SA algorithm (5) if we set λ k " 1 for all k. With a large λ k the stochastic operator F pθ k , ω k , X k q estimates the true gradient ∇hpθ k q with high variance. By carefully choosing λ k to decay sufficiently fast, the algorithm updates the main variables with much more stable gradient estimates, which is the key driver behind the accelerated convergence rates.</p><p>Algorithm 1 Fast Two-Time-Scale Stochastic Gradient Method 1: Initialization: decision variables θ 0 P R d and ω 0 , operator estimation variables f 0 P R d and</p><formula xml:id="formula_6" coords="5,96.12,161.03,192.37,51.99">g 0 P R r , step size sequences tα k , β k , λ k u 2: for k " 0, 1, ¨¨¨, K ´1 do 3: Independent sample draw X k " ξ 4:</formula><p>Decision variable update:</p><formula xml:id="formula_7" coords="5,96.12,223.13,426.62,32.52">θ k`1 " θ k ´αk f k , ω k`1 " ω k ´βk g k .<label>(8) 5:</label></formula><p>Gradient estimation variable update:</p><formula xml:id="formula_8" coords="5,131.82,265.77,386.67,10.77">f k`1 " p1 ´λk qf k `λk F pθ k , ω k , X k q, g k`1 " p1 ´λk q g k `λk Gpθ k , ω k , X k q. (<label>9</label></formula><formula xml:id="formula_9" coords="5,518.49,266.14,4.24,9.46">)</formula><p>6: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Finite-Time Complexity</head><p>In this section we study the finite-time complexity of Algorithm 1 under various structural properties of the objective function, namely, strong convexity, convexity, PL condition, and general nonconvexity. Since the algorithm draws exactly one sample in each iteration, the time complexity translates to an equivalent sample complexity. We first state the main technical assumptions which are all fairly standard in the existing literature.</p><p>Assumption 1 (Lipschitz Continuity) There exists a positive constant L such that for any θ 1 , θ 2 P R d , ω 1 , ω 2 P R r , and X P X</p><formula xml:id="formula_10" coords="5,167.74,449.99,276.51,60.25">}F pθ 1 , ω 1 , Xq ´F pθ 2 , ω 2 , Xq} ď L p}θ 1 ´θ2 } `}ω 1 ´ω2 }q , }Gpθ 1 , ω 1 , Xq ´Gpθ 2 , ω 2 , Xq} ď L p}θ 1 ´θ2 } `}ω 1 ´ω2 }q , }∇hpθ 1 q ´∇hpθ 2 q} ď L}θ 1 ´θ2 }, }ω ‹ pθ 1 q ´ω‹ pθ 2 q} ď L}θ 1 ´θ2 }.</formula><p>Assumption 1 states that the operators F , G, and ω ‹ are Lipschitz continuous and the function h has Lipschitz gradients. This assumption is very common among works on two-time-scale SA <ref type="bibr" coords="5,496.44,534.84,25.56,9.46;5,90.00,548.39,50.96,9.46">(Zeng et al., 2024;</ref><ref type="bibr" coords="5,143.69,548.39,53.32,9.46" target="#b10">Doan, 2022;</ref><ref type="bibr" coords="5,199.73,548.39,76.28,9.46" target="#b18">Hong et al., 2023)</ref>, and holds true in the RL applications to be discussed in Section 4. Without any loss of generality, we assume L ě 1. We slightly abuse the notation to write F pθ, ωq fi E X"ξ rF pθ, ω, Xqs, Gpθ, ωq fi E X"ξ rGpθ, ω, Xqs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 1 obviously implies</head><formula xml:id="formula_11" coords="5,183.39,622.63,247.03,27.17">}F pθ 1 , ω 1 q ´F pθ 2 , ω 2 q} ď L p}θ 1 ´θ2 } `}ω 1 ´ω2 }q , }Gpθ 1 , ω 1 q ´Gpθ 2 , ω 2 q} ď L p}θ 1 ´θ2 } `}ω 1 ´ω2 }q .</formula><p>Assumption 2 (Bounded Variance) There exists a positive constant B such that for all θ P R d , ω P R r , and X P X E X"ξ r}F pθ, ω, Xq ´F pθ, ωq} 2 s ď B, E X"ξ r}Gpθ, ω, Xq ´Gpθ, ωq} 2 s ď B.</p><p>The second assumption states that the operators F and G have bounded variance. This assumption is again standard in the literature on stochastic gradient algorithms under i.i.d. noise <ref type="bibr" coords="6,458.60,108.27,64.77,9.46;6,90.00,121.82,24.48,9.46">(Rakhlin et al., 2012;</ref><ref type="bibr" coords="6,117.21,121.82,109.36,9.46">Shamir and Zhang, 2013;</ref><ref type="bibr" coords="6,229.29,121.82,97.24,9.46">Hazan and Kale, 2014)</ref>. We use H k " tX 0 , X 1 , ¨¨¨, X k u to denote the randomness information observed until iteration k.</p><p>Assumption 3 (Strong Monotonicity) There exists a constant µ G ą 0 such that xE X"ξ rGpθ, ω, Xqs, ω ´ω‹ pθqy ě µ G }ω ´ω‹ pθq} 2 , @θ P R d , ω P R r .</p><p>(10)</p><p>Assumption 3 requires the operator G to be strongly monotone in expectation. If G is the gradient of a function ζ, i.e. there exists a function ζ : R d ˆRr Ñ R such that ∇ ω ζpθ, ωq " Gpθ, ωq for all θ, ω, then (10) amounts to the strong convexity of ζ. Again, this assumption can be verified to hold in all RL applications discussed in Section 4.</p><p>To simplify notations we introduce the residual variables, which will all be driven to 0</p><formula xml:id="formula_12" coords="6,143.22,270.31,325.55,13.27">∆f k " f k ´F pθ k , ω k q , ∆g k " g k ´G pθ k , ω k q , y k " }ω k ´ω‹ pθ k q} 2 .</formula><p>(11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Strong Convexity</head><p>We first analyze the convergence of Algorithm 1 when the upper level objective is strongly convex, under the following step sizes</p><formula xml:id="formula_13" coords="6,179.68,348.77,343.06,25.24">λ k " c λ k `τ `1 , α k " c α k `τ `1 , β k " c β k `τ `1 .<label>(12)</label></formula><p>Here c α , c β , c α , and τ ě 1 are properly selected constants depending only on the structure of h and G. Their exact choices are presented in Appendix A.</p><p>Assumption 4 (Strong Convexity) The upper level objective h is strongly convex, i.e. there exists a constant µ h ą 0 such that</p><formula xml:id="formula_14" coords="6,168.88,449.09,349.31,13.27">x∇hpθ 1 q ´∇hpθ 2 q, θ 1 ´θ2 y ě µ h }θ 1 ´θ2 } 2 , @θ 1 , θ 2 P R d . (<label>13</label></formula><formula xml:id="formula_15" coords="6,518.19,451.93,4.54,9.46">)</formula><p>Theorem 1 We recall the definition of residual variables in (11) and denote z k " }θ k ´θ‹ } 2 . Under Assumptions 1-4 and the step sizes in (12), the iterates of Algorithm 1 satisfy for all k</p><formula xml:id="formula_16" coords="6,150.14,505.45,372.59,26.99">Erz k`1 s ď Bτ 2 k `τ `1 `τ 2 pk `τ `1q 2 `}∆f 0 } 2 `}∆g 0 } 2 `z0 `y0 ˘.<label>(14)</label></formula><p>The theorem states that when h is strongly convex the iterate of Algorithm 1 converge to the globally optimal solution θ ‹ in the sense of }θ k ´θ‹ } 2 with a rate of Op1{kq, which matches the complexity of the standard SGD under the oracle H in (6) <ref type="bibr" coords="6,296.16,569.70,91.97,9.46">(Rakhlin et al., 2012)</ref>. This rate is much better than the best-known complexity of the standard two-time-scale SA algorithm established in <ref type="bibr" coords="6,475.15,583.25,48.76,9.46;6,89.64,596.80,32.76,9.46" target="#b18">Hong et al. (2023);</ref><ref type="bibr" coords="6,125.96,596.80,80.21,9.46">Zeng et al. (2024)</ref>, which is Op1{k 2{3 q. (1)-(3) under a strongly convex h abstracts the objective of gradient-based temporal difference learning in RL, which we present in Section 4.1.</p><p>Remark 2 We note that Markovian/i.i.d. sampling results in a difference in the analysis of f k , g k in Lemma 2 and 3. In the proof of Lemma 2 in Appendix A.3 for example, it affects how we handle the cross term</p><formula xml:id="formula_17" coords="6,119.59,675.61,366.64,30.12">b k " x∆f k , F pθ, ω k , X k q ´F pθ, ω k qy ñ # Erb k s " 0 under i.i.d samples Erb k s ‰ 0 under Markovian samples</formula><p>When X k is from a Markov chain with ξ as the stationary distribution, Erb k s captures the gap between ξ and the distribution of X k . This gap decays exponentially, under the assumption that the Markov chain is geometrically ergodic, which is a standard assumption in the literature <ref type="bibr" coords="7,492.22,121.62,29.78,9.40;7,90.00,135.17,55.00,9.40">(Xiong et al., 2021;</ref><ref type="bibr" coords="7,148.40,135.17,87.45,9.40" target="#b6">Chen et al., 2022b;</ref><ref type="bibr" coords="7,239.24,135.17,78.65,9.40">Zeng et al., 2022)</ref>. Using techniques first developed in Srikant and Ying (2019), we can show that Er}b k }s ď Opα k logp1{α k qq. The rest of the proof of Theorem 1 can proceed without modification, and eventually we have Erz k`1 s ď Oplogpk `1q{pk `1qq, only differing from (14) by a logarithm factor. The same extension can be made similarly on the results to be presented later in this section for convex, PL, and non-convex functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convexity</head><p>When h is convex, we show the convergence to the globally optimal value function. The step sizes are</p><formula xml:id="formula_18" coords="7,168.57,250.39,349.62,25.73">λ k " 1 4pK `1q 1{2 , α k " c α pK `1q 1{2 , β k " c β pK `1q 1{2 , (<label>15</label></formula><formula xml:id="formula_19" coords="7,518.19,258.32,4.54,9.46">)</formula><p>where the parameters c α , c β need to be small enough that</p><formula xml:id="formula_20" coords="7,90.00,283.51,433.36,35.17">c α ď c β , c β ď 1 72pL`1q 6 , c β ď 1{4, c β ď 1{µ G , and p16 `23pL`1q 6 µ G q c 2 α c β `12pL `1q 6 c 2 α ď 1.</formula><p>Note that we use horizon-aware constant step sizes in this case, selected a priori with respect to the total number of iterations K.</p><p>Theorem 2 Suppose the function h is convex in θ. We recall the definition of residual variables in (11) and denote z k " }θ k ´θ‹ } 2 . We also define w k "</p><formula xml:id="formula_21" coords="7,322.73,351.45,209.77,19.79">1 p1`1 K`1 q k and s θ K " p ř K k"0 w k θ k q{p ř K k 1 "0 w k 1 q.</formula><p>Under Assumptions 1-3 and the step sizes in (15), the iterates of Algorithm 1 satisfy</p><formula xml:id="formula_22" coords="7,125.10,388.92,361.81,29.32">Erhp s θ K q ´hpθ ‹ qs ď 1 8c 2 α ? K `1 `}∆f 0 } 2 `}∆g 0 } 2 `z0 `y0 ˘`Bλ 2 0 8c α ? K `1 .</formula><p>Under a convex objective, Algorithm 1 is guaranteed to find the globally optimal objective function with rate Op1{K 1{2 q, again matching the complexity of standard SGD under the oracle (6) (Lan, 2020). Compared with the standard two-time-scale SA algorithm which has been shown to converge with rate Op1{K 1{4 q <ref type="bibr" coords="7,224.81,466.34,81.06,9.46" target="#b18">(Hong et al., 2023;</ref><ref type="bibr" coords="7,308.58,466.34,74.33,9.46">Zeng et al., 2024)</ref>, our proposed algorithm enjoys significantly improved time and sample complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Non-Convexity with Polyak-Lojasiewicz Condition</head><p>We now shift to the non-convex regime. The optimal solution to (1) may not be unique without the strong convexity assumption. Let Θ ‹ Ď R d denote the set of optimal solutions to (1) and h ‹ denote the optimal function value. We study the complexity of Algorithm 1 when the upper-level objective function of (1) satisfies the PL condition below.</p><p>Assumption 5 (Polyak-Lojasiewicz Condition) There exists a constant µ h ą 0 such that 1 2 }∇hpθq} 2 ě µ h phpθq ´h‹ q , @θ P R d .</p><p>We study Algorithm 1 under the following choice of step sizes</p><formula xml:id="formula_23" coords="7,179.68,650.64,230.63,25.24">λ k " c λ k `τ `1 , α k " c α k `τ `1 , β k " c β k `τ</formula><p>`1 , where c α , c β , c λ , and τ ě 1 again need to be properly selected according to the structure of h and G. Detailed choice of the parameters can be found in Appendix C.</p><p>Theorem 3 We recall the residual variables defined in (11) and denote x k " hpθ k q ´h‹ . Under Assumptions 1-3, Assumption 5, and the step sizes above, the iterates of Algorithm 1 satisfy for all k</p><formula xml:id="formula_24" coords="8,148.98,128.88,314.05,26.99">Erx k`1 s ď Bτ 2 k `τ `1 `τ 2 pk `τ `1q 2 `}∆f 0 } 2 `}∆g 0 } 2 `x0 `y0 ˘.</formula><p>The theorem states that under the PL condition hpθ k q converges with rate Op1{kq to the globally optimal function value h ‹ . This again outperforms the standard two-time-scale SA algorithm which has a complexity of Op1{k 2{3 q. Functions observing the PL condition include the policy optimization objectives in the linear-quadratic regulator and entropy-regularized MDP, which are the subjects of discussion in Section 4.2 and 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">General Nonconvexity</head><p>For non-convex functions without additional structure, we consider step sizes that decay as 1{ ? k</p><formula xml:id="formula_25" coords="8,174.60,287.48,348.13,25.52">λ k " 1 4pk `1q 1{2 , α k " α 0 pk `1q 1{2 , β k " β 0 pk `1q 1{2 ,<label>(16)</label></formula><p>with α 0 , β 0 chosen such that α 0 ď β 0 ,</p><formula xml:id="formula_26" coords="8,259.37,324.28,139.27,15.33">β 0 ď 1 72L , β 0 ď λ 0 , α 0 ď 1 72L 2 .</formula><p>Theorem 4 Under Assumptions 1-3 and the step sizes in (16), we have for all k</p><formula xml:id="formula_27" coords="8,100.23,373.70,411.54,25.98">min tďk E " }∇hpθ t q} 2 ‰ ď 32 α 0 pk `1q 1{2 `}∆f 0 } 2 `}∆g 0 } 2 `phpθ 0 q ´h‹ q `y0 ˘`4 logpk `2q α 0 pk `1q 1{2 .</formula><p>The convergence in the non-convex case is not global, but rather to a stationary point of h. The state-of-the-art convergence rate of the standard two-time-scale SA algorithm is Op1{k 2{5 q <ref type="bibr" coords="8,493.74,425.82,28.26,9.46;8,90.00,439.37,52.41,9.46" target="#b18">(Hong et al., 2023;</ref><ref type="bibr" coords="8,145.12,439.37,76.59,9.46">Zeng et al., 2024)</ref>, which we improve to Op1{k 1{2 q. Applications of the optimization framework for non-convex functions include gradient-based policy evaluation under non-linear function approximation which we present in Section 4.1 and sample-based policy optimization under the standard MDP <ref type="bibr" coords="8,172.11,480.02,73.08,9.46">(Wu et al., 2020;</ref><ref type="bibr" coords="8,247.92,480.02,95.59,9.46" target="#b5">Chen and Zhao, 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Motivating Applications</head><p>4.1. Gradient-Based Policy Evaluation in Discounted-Reward MDP Temporal difference learning with gradient correction (TDC) is a popular algorithm for policy evaluation in the setting where samples are collected by a behavior policy different from the one to be evaluated. Originally designed to work with linear function approximation <ref type="bibr" coords="8,389.05,573.36,79.65,9.46">(Sutton et al., 2008</ref><ref type="bibr" coords="8,476.22,573.36,23.01,9.46">(Sutton et al., , 2009))</ref>, this method has later been extended to the non-linear function approximation setting in <ref type="bibr" coords="8,444.94,586.91,74.52,9.46">Maei et al. (2009)</ref>.</p><p>Consider a Markov decision process (MDP) characterized by M " pS, A, P, r, γq, where S, A are the state and action space, P : S ˆA Ñ ∆ S is the transition kernel, r : S ˆA Ñ r0, 1s is the reward function, and γ P p0, 1q is the discount factor. In the linear function approximation case, each state s P S is associated with a feature vector ϕpsq P R d . We denote by Φ the stacked feature matrix</p><formula xml:id="formula_28" coords="8,218.30,662.52,175.41,45.02">Φ " » - - ´ϕps 1 q T ´φps |S| q T ´fi ffi fl P R |S|ˆd .</formula><p>Policy evaluation studies finding the value function of a policy π P ∆ S A . Given a parameter θ P R d , the approximated value function is Φθ P R |S| . Let Π denote the orthogonal projection onto the subspace spanned by Φ. The objective of TDC is to find a parameter θ that minimizes the mean square projected Bellman error</p><formula xml:id="formula_29" coords="9,236.01,153.62,286.72,18.85">min θ Jpθq fi }Φθ ´ΠT π Φθ} 2 µπ ,<label>(17)</label></formula><p>where µ π denotes the stationary distribution of states under policy π and }v} 2 µπ "</p><formula xml:id="formula_30" coords="9,90.00,180.25,431.71,27.32">ř sPS µ π psqvpsq 2 for any v P R |S| .</formula><p>As noted in Sutton et al. ( <ref type="formula" coords="9,220.28,211.29,18.29,9.46">2009</ref>), unbiased stochastic gradients of J cannot be directly obtained using samples. Instead, we need to introduce an auxiliary variable ω P R d and solve</p><formula xml:id="formula_31" coords="9,107.71,243.76,415.02,12.60">∇ θ Jpθq " ´2E π rrps, aq `γϕps 1 q J θϕpsq ´ϕpsq J θϕpsqs `2γE π rϕps 1 qϕpsq J s ω " 0,<label>(18)</label></formula><formula xml:id="formula_32" coords="9,163.62,261.81,354.56,12.46">E π rϕpsqϕpsq J s ω " E π rrps, aq `γϕps 1 q J θϕpsq ´ϕpsq J θϕpsqs. (<label>19</label></formula><formula xml:id="formula_33" coords="9,518.19,263.98,4.54,9.46">)</formula><p>It is straightforward to see that the objective ( <ref type="formula" coords="9,298.29,285.08,8.87,9.46" target="#formula_31">18</ref>)-( <ref type="formula" coords="9,320.46,285.08,8.87,9.46" target="#formula_32">19</ref>) and stochastic oracles of the TDC algorithm exactly matches (1)-( <ref type="formula" coords="9,180.84,298.63,3.96,9.46">3</ref>), where the operator in the lower level problem ( <ref type="formula" coords="9,397.38,298.63,9.09,9.46" target="#formula_32">19</ref>) is strongly monotone and the upper level objective is strongly convex. As a result, Algorithm 1 specializes to a variant of the TDC algorithm guaranteed to converge with rate r Op1{kq, which matches the best known convergence rate of TDC under linear function approximation <ref type="bibr" coords="9,303.73,339.28,84.09,9.46" target="#b17">(Haque et al., 2023)</ref>. When non-linear function approximation is considered, TDC solves a system of equations that resemble ( <ref type="formula" coords="9,413.89,352.83,8.52,9.46" target="#formula_31">18</ref>)-( <ref type="formula" coords="9,435.18,352.83,8.52,9.46" target="#formula_32">19</ref>), where the lower level operator is still strongly monotone but the upper level objective is non-convex <ref type="bibr" coords="9,443.19,366.38,76.12,9.46">(Maei et al., 2009)</ref>. The best known convergence rate for TDC under non-linear function approximation is r Op1{k ´1{2 q to a stationary point of J (Wang et al., 2021), which we again match by our result in Theorem 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Policy Optimization for Linear-Quadratic Regulators</head><p>Linear-quadratic regulator (LQR) studies finding an optimal control sequence tu k u that stabilizes a linear system with the minimum cost</p><formula xml:id="formula_34" coords="9,184.02,472.83,338.71,49.23">tu ‹ k u " min tu k u lim T Ñ8 1 T E " T ÿ k"0 `xJ k Qx k `uJ k Ru k ˘| x 0 ı subject to x k`1 " Ax k `Bu k `wk ,<label>(20)</label></formula><p>where</p><formula xml:id="formula_35" coords="9,120.18,531.43,88.54,12.73">x k P R d 1 , u k P R d 2</formula><p>are the state and the control variables, w k P R d 1 is the system noise, A P R d 1 ˆd1 and B P R d 1 ˆd2 are the transition matrices, and</p><formula xml:id="formula_36" coords="9,357.82,544.98,107.27,12.13">Q P R d 1 ˆd1 , R P R d 2 ˆd2</formula><p>are positivedefinite cost matrices. We assume that tw k u is independently and identically distributed according to N p0, Ψq for some covariance matrix</p><formula xml:id="formula_37" coords="9,251.90,572.08,54.10,12.13">Ψ P R d 1 ˆd1 .</formula><p>It is a classic result in the control literature that the optimal control sequence tu ‹ k u is a timeinvariant linear function of the state</p><formula xml:id="formula_38" coords="9,274.22,620.40,248.51,13.60">u ‹ k " ´K‹ x k ,<label>(21)</label></formula><p>for a matrix K ‹ P R d 2 ˆd1 as a function of A, B, Q, R <ref type="bibr" coords="9,325.56,644.69,74.55,9.46" target="#b0">(Bertsekas, 2012)</ref>. Exploiting this fact allows us to re-formulate the LQR objective as finding the optimal control matrix K, which results in an optimization problem of the following form</p><formula xml:id="formula_39" coords="9,193.35,692.54,90.99,16.26">min K tracepP K Ψqq subject to P K " Q `KJ RK `pA ´BKq J P K pA ´BKq.<label>(22)</label></formula><p>To encourage exploration, we consider the regularized objective with σ ě 0 and</p><formula xml:id="formula_40" coords="10,170.46,118.66,352.27,43.12">Ψ σ " Ψ `σ2 BB J min K JpKq fi tracepP K Ψ σ q `σ2 tracepRq subject to (22),<label>(23)</label></formula><p>which is known to have the same optimal solution as the unregularized problem <ref type="bibr" coords="10,432.38,174.65,86.92,9.46" target="#b14">(Gravell et al., 2020)</ref>. We are interested in finding the solution to (23) under unknown transition matrices A and B, by taking a gradient-based approach. The natural gradient of J with respect to K, which we denote by r ∇ K J and which can be regarded as ∇ K J on a slightly transformed system of coordinates, has the following closed-form expression</p><formula xml:id="formula_41" coords="10,210.97,252.60,190.06,12.51">r ∇ K J " 2 `R `BJ P K B ˘K ´2B J P K A.</formula><p>To obtain a reliable stochastic estimate of r ∇ K J the key is to estimate R `BJ P K B and B J P K A. For simplicity of notation, we define</p><formula xml:id="formula_42" coords="10,166.21,318.90,356.52,31.58">Ω K " ˜Ω11 K Ω 12 K Ω 21 K Ω 22 K ¸" ˜Q `AJ P K A A J P K B B J P K A R `BJ P K B ¸,<label>(24)</label></formula><p>of which R `BJ P K B and B J P K A are sub-matrices. We also define svecp¨q as the operation to vectorize the upper triangular sub-matrix of a given symmetric matrix with the off-diagonal entries weighted by ? 2. We further define ϕpx, uq " svecp "</p><p>x J , u J ‰ J "</p><p>x J , u J ‰ q for any x P R d 1 , u P R d 2 , which can be seen as the feature vector for the system in state px, uq. It is shown in <ref type="bibr" coords="10,446.32,409.24,76.42,9.46">Yang et al. (2019)</ref> that Ω K and JpKq jointly satisfy</p><formula xml:id="formula_43" coords="10,124.03,445.64,398.70,30.03">E x"µ K ,u"N p´Kx,σ 2 Iq " M x,u,x 1 ,u 1 ‰ « JpKq svecpΩ K q ff " E x"µ K ,u"N p´Kx,σ 2 Iq rc x,u s ,<label>(25)</label></formula><p>where the matrix M x,u,x 1 ,u 1 and vector c x,u are</p><formula xml:id="formula_44" coords="10,105.71,512.30,400.59,29.68">M x,u,x 1 ,u 1 " « 1 0 ϕpx, uq ϕpx, uq rϕpx, uq ´ϕ px 1 , u 1 qs J ff , c x,u " « x J Qx `uJ Ru `xJ Qx `uJ Ru ˘ϕpx, uq ff .</formula><p>Equation ( <ref type="formula" coords="10,154.98,558.76,9.09,9.46" target="#formula_43">25</ref>) maps to the lower level problem (3). Under the assumption that K is a stable controller with respect to A and B (Yang et al., 2019), the linear operator ErM x,u,x 1 ,u 1 s is strongly monotone <ref type="bibr" coords="10,135.71,587.37,77.96,9.46">(Yang et al., 2019)</ref>. The upper level problem that corresponds to (2) is to solve r ∇ K J " 0, which is known to observe the PL condition <ref type="bibr" coords="10,293.39,600.92,83.52,9.46" target="#b12">(Fazel et al., 2018)</ref>. This means that we can apply Algorithm 1 to this problem, which specializes to an single-looped "actor-critic" algorithm with a finite-time convergence rate of r Op1{kq to the globally optimal solution measured by JpK k q ´Jpθ ‹ q. The complexity of the current state-of-the-art single-looped algorithm for LQR is r Op1{k 2{3 q, which we improve over. We also note the recent few works on nested-loop actor-critic algorithms that rely on batched multi-trajectory samples <ref type="bibr" coords="10,274.32,668.66,69.92,9.46" target="#b19">(Ju et al., 2022;</ref><ref type="bibr" coords="10,347.36,668.66,88.00,9.46">Zhou and Lu, 2023)</ref>. These algorithms achieve the complexity r Op1{kq as well, but in practice are much less convenient to implement than our algorithm which is single-looped and only requires a single online trajectory of samples.</p><p>Algorithm 2 Fast Single-Loop Actor-Critic Algorithm for LQR 1: Initialization: Control matrix K 0 , auxiliary variable Ω0 , Ĵ0 , gradient estimation variable f 0 , g J 0 , g Ω 0 , step size sequences tα k , β k , λ k u 2: Sample an initial state x 0 " D, takes an initial control u 0 " N p´K 0 x 0 , σ 2 Iq, and observe cost c 0 " x J 0 Qx 0 `uJ 0 Ru 0 and next state x 1 " N pAx 0 `Bu 0 , Ψq 3: for k " 0, 1, 2, ... do</p><formula xml:id="formula_45" coords="11,96.12,178.77,799.07,39.44">4: Take the control u k`1 " N p´K k x k`1 , σ 2 Iq. Observe cost c k`1 " x J k`1 Qx k`1 ùJ k`1 Ru k`1 and next state x k`2 " N pAx k`1 `Bu k`1 , Ψq 5:</formula><p>Control matrix update:</p><formula xml:id="formula_46" coords="11,96.12,228.36,268.88,27.52">K k`1 " K k ´αk f k 6:</formula><p>Auxiliary variable update:</p><formula xml:id="formula_47" coords="11,96.12,263.26,317.95,35.26">Ĵk`1 " Ĵk ´βk g J k , Ωk`1 " Ωk ´βk g Ω k 7:</formula><p>Gradient estimation variable update:</p><formula xml:id="formula_48" coords="11,120.30,305.90,801.51,69.10">f k`1 " p1 ´λk qf k `λk p Ω22 k K k ´Ω 21 k q g J k`1 " p1 ´λk qg J k `λk p Ĵk ´xJ k Qx k ´uJ k Ru k q g Ω k`1 " p1 ´λk qg Ω k ´λk « x k u k ff« x k u k ff J ˜"x J k u J k ‰ Ωk « x k u k ff `Ĵ k ´xJ k Qx k ´uJ k Ru k 8:</formula><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Policy Optimization for Entropy-Regularized MDPs</head><p>The optimization framework (1)-( <ref type="formula" coords="11,239.03,445.01,4.16,9.46">3</ref>) also applies to the policy optimization problem in the entropyregularized MDP. Consider the MDP M " pS, A, P, r, γq introduced in Section 4.1. The regularized value function of a policy π P ∆ S A is</p><formula xml:id="formula_49" coords="11,121.35,497.65,369.31,32.22">V π τ psq " E a k "πp¨|s k q,s k`1 "Pp¨|s k ,a k q " 8 ÿ k"0 γ k `rps k , a k q ´τ log πpa k | s k q ˘| s 0 " s ı ,</formula><p>where τ is a positive regularization weight. Under initial state distribution ρ P ∆ S , the expected cumulative reward under policy π is J τ pπq fi E s"ρ rV π τ psqs. We consider the tabular setting where the policy is represented through the softmax function by a parameter θ P R |S|ˆ|A| π θ pa | sq " exp pθ s,a q ř a 1 PA exp `θs,a 1 ˘.</p><p>Policy optimization studies finding the parameter θ ‹ that maximizes the cumulative reward J τ pπ θ q. By the first-order condition, this is equivalent to solving </p><formula xml:id="formula_50" coords="11,93.70,678.43,424.60,25.04">∇ θ J τ pπ θ q " 1 1 ´γ E π θ " `rps, aq ´τ log π θ pa | sq `γV π θ τ ps 1 q ´V π θ τ psq ˘∇θ log π θ pa | sq ‰ " 0.</formula><formula xml:id="formula_51" coords="12,178.40,273.79,339.79,16.79">E π θ " rps, aq ´τ log π θ pa | sq `γV π θ τ ps 1 q ´V π θ τ psq ı " 0. (<label>26</label></formula><formula xml:id="formula_52" coords="12,518.19,279.23,4.54,9.46">)</formula><p>This objective again falls under the general two-time-scale optimization framework, with the operator in lower level problem (26) being strongly monotone. The upper level objective function J τ satisfies the PL condition <ref type="bibr" coords="12,203.36,335.86,74.49,9.46">(Mei et al., 2020)</ref>. Applying Algorithm 1 to this problem results in a fast actor-critic algorithm (Algorithm 2) that is able to reduce the optimality gap J τ pθ k q ´Jτ pθ ‹ q with rate r</p><p>Op1{kq. The analysis in Zeng et al. ( <ref type="formula" coords="12,272.92,362.96,19.60,9.46">2024</ref>) implies that the standard two-time-scale algorithm (5) solves this problem with a complexity of r Op1{k 2{3 q, which we vastly improve over.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Numerical Simulations</head><p>We experimentally verify that Algorithm 1 converges faster than (5) in solving 1) a policy evaluation problem under linear function approximation, and 2) policy optimization for the LQR.</p><p>Policy evaluation problem under linear function approximation. As we have discussed in Section 4.1, TDC is the state-of-the-art policy evaluation algorithm under off-policy samples which enjoys a convergence rate of r Op1{kq under linear function approximation. Numerically we evaluate Algorithm 1 against TDC on the task of evaluating the value function of a randomly generated policy in a randomly generated environment with |S| " |A| " 50 and the feature dimension d " 10. Figure <ref type="figure" coords="12,121.17,515.26,5.44,9.46" target="#fig_0">1</ref> shows the empirical performance gap between the standard TDC and Algorithm 1, though both algorithm have the same complexity in theory. For comparison we also include TD(0) learning under linear function approximation. The simulation results in Figure <ref type="figure" coords="12,402.38,542.36,5.56,9.46" target="#fig_0">1</ref>  </p><formula xml:id="formula_53" coords="12,324.62,607.51,193.57,42.91">1 0.1 0 0.1 0 0.1 fi ffi fl , Q " I 3 , R " I 2 . (<label>27</label></formula><formula xml:id="formula_54" coords="12,518.19,626.04,4.54,9.46">)</formula><p>We apply Algorithm 2 to this problem along with online actor-critic algorithm studied in Zeng et al. <ref type="bibr" coords="12,114.08,682.21,27.51,9.46">(2024)</ref>, which is equivalent to (5) applied to this context. In Figure <ref type="figure" coords="12,410.07,682.21,4.13,9.46">2</ref>, we plot the error decay under the two algorithms, which again shows that the proposed algorithm enjoys faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disclaimer</head><p>This paper was prepared for informational purposes in part by the Artificial Intelligence Research group of JP Morgan Chase &amp; Co and its affiliates ("JP Morgan"), and is not a product of the Research Department of JP Morgan. JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.</p><p>V. R. Konda and J. N. Tsitsiklis. Convergence rate of linear two-time-scale stochastic approximation.</p><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Analysis for Strongly Convex Functions</head><p>The exact choice of the step size parameters is made such that α k ď β k ď λ k ď 1{4, c α ě 8 µ h , and</p><formula xml:id="formula_55" coords="17,105.86,136.83,400.28,59.11">α k ď mint λ k µ h , µ h 6pL `1q 4 , 8µ G µ h β k , µ h µ G 152pL `1q 6 β k , µ G β k 8p8L 4 `9L 2 µ G q , λ k 4 p48L 2 `10L µ G q ´1u, β k ď mint 1 240pL `1q 6 , 1 µ G , λ k 4 p56L 2 `9 2µ h q ´1, λ k 4 p48L 2 `10L µ G q ´1, µ G 168L 4 u,</formula><p>which requires β 0 to be selected sufficiently small and τ sufficiently large. Note that parameters c α , β 0 , and τ always exist and only depend on the structural constants of h and G.</p><p>The proof of Theorem 1 is built on the lemmas below. Their proofs can be found at the end of the section.</p><p>Lemma 1 Under Assumption 1-3, the sequence of variables tθ k , ω k , f k , g k u satisfy for all k</p><formula xml:id="formula_56" coords="17,184.73,282.23,242.53,66.84">}f k } ď }∆f k } `L? y k `LpL `1q ? z k , }g k } ď }∆g k } `L? y k , }θ k`1 ´θk } ď α k p}∆f k } `L? y k `LpL `1q ? z k q , }ω k`1 ´ωk } ď β k p}∆g k } `L? y k q .</formula><p>Lemma 2 Suppose that the step size λ k satisfy λ k ď 1{4 for all k. Then, under Assumption 1-3, we have the following bound on the iteration-wise convergence of Er}∆f k } 2 s.</p><formula xml:id="formula_57" coords="17,122.88,392.92,366.25,55.44">Er}∆f k`1 } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ `2Bλ 2 k .</formula><p>Lemma 3 Suppose that the step size λ k satisfy λ k ď 1{4 for all k. Then, under Assumption 1-3, we have the following bound on the iteration-wise convergence of</p><formula xml:id="formula_58" coords="17,122.88,468.96,366.25,76.09">Er}∆g k } 2 s Er}∆g k`1 } 2 s ď p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ `2Bλ 2 k .</formula><p>Lemma 4 Suppose the step size α k is chosen such that α k ď µ h 6pL`1q 4 and α k ď 1 2µ h for all k. Under Assumption 1-4, we have</p><formula xml:id="formula_59" coords="17,193.48,587.05,225.04,27.59">z k`1 ď p1 ´µh α k 4 qz k `9L 2 α k µ h y k `9α k 2µ h }∆f k } 2 .</formula><p>Lemma 5 Suppose that the step sizes satisfy α k ď β k ď 1 72pL`1q 6 and β k ď 1{µ G for all k. Then, under Assumption 1-3, we have</p><formula xml:id="formula_60" coords="17,93.50,657.29,425.01,51.49">y k`1 ď p1 ´µG β k qy k ´´µ G 4 β k ´8L 4 α k ´L2 β 2 k ¯yk `ˆ19pL `1q 6 α 2 k µ G β k `3pL `1q 6 α k β k ˙zk `ˆ8β k µ G `2Lβ 2 k ˙}∆g k } 2 `8Lα k }∆f k } 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Proof of Theorem 1</head><p>We consider the Lyapunov function</p><formula xml:id="formula_61" coords="18,220.75,139.00,170.50,13.27">V k fi Er}∆f k } 2 `}∆g k } 2 `zk `yk s.</formula><p>Combining Lemma 2-5, we get</p><formula xml:id="formula_62" coords="18,92.62,192.65,430.11,457.91">V k`1 " Er}∆f k`1 } 2 `}∆g k`1 } 2 `zk`1 `yk`1 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ `2Bλ 2 k `p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ `2Bλ 2 k `Erp1 ´µh α k 4 qz k `9L 2 α k µ h y k `9α k 2µ h }∆f k } 2 s `Erp1 ´µG β k qy k ´´µ G 4 β k ´8L 4 α k ´L2 β 2 k ¯yk `ˆ19pL `1q 6 α 2 k µ G β k `3pL `1q 6 α k β k ˙zk `ˆ8β k µ G `2Lβ 2 k ˙}∆g k } 2 `8Lα k }∆f k } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´48L 2 β 2 k λ k ´9α k 2µ h ´8Lα k qEr}∆f k } 2 s `p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´48L 2 β 2 k λ k ´8β k µ G ´2Lβ 2 k qEr}∆g k } 2 s `p1 ´µh α k 8 qErz k s ´p µ h α k 8 ´15pL `1q 6 α k β k ´19pL `1q 6 α 2 k µ G β k qErz k s `p1 ´µG β k qEry k s ´ˆµ G 4 β k ´8L 4 α k ´21L 4 β 2 k ´9L 2 α k µ h ˙Ery k s `4Bλ 2 k ď p1 ´µh α k 8 qV k `4Bλ 2 k ´p λ k 4 ´48L 2 β 2 k λ k ´9α k 2µ h ´8Lα k qEr}∆f k } 2 s ´p λ k 4 ´48L 2 β 2 k λ k ´8β k µ G ´2Lβ 2 k qEr}∆g k } 2 s ´p µ h α k 8 ´15pL `1q 6 α k β k ´19pL `1q 6 α 2 k µ G β k qErz k s ´ˆµ G 4 β k ´8L 4 α k ´21L 4 β 2 k ´9L 2 α k µ h ˙Ery k s,<label>(28)</label></formula><p>where the third inequality simplifies the terms with the conditions α k ď λ k µ h and α k ď 8µ G β k µ h . The terms on the right hand side of (28) except for the first two are non-positive under the step size conditions</p><formula xml:id="formula_63" coords="18,159.47,691.91,363.90,16.70">β k ď 1 µ G , β k ď λ k 4 p56L 2 `9 2µ h q ´1, β k ď λ k 4 p48L 2 `10L µ G q ´1, α k ď µ h µ G 152pL`1q 6 β k , β k ď 1 240pL`1q 6 , β k ď µ G 168L 4 , and α k ď µ G β k 8p8L 4 `9L 2 µ G q . This leads to V k`1 ď p1 ´µh α k 8 qV k `4Bλ 2 k ď p1 ´2 k `τ `1 qV k `4Bλ 2 k ,</formula><p>where the second inequality follows from c α ě 16 µ h . Multiplying by pk `τ `1q 2 on both sides, we have</p><formula xml:id="formula_64" coords="19,152.31,189.99,306.68,89.43">pk `τ `1q 2 V k`1 ď pk `τ `1qpk `τ ´1qV k `4Bpk `τ `1q 2 λ 2 k ď pk `τ q 2 V k `Bpk `τ `1q 2 4pk `1q 2 ď pk `τ q 2 V k `p B 2 `Bτ 2 2 q ď τ 2 V 0 `Bτ 2 pk `1q</formula><p>Dividing by pk `τ `1q 2 now, we have</p><formula xml:id="formula_65" coords="19,89.61,308.57,432.39,126.53">V k`1 ď τ 2 V 0 pk `τ `1q 2 `Bτ 2 pk `1q pk `τ `1q 2 ď τ 2 V 0 pk `τ `1q 2 `Bτ 2 pk `τ `1q , which obviously implies Er}θ k`1 ´θ‹ } 2 s ď τ 2 V 0 pk `τ `1q 2 `Bτ 2 pk `τ `1q . ■ A.2. Proof of Lemma 1</formula><p>By definition,</p><formula xml:id="formula_66" coords="19,89.61,464.08,399.54,104.68">F pθ ‹ , ω ‹ pθqq " ∇hpθ ‹ q " 0, which implies }f k } " }∆f k `F pθ k , ω k q ´F pθ k , ω ‹ pθ k qq `F pθ k , ω ‹ pθ k qq ´F pθ ‹ , ω ‹ pθ ‹ qq} ď }∆f k } `}F pθ k , ω k q ´F pθ k , ω ‹ pθ k qq} `}F pθ k , ω ‹ pθ k qq ´F pθ ‹ , ω ‹ pθ ‹ qq} ď }∆f k } `L}ω k ´ω‹ pθ k q} `LpL `1q}θ k ´θ‹ } ď }∆f k } `L? y k `LpL `1q ? z k ,</formula><p>where the second inequality follows from the Lipschitz continuity of F . Similarly, since Gpθ, ω ‹ pθqq " 0 for any θ, we can derive</p><formula xml:id="formula_67" coords="19,205.88,612.18,200.24,45.68">}g k } " }∆g k `Gpθ k , ω k q ´Gpθ k , ω ‹ pθqq} ď }∆g k } `}Gpθ k , ω k q ´Gpθ k , ω ‹ pθqq} ď }∆g k } `L? y k .</formula><p>The bounds on }θ k`1 ´θk } and }ω k`1 ´ωk } easily follow from the two inequalities above and (8). ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Proof of Lemma 2</head><p>By the update rule in (9), we have</p><formula xml:id="formula_68" coords="20,106.07,138.51,399.87,60.39">∆f k`1 " f k`1 ´F pθ k`1 , ω k`1 q " p1 ´λk qf k `λk F pθ k , ω k , X k q ´F pθ k`1 , ω k`1 q " p1 ´λk qf k `λk F pθ k , ω k q ´F pθ k`1 , ω k`1 q `λk pF pθ k , ω k q ´F pθ k , ω k , X k qq " p1 ´λk q∆f k `F pθ k , ω k q ´F pθ k`1 , ω k`1 q `λk pF pθ k , ω k q ´F pθ k , ω k , X k qq .</formula><p>This leads to</p><formula xml:id="formula_69" coords="20,100.27,235.02,422.46,203.83">}∆f k`1 } 2 " p1 ´λk q 2 }∆f k } 2 `› › › pF pθ k , ω k q ´F pθ k`1 , ω k`1 qq `λk pF pθ k , ω k q ´F pθ k , ω k , X k qq › › › 2 `2p1 ´λk q∆f J k pF pθ k , ω k q ´F pθ k`1 , ω k`1 qq `2p1 ´λk qλ k ∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq ď p1 ´λk q 2 }∆f k } 2 `2}F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 `2λ 2 k }F pθ k , ω k q ´F pθ k , ω k , X k q} 2 `λk 2 }∆f k } 2 `2 λ k }F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq ď p1 ´λk q}∆f k } 2 `2λ 2 k }F pθ k , ω k q ´F pθ k , ω k , X k q} 2 `pλ 2 k ´λk 2 q}∆f k } 2 `4 λ k }F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq ,<label>(29)</label></formula><p>where the last inequality follows from λ k ď 1.</p><p>The second term on the right hand side of ( <ref type="formula" coords="20,296.00,465.74,9.09,9.46" target="#formula_69">29</ref>) is bounded in expectation, i.e.</p><formula xml:id="formula_70" coords="20,207.29,487.60,197.41,14.29">E X k "ξ r}F pθ k , ω k , X k q ´F pθ k , ω k q} 2 s ď B.</formula><p>In addition, the last term of ( <ref type="formula" coords="20,216.67,515.14,9.09,9.46" target="#formula_69">29</ref>) is zero in expectation since</p><formula xml:id="formula_71" coords="20,90.00,513.51,432.00,89.39">Er∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq | H k´1 s " ∆f J k ErpF pθ k , ω k q ´F pθ k , ω k , X k qq | H k´1 s " 0. As a result, we have Er}∆f k`1 } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 2 ´λ2 k qEr}∆f k } 2 s `4 λ k Er}F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 s `2Bλ 2 k . (<label>30</label></formula><formula xml:id="formula_72" coords="20,518.19,585.00,4.54,9.46">)</formula><p>By Lemma 1 and the Lipschitz condition of F ,</p><formula xml:id="formula_73" coords="20,153.45,635.78,305.11,67.34">}F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 ď 2L 2 }θ k`1 ´θk } 2 `2L 2 }ω k`1 ´ωk } 2 ď 6L 2 α 2 k p}∆f k } 2 `L2 y k `pL `1q 4 z k q `4L 2 β 2 k p}∆g k } 2 `L2 y k q ď 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k .</formula><p>Plugging this bound into (30), we get</p><formula xml:id="formula_74" coords="21,115.68,118.14,379.93,127.21">Er}∆f k`1 } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 2 ´λ2 k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ `2Bλ 2 k ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ `2Bλ 2 k ,</formula><p>where the last inequality follows from λ k ď 1{4. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Proof of Lemma 3</head><p>By the update rule in (9), we have</p><formula xml:id="formula_75" coords="21,106.11,350.57,399.78,60.39">∆g k`1 " g k`1 ´Gpθ k`1 , ω k`1 q " p1 ´λk qg k `λk Gpθ k , ω k , X k q ´Gpθ k`1 , ω k`1 q " p1 ´λk qg k `λk Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q `λk pGpθ k , ω k q ´Gpθ k , ω k , X k qq " p1 ´λk q∆g k `Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q `λk pGpθ k , ω k q ´Gpθ k , ω k , X k qq .</formula><p>This leads to</p><formula xml:id="formula_76" coords="21,100.25,450.23,417.94,203.83">}∆g k`1 } 2 " p1 ´λk q 2 }∆g k } 2 `› › › pGpθ k , ω k q ´Gpθ k`1 , ω k`1 qq `λk pGpθ k , ω k q ´Gpθ k , ω k , X k qq › › › 2 `2p1 ´λk q∆g J k pGpθ k , ω k q ´Gpθ k`1 , ω k`1 qq `2p1 ´λk qλ k ∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq ď p1 ´λk q 2 }∆g k } 2 `2}Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 `2λ 2 k }Gpθ k , ω k q ´Gpθ k , ω k , X k q} 2 `λk 2 }∆g k } 2 `2 λ k }Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq ď p1 ´λk q}∆g k } 2 `2λ 2 k }Gpθ k , ω k q ´Gpθ k , ω k , X k q} 2 `pλ 2 k ´λk 2 q}∆g k } 2 `4 λ k }Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq . (<label>31</label></formula><formula xml:id="formula_77" coords="21,518.19,642.63,4.54,9.46">)</formula><p>The second term on the right hand side of ( <ref type="formula" coords="21,296.00,669.49,9.09,9.46" target="#formula_76">31</ref>) is bounded in expectation, i.e.</p><formula xml:id="formula_78" coords="21,207.25,692.91,197.51,14.29">E X k "ξ r}Gpθ k , ω k , X k q ´Gpθ k , ω k q} 2 s ď B.</formula><p>In addition, the last term of ( <ref type="formula" coords="22,217.08,94.72,9.09,9.46" target="#formula_69">29</ref>) is zero in expectation since</p><formula xml:id="formula_79" coords="22,90.00,93.09,432.73,89.41">Er∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq | H k´1 s " ∆g J k ErpGpθ k , ω k q ´Gpθ k , ω k , X k qq | H k´1 s " 0. As a result, we have Er}∆g k`1 } 2 s ď p1 ´λk qEr}∆g k } 2 s ´p λ k 2 ´λ2 k qEr}∆g k } 2 s `4 λ k Er}Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 s `2Bλ 2 k .<label>(32)</label></formula><p>By Lemma 1 and the Lipschitz condition of G,</p><formula xml:id="formula_80" coords="22,153.45,215.41,305.11,67.34">}Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 ď 2L 2 }θ k`1 ´θk } 2 `2L 2 }ω k`1 ´ωk } 2 ď 6L 2 α 2 k p}∆f k } 2 `L2 y k `pL `1q 4 z k q `4L 2 β 2 k p}∆g k } 2 `L2 y k q ď 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k .</formula><p>Plugging this bound into (32), we get</p><formula xml:id="formula_81" coords="22,106.11,317.80,399.78,109.09">Er}∆g k`1 } 2 s ď p1 ´λk qEr}∆g k } 2 s ´p λ k 2 ´λ2 k qEr}∆g k } 2 s `2Bλ 2 k `4 λ k E " 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ ď p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ `2Bλ 2 k ,</formula><p>where the last inequality follows from λ k ď 1{4. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Proof of Lemma 4</head><p>We have by the update rule in (8) and the definition of the residual variables in (11)</p><formula xml:id="formula_82" coords="22,106.50,521.88,411.69,67.34">z k`1 " }θ k`1 ´θ‹ } 2 " }θ k ´αk f k ´θ‹ } 2 " }θ k ´αk F pθ k , ω k q ´αk ∆f k ´θ‹ } 2 " }θ k ´θ‹ ´αk F pθ k , ω k q} 2 ´2α k xθ k ´θ‹ ´αk F pθ k , ω k q, ∆f k y `α2 k }∆f k } 2 . (<label>33</label></formula><formula xml:id="formula_83" coords="22,518.19,577.80,4.54,9.46">)</formula><p>To bound the first term on the right hand side of (33),</p><formula xml:id="formula_84" coords="22,108.95,624.46,393.59,81.73">}θ k ´θ‹ ´αk F pθ k , ω k q} 2 " }θ k ´θ‹ ´αk pF pθ k , ω ‹ pθ k qq ´F pθ ‹ , ω ‹ pθ ‹ qqq `αk pF pθ k , ω ‹ pθ k qq ´F pθ k , ω k qq } 2 " z k `α2 k }F pθ k , ω ‹ pθ k qq ´F pθ ‹ , ω ‹ pθ ‹ qq} 2 `α2 k }F pθ k , ω ‹ pθ k qq ´F pθ k , ω k q} 2 ´2α k xθ k ´θ‹ , F pθ k , ω ‹ pθ k qq ´F pθ ‹ , ω ‹ pθ ‹ qqy `2α k xθ k ´θ‹ , F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy ´2α 2 k xF pθ k , ω ‹ pθ k qq ´F pθ ‹ , ω ‹ pθ ‹ q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy ď z k `α2 k }F pθ k , ω ‹ pθ k qq ´F pθ ‹ , ω ‹ pθ ‹ qq} 2 `α2 k }F pθ k , ω ‹ pθ k qq ´F pθ k , ω k q} 2 ´2µ h α k z k `µh α k z k `αk µ h }F pθ k , ω ‹ pθ k qq ´F pθ k , ω k q} 2 `α2 k z k `α2 k }F pθ k , ω ‹ pθ k qq ´F pθ k , ω k q} 2 ď p1 ´µh α k `α2 k qz k `2L 2 pL `1q 2 α 2 k z k `p2L 2 α 2 k `L2 α k µ h qy k ď p1 ´µh α k `3pL `1q 4 α 2 k qz k `p2L 2 α 2 k `L2 α k µ h qy k ,</formula><p>where the first equality follows from F pθ ‹ , ω ‹ pθ ‹ qq " 0, the first inequality plugs in ( <ref type="formula" coords="23,467.86,240.59,9.09,9.46" target="#formula_14">13</ref>) and uses the fact that 2}a}}b} ď c}a} 2 `1 c }b} 2 for any vector a, b and scalar c ą 0, and the second inequality follows from the Lipschitz continuity of F and ω ‹ . Choosing the step size such that α k ď µ h 6pL`1q 4 and α k ď 1 2µ h results in</p><formula xml:id="formula_85" coords="23,178.82,308.72,339.37,27.59">}θ k ´θ‹ ´αk F pθ k , ω k q} 2 ď p1 ´µh α k 2 qz k `2L 2 α k µ h y k . (<label>34</label></formula><formula xml:id="formula_86" coords="23,518.19,318.40,4.54,9.46">)</formula><p>The second term on the right hand side of ( <ref type="formula" coords="23,296.00,348.01,9.09,9.46" target="#formula_82">33</ref>) can be treated based on (34)</p><formula xml:id="formula_87" coords="23,177.55,372.66,340.64,101.04">´2α k xθ k ´θ‹ ´αk F pθ k , ω k q, ∆f k y ď µ h α k 4 }θ k ´θ‹ ´αk F pθ k , ω k q} 2 `4α k µ h }∆f k } 2 ď µ h α k 4 p1 ´µh α k 2 qz k `µh α k 4 ¨2L 2 α k µ h y k `4α k µ h }∆f k } 2 ď p µ h α k 4 ´µ2 h α 2 k 8 qz k `L2 α 2 k 2 y k `4α k µ h }∆f k } 2 . (<label>35</label></formula><formula xml:id="formula_88" coords="23,518.19,455.79,4.54,9.46">)</formula><p>Plugging ( <ref type="formula" coords="23,153.61,485.40,9.09,9.46" target="#formula_85">34</ref>) and ( <ref type="formula" coords="23,192.99,485.40,9.09,9.46" target="#formula_87">35</ref>) into (33), we get</p><formula xml:id="formula_89" coords="23,148.47,507.01,314.55,87.91">z k`1 ď p1 ´µh α k 2 qz k `2L 2 α k µ h y k `p µ h α k 4 ´µ2 h α 2 k 8 qz k `L2 α 2 k 2 y k `4α k µ h }∆f k } 2 `α2 k }∆f k } 2 ď p1 ´µh α k 4 qz k `9L 2 α k µ h y k `9α k 2µ h }∆f k } 2 ,</formula><p>where again we use α k ď 1 2µ h to simplify the terms. ■ A.6. Proof of Lemma 5</p><p>By the update rule in (8), ω k`1 ´ω‹ pθ k`1 q " ω k`1 ´ω‹ pθ k`1 q " pω k ´ω‹ pθ k qq ´βk g k ´pω ‹ pθ k`1 q ´ω‹ pθ k qq " pω k ´ω‹ pθ k qq ´βk Gpθ k , ω k q ´βk ∆g k ´pω ‹ pθ k`1 q ´ω‹ pθ k qq .</p><p>Taking the norm yields</p><formula xml:id="formula_90" coords="24,106.52,156.73,416.22,115.87">y k`1 " }ω k`1 ´ω‹ pθ k`1 q} 2 " }ω k ´ω‹ pθ k q ´βk Gpθ k , ω k q} 2 loooooooooooooooooomoooooooooooooooooon T 1 `β2 k }∆g k } 2 `}ω ‹ pθ k`1 q ´ω‹ pθ k q} 2 loooooooooooomoooooooooooon T 2 ´2 pω k ´ω‹ pθ k q ´βk Gpθ k , ω k qq J pω ‹ pθ k`1 q ´ω‹ pθ k qq looooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooon T 3 ´2β k pω k ´ω‹ pθ k q ´βk Gpθ k , ω k qq J ∆g k loooooooooooooooooooooooooomoooooooooooooooooooooooooon T 4 `2β k ∆g J k pω ‹ pθ k`1 q ´ω‹ pθ k qq loooooooooooooooooomoooooooooooooooooon T 5 . (36)</formula><p>We bound each term of (36) individually. First, since by definition Gpθ, ω ‹ pθqq " 0 for all θ, we have</p><formula xml:id="formula_91" coords="24,134.12,320.07,343.27,68.21">T 1 " }ω k ´ω‹ pθ k q} 2 ´2β k pω k ´ω‹ pθ k qq J Gpθ k , ω k q `β2 k }Gpθ k , ω k q} 2 " y k ´2β k pω k ´ω‹ pθ k qq J Gpθ k , ω k q `β2 k }Gpθ k , ω k q ´Gpθ k , ω ‹ pθ k qq} 2 ď y k ´2µ G β k }ω k ´ω‹ pθ k q} 2 `L2 β 2 k }ω k ´ω‹ pθ k q} 2 " p1 ´2µ G β k `L2 β 2 k qy k ,</formula><p>where the first inequality follows from Assumption 3 and the Lipschitz continuity of G.</p><p>The term T 2 can be simply treated using the Lipschitz condition of ω ‹</p><formula xml:id="formula_92" coords="24,102.22,431.92,407.06,35.92">T 2 " }ω ‹ pθ k`1 q ´ω‹ pθ k q} 2 ď L 2 }θ k`1 ´θk } 2 ď L 2 α 2 k p}∆f k } `L? y k `LpL `1q ? z k q 2 ď 3L 2 α 2 k `}∆f k } 2 `L2 y k `pL `1q 4 z k ˘.</formula><p>By the Cauchy-Schwarz inequality,</p><formula xml:id="formula_93" coords="24,91.33,504.38,429.35,78.08">T 3 ď 2}ω k ´ω‹ pθ k q ´βk Gpθ k , ω k q}}ω ‹ pθ k`1 q ´ω‹ pθ k q} ď 2}ω k ´ω‹ pθ k q}}ω ‹ pθ k`1 q ´ω‹ pθ k q} `2β k }Gpθ k , ω k q ´Gpθ k , ω ‹ pθ k qq}}ω ‹ pθ k`1 q´ω ‹ pθ k q} ď p2L `2L 2 β k q ? y k }θ k`1 ´θk } ď 4Lα k ? y k p}∆f k } `L? y k `LpL `1q ? z k q .</formula><p>where the second inequality follows from Gpθ, ω ‹ pθqq " 0 for all θ, and the fourth inequality applies Lemma 1 and the step size rule β k ď 1{L.</p><p>We can bound T 4 following a similar line of analysis</p><formula xml:id="formula_94" coords="24,162.54,640.81,286.92,17.06">T 4 ď 2β k }ω k ´ω‹ pθ k q ´βk Gpθ k , ω k q}}∆g k } ď 4β k ? y k }∆g k }.</formula><p>Finally, we treat T 5 again by the Cauchy-Schwarz inequality</p><formula xml:id="formula_95" coords="24,180.92,693.58,165.41,12.60">T 5 ď 2β k }∆g k }}ω ‹ pθ k`1 q ´ω‹ pθ k q} ď 2Lα k β k }∆g k } ´}∆f k } `L? y k `LpL `1q ? z k ¯,</formula><p>where the second inequality again uses Lemma 1.</p><p>Applying the bounds on T 1 -T 5 to (36), we get</p><formula xml:id="formula_96" coords="25,93.50,159.12,713.17,186.14">y k`1 ď p1 ´2µ G β k `L2 β 2 k qy k `β2 k }∆g k } 2 `3L 2 α 2 k `}∆f k } 2 `L2 y k `pL `1q 4 z k 4Lα k ? y k p}∆f k } `L? y k `LpL `1q ? z k q `4β k ? y k }∆g k } `2Lα k β k }∆g k } ´}∆f k } `L? y k `LpL `1q ? z k ď p1 ´2µ G β k `L2 β 2 k qy k `β2 k }∆g k } 2 `3L 2 α 2 k `}∆f k } 2 `L2 y k `pL `1q 4 z k 2Lα k y k `2Lα k }∆f k } 2 `4L 2 α k y k `µG 4 β k y k `16pL `1q 6 α 2 k µ G β k z k `µG β k 2 y k `8β k µ G }∆g k } 2 `Lα k β k }∆g k } 2 `3Lα k β k `}∆f k } 2 `L2 y k `pL `1q 4 z k ď p1 ´µG β k qy k ´´µ G 4 β k ´8L 4 α k ´L2 β 2 k ¯yk `ˆ19pL `1q 6 α 2 k µ G β k `3pL `1q 6 α k β k ˙zk `ˆ8β k µ G `2Lβ 2 k ˙}∆g k } 2 `8Lα k }∆f k } 2 ,</formula><p>where the second inequality follows from the fact that 2}a}}b} ď c}a} 2 `1 c }b} 2 for any vector a, b and scalar c ą 0. We have also simplified terms using the conditions α k ď β k ď 1 72pL`1q 6 and β k ď 1{µ G . ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Analysis for Convex Functions</head><p>Our analysis in this section relies on Lemma 2, 3, and 5, which we have established for the proof of Theorem 1. Note that these lemmas do not require the function h to be strongly convex and therefore can be applied here as well. We also introduce the following additional lemma on the iteration-wise drift of z k . The proof of this lemma can be found at the end of this section.</p><p>Lemma 6 Under Assumptions made in Theorem 2 and the step sizes in (15), we have</p><formula xml:id="formula_97" coords="25,123.77,544.01,364.46,27.78">z k`1 ď z k `p16 `4L 2 µ G q α 2 k z k β k ´2α k ´hpθ k q ´hpθ ‹ q ¯`3α 2 k }∆f k } 2 `3L 2 α 2 k y k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Proof of Theorem 2</head><p>We consider the Lyapunov function</p><formula xml:id="formula_98" coords="25,220.75,629.19,170.50,13.27">V k fi Er}∆f k } 2 `}∆g k } 2 `zk `yk s.</formula><p>Combining the results from Lemma 2, 3, 5, and 6, and simplifying the terms with the choice of step sizes given in (15),</p><formula xml:id="formula_99" coords="25,91.47,695.41,21.80,10.77">V k`1 " Er}∆f k`1 } 2 `}∆g k`1 } 2 `zk`1 `yk`1 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ `2Bλ 2 k `p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `6pL `1q 6 α 2 k z k ‰ `2Bλ 2 k `E" z k `p16 `4L 2 µ G q α 2 k z k β k ´2α k ´hpθ k q ´hpθ ‹ q ¯`3α 2 k }∆f k } 2 `3L 2 α 2 k y k `p1 ´µG β k qy k ´´µ G 4 β k ´8L 4 α k ´L2 β 2 k ¯yk `ˆ19pL `1q 6 α 2 k µ G β k `3pL `1q 6 α k β k ˙zk `ˆ8β k µ G `2Lβ 2 k ˙}∆g k } 2 `8Lα k }∆f k } 2 ı ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´48L 2 β 2 k λ k ´11Lα k qEr}∆f k } 2 s `p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´p50L 2 `8 µ G qβ k qEr}∆g k } 2 s `p1 `p16 `23pL `1q 6 µ G q α 2 k β k `12pL `1q 6 α 2 k qErz k s ´2α k Erhpθ k q ´hpθ ‹ qs `p1 ´µG β k qEry k s ´ˆµ G 2 β k ´11L 4 α k ´L2 β 2 k ´80L 4 β 2 k λ k ˙Ery k s `4Bλ 2 k ď p1 ´λk qEr}∆f k } 2 s`p1 ´λk qEr}∆g k } 2 s`p1 `p16 `23pL `1q 6 µ G q α 2 k β k `12pL`1q 6 α 2 k qErz k s `p1 ´µG β k qEry k s ´2α k Erhpθ k q ´hpθ ‹ qs `4Bλ 2 k ď p1 `p16 `23pL `1q 6 µ G q α 2 k β k `12pL `1q 6 α 2 k qV k ´2α k Erhpθ k q ´hpθ ‹ qs `4Bλ 2 k ,</formula><p>where we use the step size conditions</p><formula xml:id="formula_100" coords="26,90.00,513.09,433.36,30.50">α k ď 1 44L 4 λ k , β k ď λ k 4 mintp11L`48L 2 q ´1, p50L 2 `8 µ G q ´1u, β k ď µ G 640L 4 λ k , and β k ď µ G 16L 2 .</formula><p>Re-arranging the inequality, we have for all k " 0, 1, ¨¨¨, K ´1</p><formula xml:id="formula_101" coords="26,90.00,562.32,414.65,146.48">2α k Erhpθ k q ´hpθ ‹ qs ď p1 `p16 `23pL `1q 6 µ G q α 2 k β k `12pL `1q 6 α 2 k qV k ´Vk`1 `4Bλ 2 k ď p1 `1 K `1 qV k ´Vk`1 `Bλ 2 0 4pK `1q , since c α and c β are chosen such that p16 `23pL`1q 6 µ G q c 2 α c β `12pL `1q 6 c 2 α ď 1. Defining w k " 1 p1`1 K`1 q k , we have 2c α ? K `1 K ÿ k"0 w k Erhpθ k q ´hpθ ‹ qs ď K ÿ k"0 ˆp1 `1 K `1 qw k V k ´wk V k`1 `Bλ 2 0 4pK `1q w k ď V 0 `Bλ 2 0 4pK `1q K ÿ k"0 w k . (<label>37</label></formula><formula xml:id="formula_102" coords="27,518.19,103.39,4.54,9.46">)</formula><p>The sum of the normalizing factor can be bounded as</p><formula xml:id="formula_103" coords="27,144.33,159.66,373.86,32.89">K ÿ k"0 w k " K ÿ k"0 1 p1 `1 K`1 q k ě 2c α pK `1qp1 `1 K `1 q K ě 4c α pK `1q, (<label>38</label></formula><formula xml:id="formula_104" coords="27,518.19,171.10,4.54,9.46">)</formula><p>where the last inequality follows from p1 `1 K`1 q K ě 2 for any K ě 1.</p><p>Combining ( <ref type="formula" coords="27,163.31,220.22,9.09,9.46" target="#formula_101">37</ref>) and ( <ref type="formula" coords="27,202.33,220.22,8.36,9.46" target="#formula_103">38</ref>), we have</p><formula xml:id="formula_105" coords="27,128.06,238.91,529.29,69.47">1 ř K k 1 "0 w k 1 K ÿ k"0 w k Erhpθ k q ´hpθ ‹ qs ď ? K `1 2c α ř K k 1 "0 w k 1 ˜V0 `Bλ 2 0 4pK `1q K ÿ k"0 w k ḑ V 0 8c 2 α ? K `1 `Bλ 2 0 8c α ? K `1 . (<label>39</label></formula><formula xml:id="formula_106" coords="27,106.94,288.75,415.79,53.77">) Let s θ K " ř K k"0 w k θ k ř K k 1 "0 w k 1</formula><p>. Since the function h is convex, the claimed result easily follows from (39).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Proof of Lemma 6</head><p>From the update rule (8), we have</p><formula xml:id="formula_107" coords="27,129.26,412.14,388.92,141.73">z k`1 " }θ k`1 ´θ‹ } 2 " }θ k ´αk f k ´θ‹ } 2 " }θ k ´θ‹ } 2 `α2 k }f k } 2 ´2α k xθ k ´θ‹ , f k y " }θ k ´θ‹ } 2 `α2 k }f k } 2 ´2α k xθ k ´θ‹ , ∆f k y ´2α k xθ k ´θ‹ , F pθ k , ω k qy " z k `α2 k }f k } 2 ´2α k xθ k ´θ‹ , ∆f k y ´2α k xθ k ´θ‹ , F pθ k , ω ‹ pθ k qqy ´2α k xθ k ´θ‹ , F pθ k , ω k q ´F pθ k , ω ‹ pθ k qqy ď z k ´2α k ´hpθ k q ´hpθ ‹ q ¯`α 2 k }f k } 2 ´2α k xθ k ´θ‹ , ∆f k y ´2α k xθ k ´θ‹ , F pθ k , ω k q ´F pθ k , ω ‹ pθ k qqy, (<label>40</label></formula><formula xml:id="formula_108" coords="27,518.19,543.44,4.54,9.46">)</formula><p>where the last inequality follows from the convexity of h, i.e. xθ k ´θ‹ , ∇hpθ k qy ě hpθ k q ´hpθ ‹ q. To bound the fourth term on the right hand side of (40),</p><formula xml:id="formula_109" coords="27,150.73,603.12,367.46,27.78">´2α k xθ k ´θ‹ , ∆f k y ď 2α k }θ k ´θ‹ }}∆f k } ď 16α 2 k λ k z k `λk 16 }∆f k } 2 , (<label>41</label></formula><formula xml:id="formula_110" coords="27,518.19,612.99,4.54,9.46">)</formula><p>where the final inequality follows from the fact that 2}a}}b} ď c}a} 2 `1 c }b} 2 for any vector a, b and scalar c ą 0.</p><p>The fifth term on the right hand side of ( <ref type="formula" coords="27,283.89,671.03,9.09,9.46" target="#formula_107">40</ref>) can be treated in a similar manner</p><formula xml:id="formula_111" coords="27,107.07,693.58,397.87,12.60">´2α k xθ k ´θ‹ , F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy ď 2α k }θ k ´θ‹ }}F pθ k , ω ‹ pθ k qq ´F pθ k , ω k q} ď 2Lα k }θ k ´θ‹ }}ω ‹ pθ k q ´ωk } ď 4L 2 α 2 k µ G β k z k `µG 4 β k y k . (<label>42</label></formula><formula xml:id="formula_112" coords="28,518.19,119.44,4.54,9.46">)</formula><p>Plugging ( <ref type="formula" coords="28,153.61,147.77,9.09,9.46" target="#formula_109">41</ref>) and ( <ref type="formula" coords="28,192.99,147.77,9.09,9.46" target="#formula_111">42</ref>) into (40), we get</p><formula xml:id="formula_113" coords="28,90.00,168.56,606.22,173.63">z k`1 ď z k ´2α k ´hpθ k q ´hpθ ‹ q ¯`α 2 k }f k } 2 ´2α k xθ k ´θ‹ , ∆f k y ´2α k xθ k ´θ‹ , F pθ k , ω k q ´F pθ k , ω ‹ pθ k qqy ď z k ´2α k ´hpθ k q ´hpθ ‹ q ¯`3α 2 k `}∆f k } 2 `L2 y k `pL `1q 4 z k 16α 2 k λ k z k `λk 16 }∆f k } 2 `4L 2 α 2 k µ G β k z k `µG 4 β k y k ď z k `p16 `4L 2 µ G q α 2 k z k β k ´2α k ´hpθ k q ´hpθ ‹ q ¯`3α 2 k }∆f k } 2 `3L 2 α 2 k y k . ■ Appendix C.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis for Functions Satisfying Polyak-Lojasiewicz Condition</head><p>The exact requirement on step sizes is</p><formula xml:id="formula_114" coords="28,179.68,371.38,343.06,25.24">λ k " c λ k `τ `1 , α k " c α k `τ `1 , β k " c β k `τ `1 ,<label>(43)</label></formula><p>where c α , c β , c λ need to be carefully balanced and τ selected sufficiently large such that α k ď</p><formula xml:id="formula_115" coords="28,90.00,417.61,828.70,68.33">β k ď λ k ď 1{4, c β ď 1{L, c β ď µ G 320L 4 c λ , c β ď µ 3 h 480pL`1q 6 , c β ď µ G 8L 2 , β k ď λ k 4 mintp 2L 2 µ 3 h 58L 2 q ´1, p50L 2 `8 µ G q ´1u, α k ď µ G β k 4 ˆ14L 3 `132pL`1q 6 µ 3 h ˙, α k ď µ 2 h 3072pL`1q 6 λ k , α k ď β k , and c α ě 2 mintµ h ,µ G u .</formula><p>The PL condition implies the following quadratic growth inequality, where Proj Θ ‹ pθq denotes the projection of θ to Θ ‹ <ref type="bibr" coords="28,200.15,501.00,87.41,9.46" target="#b20">(Karimi et al., 2016)</ref>.</p><formula xml:id="formula_116" coords="28,225.87,521.38,292.32,25.64">2 µ h phpθq ´h‹ q ě }Proj Θ ‹ pθq ´θ} 2 . (<label>44</label></formula><formula xml:id="formula_117" coords="28,518.19,529.11,4.54,9.46">)</formula><p>Equation ( <ref type="formula" coords="28,136.66,557.44,9.09,9.46" target="#formula_116">44</ref>) plays an important role in the proof of a lemma that we now introduce.</p><p>Lemma 7 Under Assumption 1-3 and Assumption 5, the sequence of variables tθ k , ω k , f k , g k u satisfy for all k</p><formula xml:id="formula_118" coords="28,176.44,618.18,259.11,88.00">}f k } ď }∆f k } `L? y k `2LpL `1q µ h ? x k , }g k } ď }∆g k } `L? y k , }θ k`1 ´θk } ď α k ˆ}∆f k } `L? y k `2LpL `1q µ h ? x k ˙, }ω k`1 ´ωk } ď β k p}∆g k } `L? y k q .</formula><p>Our analysis in this section also relies on a few more lemmas which we present below. The proofs of all lemmas can be found at the end of the section.</p><p>Lemma 8 Under Assumption 1-3 and Assumption 5 and the step sizes given in (43), we have the following bound on }∆f</p><formula xml:id="formula_119" coords="29,116.92,144.62,378.16,80.88">k`1 } 2 Er}∆f k`1 } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k ȷ `2Bλ 2 k .</formula><p>Lemma 9 Under Assumption 1-3 and Assumption 5 and the step sizes given in (43), we have the following bound on</p><formula xml:id="formula_120" coords="29,116.92,249.26,378.16,80.88">}∆g k`1 } 2 Er}∆g k`1 } 2 s ď p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k ȷ `2Bλ 2 k .</formula><p>Lemma 10 Under Assumption 1-3 and Assumption 5 and the step sizes given in (43), we have</p><formula xml:id="formula_121" coords="29,175.34,363.23,261.32,61.22">x k`1 ď p1 ´µh α k qx k ´p µ h 4 α k ´6pL `1q 5 µ 2 h α 2 k qx k `p 4L 4 µ 3 h `2Lqα k y k `p 2L 2 α k µ 3 h `2Lα 2 k q}∆f k } 2 .</formula><p>Lemma 11 Under Assumption 1-3 and Assumption 5 and the step sizes given in (43), we have</p><formula xml:id="formula_122" coords="29,108.18,456.45,395.63,61.22">y k`1 ď p1 ´µG β k qy k ´ˆµ G 2 β k ´ˆ12L 3 `128pL `1q 6 µ 3 h ˙αk ´L2 β 2 k ˙yk `ˆµ h 8 α k `24pL `1q 6 µ 2 h α k β k ˙xk `ˆ8β k µ G `2Lβ 2 k ˙}∆g k } 2 `8Lα k }∆f k } 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Proof of Theorem 3</head><p>Combining the results from Lemma 8-11 and simplifying the terms with the choice of step sizes given in (43),</p><formula xml:id="formula_123" coords="29,93.79,588.48,316.05,121.41">ErV k`1 s " Er}∆f k`1 } 2 `}∆g k`1 } 2 `xk`1 `yk`1 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k ȷ `2Bλ 2 k `p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k ȷ `2Bλ 2 k `E" p1´µ h α k qx k ´p µ h 4 α k ´6pL`1q 5 µ 2 h α 2 k qx k `p 4L 4 µ 3 h `2Lqα k y k `p 2L 2 α k µ 3 h `2Lα 2 k q}∆f k } 2 `p1 ´µG β k qy k ´ˆµ G 2 β k ´ˆ12L 3 `128pL `1q 6 µ 3 h ˙αk ´L2 β 2 k ˙yk `ˆµ h 8 α k `24pL `1q 6 µ 2 h α k β k ˙xk `ˆ8β k µ G `2Lβ 2 k ˙}∆g k } 2 `8Lα k }∆f k } 2 ı ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´48L 2 β 2 k λ k ´p 2L 2 µ 3 h `10Lqα k qEr}∆f k } 2 s `p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´p50L 2 `8 µ G qβ k qEr}∆g k } 2 s `8 λ k E " 10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k ȷ `4Bλ 2 k `p1 ´µh α k qErx k s ´p µ h 4 α k ´6pL `1q 5 µ 2 h α 2 k qErx k s `p 4L 4 µ 3 h `2Lqα k Ery k s `p1 ´µG β k qEry k s ´ˆµ G 2 β k ´ˆ12L 3 `128pL `1q 6 µ 3 h ˙αk ´L2 β 2 k ˙Ery k s `ˆµ h 8 α k `24pL `1q 6 µ 2 h α k β k ˙Erx k s ď p1 ´λk qEr}∆f k } 2 s`p1 ´λk qEr}∆g k } 2 s `p1 ´µh α k qErx k s`p1 ´µG β k qEry k s`4Bλ 2 k ´p λ k 4 ´p 2L 2 µ 3 h `58L 2 qβ k qEr}∆f k } 2 s ´p λ k 4 ´p50L 2 `8 µ G qβ k qEr}∆g k } 2 s ´p µ h 16 α k ´30pL`1q 6 µ 2 h α k β k qErx k s ´ˆµ G 4 β k ´ˆ14L 3 `132pL`1q 6 µ 3 h ˙αk ´L2 β 2 k ˙Ery k s,</formula><p>where we use the step size conditions</p><formula xml:id="formula_124" coords="30,91.20,485.86,430.80,69.28">β k ď µ G 320L 4 λ k and α k ď µ 2 h 3072pL`1q 6 λ k . Again, due to carefully selected step sizes, (specifically, β k ď µ 3 h 480pL`1q 6 , β k ď µ G 8L 2 , β k ď λ k 4 mintp 2L 2 µ 3 h `58L 2 q ´1, p50L 2 `8 µ G q ´1u, and α k ď µ G β k 4 ˆ14L 3 `132pL`1q 6 µ 3 h ˙)</formula><p>, we can make the last four terms of the inequality above non-positive. As a result, we have</p><formula xml:id="formula_125" coords="30,96.09,573.51,419.12,134.83">ErV k`1 s ď p1 ´λk qEr}∆f k } 2 s `p1 ´λk qEr}∆g k } 2 s `p1 ´µh α k qErx k s `p1 ´µG β k qEry k s `4Bλ 2 k ď p1 ´mintµ h , µ G uα k qErV k s `4Bλ 2 k " p1 ´mintµ h , µ G uc α k `τ `1 qErV k s `B 4pk `1q 2 ď p1 ´2 k `τ `1 qErV k s `B 4pk `1q 2 ď k `τ ´1 k `τ `1 ErV k s `B 4pk `1q 2 .</formula><p>Multiplying by pk `τ `1q 2 on both sides of the inequality, we have</p><formula xml:id="formula_126" coords="31,145.40,117.27,375.58,87.88">pk `τ `1q 2 ErV k`1 s ď pk `τ `1qpk `τ ´1qErV k s `Bpk `τ `1q 2 4pk `1q 2 ď pk `τ q 2 ErV k s `ˆB 2 `Bτ 2 2 ď τ 2 V 0 `ˆB 2 `Bτ 2 2 ˙pk `1q.</formula><p>As a result,</p><formula xml:id="formula_127" coords="31,214.07,241.00,182.16,57.66">ErV k`1 s ď Bτ 2 pk `1q pk `τ `1q 2 `τ 2 V 0 pk `τ `1q 2 ď Bτ 2 k `τ `1 `τ 2 V 0 pk `τ `1q 2 ,</formula><p>which obviously leads to the claimed result. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Proof of Lemma 7</head><p>By definition, F pθ, ω ‹ pθqq " ∇hpθq " 0, @θ P Θ ‹ which implies</p><formula xml:id="formula_128" coords="31,94.64,453.50,422.72,90.38">}f k } " }∆f k `F pθ k , ω k q ´F pθ k , ω ‹ pθ k qq `F pθ k , ω ‹ pθ k qq ´F pProj Θ ‹ pθ k q, ω ‹ pProj Θ ‹ pθ k qqq} ď }∆f k } `}F pθ k , ω k q ´F pθ k , ω ‹ pθ k qq} `}F pθ k , ω ‹ pθ k qq ´F pProj Θ ‹ pθ k q, ω ‹ pProj Θ ‹ pθ k qqq} ď }∆f k } `L}ω k ´ω‹ pθ k q} `LpL `1q}θ k ´Proj Θ ‹ pθ k q} ď }∆f k } `L? y k `2LpL `1q µ h ? x k ,</formula><p>where the second inequality follows from the Lipschitz continuity of F and ω ‹ and the final inequality applies (44). Similarly, since Gpθ, ω ‹ pθqq " 0 for any θ, we can derive</p><formula xml:id="formula_129" coords="31,205.88,607.11,200.24,45.68">}g k } " }∆g k `Gpθ k , ω k q ´Gpθ k , ω ‹ pθqq} ď }∆g k } `}Gpθ k , ω k q ´Gpθ k , ω ‹ pθqq} ď }∆g k } `L? y k .</formula><p>The bounds on }θ k`1 ´θk } and }ω k`1 ´ωk } easily follow from the two inequalities above and (9). ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZENG DOAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Proof of Lemma 8</head><p>By the update rule in (9), we have</p><formula xml:id="formula_130" coords="32,106.07,136.09,399.87,60.39">∆f k`1 " f k`1 ´F pθ k`1 , ω k`1 q " p1 ´λk qf k `λk F pθ k , ω k , X k q ´F pθ k`1 , ω k`1 q " p1 ´λk qf k `λk F pθ k , ω k q ´F pθ k`1 , ω k`1 q `λk pF pθ k , ω k q ´F pθ k , ω k , X k qq " p1 ´λk q∆f k `F pθ k , ω k q ´F pθ k`1 , ω k`1 q `λk pF pθ k , ω k q ´F pθ k , ω k , X k qq .</formula><p>This leads to</p><formula xml:id="formula_131" coords="32,100.27,228.02,422.46,203.83">}∆f k`1 } 2 " p1 ´λk q 2 }∆f k } 2 `› › › pF pθ k , ω k q ´F pθ k`1 , ω k`1 qq `λk pF pθ k , ω k q ´F pθ k , ω k , X k qq › › › 2 `2p1 ´λk q∆f J k pF pθ k , ω k q ´F pθ k`1 , ω k`1 qq `2p1 ´λk qλ k ∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq ď p1 ´λk q 2 }∆f k } 2 `2}F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 `2λ 2 k }F pθ k , ω k q ´F pθ k , ω k , X k q} 2 `λk 2 }∆f k } 2 `2 λ k }F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq ď p1 ´λk q}∆f k } 2 `2λ 2 k }F pθ k , ω k q ´F pθ k , ω k , X k q} 2 `pλ 2 k ´λk 2 q}∆f k } 2 `4 λ k }F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq ,<label>(45)</label></formula><p>where the last inequality follows from λ k ď 1.</p><p>The second term on the right hand side of ( <ref type="formula" coords="32,296.00,456.39,9.09,9.46" target="#formula_131">45</ref>) is bounded in expectation, i.e.</p><formula xml:id="formula_132" coords="32,207.29,475.95,197.41,14.29">E X k "ξ r}F pθ k , ω k , X k q ´F pθ k , ω k q} 2 s ď B.</formula><p>In addition, the last term of ( <ref type="formula" coords="32,216.67,501.20,9.09,9.46" target="#formula_131">45</ref>) is zero in expectation since</p><formula xml:id="formula_133" coords="32,90.00,499.57,432.00,87.10">Er∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq | H k´1 s " ∆f J k ErpF pθ k , ω k q ´F pθ k , ω k , X k qq | H k´1 s " 0. As a result, we have Er}∆f k`1 } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 2 ´λ2 k qEr}∆f k } 2 s `4 λ k Er}F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 s `2Bλ 2 k . (<label>46</label></formula><formula xml:id="formula_134" coords="32,518.19,568.76,4.54,9.46">)</formula><p>By Lemma 7 and the Lipschitz condition of F ,</p><formula xml:id="formula_135" coords="32,148.94,614.90,314.11,94.65">}F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 ď 2L 2 }θ k`1 ´θk } 2 `2L 2 }ω k`1 ´ωk } 2 ď 6L 2 α 2 k p}∆f k } 2 `L2 y k `4pL `1q 4 µ 2 h x k q `4L 2 β 2 k p}∆g k } 2 `L2 y k q ď 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k .</formula><p>Plugging this bound into (46), we get</p><formula xml:id="formula_136" coords="33,100.08,111.87,411.83,114.32">Er}∆f k`1 } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 2 ´λ2 k qEr}∆f k } 2 s `2Bλ 2 k `4 λ k E " 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k ȷ ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k ȷ `2Bλ 2 k ,</formula><p>where the last inequality follows from λ k ď 1{4. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Proof of Lemma 9</head><p>By the update rule in (9), we have</p><formula xml:id="formula_137" coords="33,106.11,314.25,399.78,60.39">∆g k`1 " g k`1 ´Gpθ k`1 , ω k`1 q " p1 ´λk qg k `λk Gpθ k , ω k , X k q ´Gpθ k`1 , ω k`1 q " p1 ´λk qg k `λk Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q `λk pGpθ k , ω k q ´Gpθ k , ω k , X k qq " p1 ´λk q∆g k `Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q `λk pGpθ k , ω k q ´Gpθ k , ω k , X k qq .</formula><p>This leads to</p><formula xml:id="formula_138" coords="33,100.25,401.60,417.94,203.83">}∆g k`1 } 2 " p1 ´λk q 2 }∆g k } 2 `› › › pGpθ k , ω k q ´Gpθ k`1 , ω k`1 qq `λk pGpθ k , ω k q ´Gpθ k , ω k , X k qq › › › 2 `2p1 ´λk q∆g J k pGpθ k , ω k q ´Gpθ k`1 , ω k`1 qq `2p1 ´λk qλ k ∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq ď p1 ´λk q 2 }∆g k } 2 `2}Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 `2λ 2 k }Gpθ k , ω k q ´Gpθ k , ω k , X k q} 2 `λk 2 }∆g k } 2 `2 λ k }Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq ď p1 ´λk q}∆g k } 2 `2λ 2 k }Gpθ k , ω k q ´Gpθ k , ω k , X k q} 2 `pλ 2 k ´λk 2 q}∆g k } 2 `4 λ k }Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq . (<label>47</label></formula><formula xml:id="formula_139" coords="33,518.19,594.01,4.54,9.46">)</formula><p>The second term on the right hand side of ( <ref type="formula" coords="33,296.00,614.13,9.09,9.46" target="#formula_138">47</ref>) is bounded in expectation, i.e.</p><formula xml:id="formula_140" coords="33,207.25,631.40,197.51,14.29">E X k "ξ r}Gpθ k , ω k , X k q ´Gpθ k , ω k q} 2 s ď B.</formula><p>In addition, the last term of ( <ref type="formula" coords="33,217.08,654.36,9.09,9.46" target="#formula_131">45</ref>) is zero in expectation since</p><formula xml:id="formula_141" coords="33,90.00,652.73,432.00,27.15">Er∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq | H k´1 s " ∆g J k ErpGpθ k , ω k q ´Gpθ k , ω k , X k qq | H k´1 s " 0.</formula><p>As a result, we have</p><formula xml:id="formula_142" coords="33,156.71,686.03,266.86,24.43">Er}∆g k`1 } 2 s ď p1 ´λk qEr}∆g k } 2 s ´p λ k 2 ´λ2 k qEr}∆g k } 2 s `4 λ k Er}Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 s `2Bλ 2 k .<label>(48)</label></formula><p>By Lemma 7 and the Lipschitz condition of G,</p><formula xml:id="formula_143" coords="34,148.94,146.74,314.11,94.65">}Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 ď 2L 2 }θ k`1 ´θk } 2 `2L 2 }ω k`1 ´ωk } 2 ď 6L 2 α 2 k p}∆f k } 2 `L2 y k `4pL `1q 4 µ 2 h x k q `4L 2 β 2 k p}∆g k } 2 `L2 y k q ď 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k .</formula><p>Plugging this bound into (48), we get</p><formula xml:id="formula_144" coords="34,109.72,272.34,391.84,132.44">Er}∆g k`1 } 2 s ď p1 ´λk qEr}∆g k } 2 s ´p λ k 2 ´λ2 k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k ȷ `2Bλ 2 k ď p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `24pL `1q 6 µ h α 2 k x k ȷ `2Bλ 2 k ,</formula><p>where the last inequality follows from λ k ď 1{4. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Proof of Lemma 10</head><p>From the update rule (8) and the Lipschitz gradient condition in Assumption 1, we have</p><formula xml:id="formula_145" coords="34,133.30,502.59,438.36,207.30">hpθ k`1 q ď hpθ k q `x∇hpθ k q, θ k`1 ´θk y `L 2 }θ k`1 ´θk } 2 " hpθ k q ´αk x∇hpθ k q, f k y `Lα 2 k 2 }f k } 2 " hpθ k q ´αk x∇hpθ k q, ∆f k y ´αk x∇hpθ k q, F pθ k , ω k qy `Lα 2 k 2 }f k } 2 ď hpθ k q ´αk x∇hpθ k q, ∆f k y ´αk x∇hpθ k q, F pθ k , ω ‹ pθ k qqy `αk x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy `3Lα 2 k 2 ˆ}∆f k } 2 `Ly k `4pL `1q 4 µ 2 h x k " hpθ k q ´αk }∇hpθ k q} 2 ´αk x∇hpθ k q, ∆f k y `αk x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy `3Lα 2 k 2 ˆ}∆f k } 2 `Ly k `4pL `1q 4 µ 2 h x k ˙,</formula><p>where the second inequality applies Lemma 7. As a result of Assumption 5, the inequality implies</p><formula xml:id="formula_146" coords="35,90.54,130.44,811.95,174.56">x x`1 " hpθ k`1 q ´hpθ ‹ q ď hpθ k q ´hpθ ‹ q ´αk }∇hpθ k q} 2 ´αk x∇hpθ k q, ∆f k y `αk x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy `3Lα 2 k 2 ˆ}∆f k } 2 `Ly k `4pL `1q 4 µ 2 h x k ď hpθ k q ´hpθ ‹ q ´2µ h α k phpθ k q ´hpθ ‹ qq ´αk x∇hpθ k q, ∆f k y `αk x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy `3Lα 2 k 2 ˆ}∆f k } 2 `Ly k `4pL `1q 4 µ 2 h x k ď p1 ´2µ h α k qx k ´αk x∇hpθ k q, ∆f k y `αk x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy `3Lα 2 k 2 ˆ}∆f k } 2 `Ly k `4pL `1q 4 µ 2 h x k ˙.<label>(49)</label></formula><p>To bound the second term on the right hand side of (49),</p><formula xml:id="formula_147" coords="35,170.01,342.78,348.17,106.24">´αk x∇hpθ k q, ∆f k y ď α k }∇hpθ k q}}∆f k } " α k }∇hpθ k q ´∇hpProj Θ ‹ pθ k qq}}∆f k } ď Lα k }θ k ´Proj Θ ‹ pθ k q}}∆f k } ď 2Lα k µ h ? x k }∆f k } ď µ h 2 α k x k `2L 2 α k µ 3 h }∆f k } 2 , (<label>50</label></formula><formula xml:id="formula_148" coords="35,518.19,429.16,4.54,9.46">)</formula><p>where the third inequality plugs in the quadratic growth inequality (44) and the final inequality follows from the fact that 2}a}}b} ď c}a} 2 `1 c }b} 2 for any vector a, b and scalar c ą 0.</p><p>The third term on the right hand side of (49) can be treated in a similar manner</p><formula xml:id="formula_149" coords="35,116.39,508.83,406.34,105.05">α k x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy ď α k }∇hpθ k q}}F pθ k , ω ‹ pθ k qq ´F pθ k , ω k q} ď 2Lα k µ h ? x k ¨pL}ω ‹ pθ k q ´ωk }q " 2L 2 α k µ h ? x k ? y k ď µ h 4 α k x k `4L 4 α k µ 3 h y k .<label>(51)</label></formula><p>Plugging ( <ref type="formula" coords="35,153.61,624.42,9.09,9.46" target="#formula_147">50</ref>) and ( <ref type="formula" coords="35,192.99,624.42,9.09,9.46" target="#formula_149">51</ref>) into (49), we get</p><formula xml:id="formula_150" coords="35,91.63,648.42,399.86,61.47">x k`1 ď p1 ´2µ h α k qx k ´αk x∇hpθ k q, ∆f k y `αk x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy `3Lα 2 k 2 ˆ}∆f k } 2 `Ly k `4pL `1q 4 µ 2 h x k ď p1 ´2µ h α k qx k `µh 2 α k x k `2L 2 α k µ 3 h }∆f k } 2 `µh 4 α k x k `4L 4 α k µ 3 h y k `3Lα 2 k 2 ˆ}∆f k } 2 `Ly k `4pL `1q 4 µ 2 h x k ď p1 ´µh α k qx k ´p µ h 4 α k ´6pL `1q 5 µ 2 h α 2 k qx k `p 4L 4 µ 3 h `2Lqα k y k `p 2L 2 α k µ 3 h `2Lα 2 k q}∆f k } 2 . ■ C.6. Proof of Lemma 11</formula><p>By the update rule in (8),</p><formula xml:id="formula_151" coords="36,115.39,260.10,169.58,10.77">ω k`1 ´ω‹ pθ k`1 q " ω k`1 ´ω‹ pθ k`1 q</formula><p>" pω k ´ω‹ pθ k qq ´βk g k ´pω ‹ pθ k`1 q ´ω‹ pθ k qq " pω k ´ω‹ pθ k qq ´βk Gpθ k , ω k q ´βk ∆g k ´pω ‹ pθ k`1 q ´ω‹ pθ k qq .</p><p>Taking the norm yields</p><formula xml:id="formula_152" coords="36,106.52,334.48,416.22,115.87">y k`1 " }ω k`1 ´ω‹ pθ k`1 q} 2 " }ω k ´ω‹ pθ k q ´βk Gpθ k , ω k q} 2 loooooooooooooooooomoooooooooooooooooon T 1 `β2 k }∆g k } 2 `}ω ‹ pθ k`1 q ´ω‹ pθ k q} 2 loooooooooooomoooooooooooon T 2 ´2 pω k ´ω‹ pθ k q ´βk Gpθ k , ω k qq J pω ‹ pθ k`1 q ´ω‹ pθ k qq looooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooon T 3 ´2β k pω k ´ω‹ pθ k q ´βk Gpθ k , ω k qq J ∆g k loooooooooooooooooooooooooomoooooooooooooooooooooooooon T 4 `2β k ∆g J k pω ‹ pθ k`1 q ´ω‹ pθ k qq loooooooooooooooooomoooooooooooooooooon T 5 . (52)</formula><p>We bound each term of (52) individually. First, since by definition Gpθ, ω ‹ pθqq " 0 for all θ, we have</p><formula xml:id="formula_153" coords="36,134.12,493.32,343.27,68.22">T 1 " }ω k ´ω‹ pθ k q} 2 ´2β k pω k ´ω‹ pθ k qq J Gpθ k , ω k q `β2 k }Gpθ k , ω k q} 2 " y k ´2β k pω k ´ω‹ pθ k qq J Gpθ k , ω k q `β2 k }Gpθ k , ω k q ´Gpθ k , ω ‹ pθ k qq} 2 ď y k ´2µ G β k }ω k ´ω‹ pθ k q} 2 `L2 β 2 k }ω k ´ω‹ pθ k q} 2 " p1 ´2µ G β k `L2 β 2 k qy k ,</formula><p>where the first inequality follows from Assumption 3 and the Lipschitz continuity of G.</p><p>The term T 2 can be simply treated using the Lipschitz condition of ω</p><formula xml:id="formula_154" coords="36,93.93,583.93,423.64,81.83">‹ T 2 " }ω ‹ pθ k`1 q ´ω‹ pθ k q} 2 ď L 2 }θ k`1 ´θk } 2 ď L 2 α 2 k ˆ}∆f k } `L? y k `2LpL `1q µ h ? x k ˙2 ď 3L 2 α 2 k ˆ}∆f k } 2 `L2 y k `4pL `1q 4 µ 2 h x k ˙.</formula><p>By the Cauchy-Schwarz inequality,</p><formula xml:id="formula_155" coords="36,93.32,695.41,10.61,10.63">T 3 ď 2}ω k ´ω‹ pθ k q ´βk Gpθ k , ω k q}}ω ‹ pθ k`1 q ´ω‹ pθ k q} ď 2}ω k ´ω‹ pθ k q}}ω ‹ pθ k`1 q´ω ‹ pθ k q} `2β k }Gpθ k , ω k q ´Gpθ k , ω ‹ pθ k qq}}ω ‹ pθ k`1 q ´ω‹ pθ k q} ď p2L `2L 2 β k q ? y k }θ k`1 ´θk } ď 4Lα k ? y k ˆ}∆f k } `L? y k `2LpL `1q µ h ? x k ˙.</formula><p>where the second inequality follows from Gpθ, ω ‹ pθqq " 0 for all θ, and the fourth inequality applies Lemma 7 and the step size rule β k ď 1{L.</p><p>We can bound T 4 following a similar line of analysis</p><formula xml:id="formula_156" coords="37,162.54,220.47,286.92,17.06">T 4 ď 2β k }ω k ´ω‹ pθ k q ´βk Gpθ k , ω k q}}∆g k } ď 4β k ? y k }∆g k }.</formula><p>Finally, we treat T 5 again by the Cauchy-Schwarz inequality</p><formula xml:id="formula_157" coords="37,176.42,266.12,259.17,43.70">T 5 ď 2β k }∆g k }}ω ‹ pθ k`1 q ´ω‹ pθ k q} ď 2Lα k β k }∆g k } ´}∆f k } `L? y k `2LpL `1q µ h ? x k ¯,</formula><p>where the second inequality again uses Lemma 7.</p><p>Applying the bounds on T 1 -T 5 to (52), we get</p><formula xml:id="formula_158" coords="37,100.23,347.70,737.24,247.25">y k`1 ď p1 ´2µ G β k `L2 β 2 k qy k `β2 k }∆g k } 2 `3L 2 α 2 k ˆ}∆f k } 2 `L2 y k `4pL `1q 4 µ 2 h x k 4Lα k ? y k ˆ}∆f k } `L? y k `2LpL `1q µ h ? x k ˙`4β k ? y k }∆g k } `2Lα k β k }∆g k } ´}∆f k } `L? y k `2LpL `1q µ h ? x k ď p1 ´2µ G β k `L2 β 2 k qy k `β2 k }∆g k } 2 `3L 2 α 2 k ˆ}∆f k } 2 `L2 y k `4pL `1q 4 µ 2 h x k 2Lα k y k `2Lα k }∆f k } 2 `4L 2 α k y k `128pL `1q 6 α k µ 3 h y k `µh α k 8 x k `µG β k 2 y k `8β k µ G }∆g k } 2 `Lα k β k }∆g k } 2 `3Lα k β k ˆ}∆f k } 2 `L2 y k `4pL `1q 4 µ 2 h x k ď p1 ´µG β k qy k ´ˆµ G 2 β k ´ˆ12L 3 `128pL `1q 6 µ 3 h ˙αk ´L2 β 2 k ˙yk `ˆµ h 8 α k `24pL `1q 6 µ 2 h α k β k ˙xk `ˆ8β k µ G `2Lβ 2 k ˙}∆g k } 2 `8Lα k }∆f k } 2</formula><p>where the second inequality follows from the fact that 2}a}}b} ď c}a} 2 `1 c }b} 2 for any vector a, b and scalar c ą 0. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Analysis for Non-Convex Functions</head><p>Our analysis in this section relies on the following lemmas, the proofs of which can be found at the end of the section.</p><p>Lemma 12 Under Assumption 1-3, the sequence of variables tθ k , ω k , f k , g k u satisfy for all k</p><formula xml:id="formula_159" coords="38,193.84,119.37,224.32,60.39">}f k } ď }∆f k } `L? y k `}∇hpθ k q}, }g k } ď }∆g k } `L? y k , }θ k`1 ´θk } ď α k p}∆f k } `L? y k `}∇hpθ k q}q , }ω k`1 ´ωk } ď β k p}∆g k } `L? y k q .</formula><p>Lemma 13 Under Assumption 1-3 and the step sizes given in (16), we have the following bound on</p><formula xml:id="formula_160" coords="38,90.00,206.90,404.35,79.71">}∆f k`1 } 2 Er}∆f k`1 } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 ‰ `2Bλ 2 k .</formula><p>Lemma 14 Under Assumption 1-3 and the step sizes given in (16), we have the following bound on</p><formula xml:id="formula_161" coords="38,90.00,310.49,404.35,79.71">}∆g k`1 } 2 Er}∆g k`1 } 2 s ď p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 ‰ `2Bλ 2 k .</formula><p>Lemma 15 Under Assumption 1-3 and the step sizes in (16), we have</p><formula xml:id="formula_162" coords="38,167.65,424.31,276.70,26.38">hpθ k`1 q ď hpθ k q ´αk 4 }∇hpθ k q} 2 `5α k 4 }∆f k } 2 `5L 2 α k 4 y k .</formula><p>Lemma 16 Under Assumption 1-3 and the step sizes in (16), we have</p><formula xml:id="formula_163" coords="38,149.54,484.53,312.93,50.55">y k`1 ď p1 ´µG β k qy k ´´µ G 2 β k ´108L 4 α k ´L2 β 2 k ¯yk `αk 8 }∇hpθ k q} 2 `ˆ8β k µ G `2Lβ 2 k ˙}∆g k } 2 `8Lα k }∆f k } 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Proof of Theorem 4</head><p>Combining the results from Lemma 13-16 and simplifying the terms with the choice of step sizes given in ( <ref type="formula" coords="38,131.93,584.69,8.36,9.46" target="#formula_25">16</ref>), we have</p><formula xml:id="formula_164" coords="38,119.19,606.84,314.59,103.05">Er}∆f k`1 } 2 `}∆g k`1 } 2 `hpθ k`1 q `yk`1 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 ‰ `2Bλ 2 k `p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 ‰ `2Bλ 2 k `E" hpθ k q ´αk 4 }∇hpθ k q} 2 `5α k 4 }∆f k } 2 `5L 2 α k 4 y k `p1 ´µG β k qy k ´´µ G 2 β k ´108L 4 α k ´L2 β 2 k ¯yk `αk 8 }∇hpθ k q} 2 `ˆ8β k µ G `2Lβ 2 k ˙}∆g k } 2 `8Lα k }∆f k } 2 ı ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´48L 2 β 2 k λ k ´37L 4 α k qEr}∆f k } 2 s `p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´p50L 2 `8 µ G qβ k qEr}∆g k } 2 s `Erhpθ k qs ´p α k 8 ´48L 2 α 2 k λ k qEr}∇hpθ k q} 2 s `p1 ´µG β k qEry k s ´ˆµ G 2 β k ´110L 4 α k ´L2 β 2 k ´80L 4 β 2 k λ k ˙Ery k s `4Bλ 2 k ď p1 ´λk qEr}∆f k } 2 s `p1 ´λk qEr}∆g k } 2 s `Erhpθ k qs ´αk 16 Er}∇hpθ k q} 2 s `p1 ´µG β k qEry k s `4Bλ 2 k ,</formula><p>where we use the step size conditions</p><formula xml:id="formula_165" coords="39,91.20,366.50,430.80,83.20">β k ď λ k 4 mintp 37L 4 `48L 2 q ´1, p50L 2 `8 µ G q ´1u, α k ď 1 768L 2 λ k , β k ď µ G 320L 4 λ k , β k ď µ G 8L 2 , and α k ď µ G 880L 4 β k . This inequality obviously implies α k Er}∇hpθ k q} 2 s ď 16Er}∆f k } 2 ´}∆f k`1 } 2 s `16Er}∆g k } 2 ´}∆g k`1 } 2 s `16Erhpθ k q ´hpθ k`1 qs `16Ery k ´yk`1 s `64Bλ 2 k .</formula><p>Since }∆f k } 2 , }∆g k } 2 , and y k are non-negative and hpθ k q ě hpθ ‹ q, we have the following inequality by summing up over k</p><formula xml:id="formula_166" coords="39,106.94,493.27,385.05,216.16">k ÿ t"0 α t Er}∇hpθ t q} 2 s ď 16 `}∆f 0 } 2 `}∆g 0 } 2 `phpθ 0 q ´hpθ ‹ qq `y0 ˘`64B k ÿ t"0 λ 2 t It is a standard result that (see, for example, Lemma 3 of Zeng et al. (2024)) k ÿ t"0 α t " α 0 k ÿ t"0 1 pt `1q 1{2 ě α 0 pk `1q 1{2 2 , k ÿ t"0 λ 2 t " λ 2 0 k ÿ t"0 1 t `1 ď 2λ 2 0 logpk `2q Finally, min tďk E " }∇hpθ t q} 2 ‰ ď 1 ř k t 1 "0 α t 1 k ÿ t"0 α t E " }∇hpθ t q} 2 ‰ ď 32 α 0 pk `1q 1{2 `}∆f 0 } 2 `}∆g 0 } 2 `phpθ 0 q ´hpθ ‹ qq `y0 ˘`4 logpk `2q α 0 pk `1q 1{2 . ■ D.2. Proof of Lemma 12</formula><p>By the update rule,</p><formula xml:id="formula_167" coords="40,160.80,200.52,290.39,78.75">}f k } " }∆f k `F pθ k , ω k q ´F pθ k , ω ‹ pθ k qq `F pθ k , ω ‹ pθ k qq} ď }∆f k } `}F pθ k , ω k q ´F pθ k , ω ‹ pθ k qq} `}F pθ k , ω ‹ pθ k qq} " }∆f k } `}F pθ k , ω k q ´F pθ k , ω ‹ pθ k qq} `}∇hpθ k q} ď }∆f k } `L}ω k ´ω‹ pθ k q} `}∇hpθ k q} ď }∆f k } `L? y k `}∇hpθ k q},</formula><p>where the second inequality follows from the Lipschitz continuity of F . Similarly, since Gpθ, ω ‹ pθqq " 0 for any θ, we can derive</p><formula xml:id="formula_168" coords="40,205.88,329.99,200.24,45.68">}g k } " }∆g k `Gpθ k , ω k q ´Gpθ k , ω ‹ pθqq} ď }∆g k } `}Gpθ k , ω k q ´Gpθ k , ω ‹ pθqq} ď }∆g k } `L? y k .</formula><p>The bounds on }θ k`1 ´θk } and }ω k`1 ´ωk } easily follow from the two inequalities above and (9). ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Proof of Lemma 13</head><p>By the update rule in (9), we have</p><formula xml:id="formula_169" coords="40,89.66,489.97,419.26,154.69">∆f k`1 " f k`1 ´F pθ k`1 , ω k`1 q " p1 ´λk qf k `λk F pθ k , ω k , X k q ´F pθ k`1 , ω k`1 q " p1 ´λk qf k `λk F pθ k , ω k q ´F pθ k`1 , ω k`1 q `λk pF pθ k , ω k q ´F pθ k , ω k , X k qq " p1 ´λk q∆f k `F pθ k , ω k q ´F pθ k`1 , ω k`1 q `λk pF pθ k , ω k q ´F pθ k , ω k , X k qq . This leads to }∆f k`1 } 2 " p1 ´λk q 2 }∆f k } 2 `› › › pF pθ k , ω k q ´F pθ k`1 , ω k`1 qq `λk pF pθ k , ω k q ´F pθ k , ω k , X k qq › › › 2 `2p1 ´λk q∆f J k pF pθ k , ω k q ´F pθ k`1</formula><p>, ω k`1 qq `2p1 ´λk qλ k ∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq ď p1 ´λk q 2 }∆f k } 2 `2}F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 `2λ 2 k }F pθ k , ω k q ´F pθ k , ω k , X k q} 2 `λk 2 }∆f k } 2 `2 λ k }F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq ď p1 ´λk q}∆f k } 2 `2λ 2 k }F pθ k , ω k q ´F pθ k , ω k , X k q} 2 `pλ 2 k ´λk 2 q}∆f k } 2 `4 λ k }F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq ,</p><p>where the last inequality follows from λ k ď 1.</p><p>The second term on the right hand side of ( <ref type="formula" coords="41,296.00,197.10,9.09,9.46" target="#formula_170">53</ref>) is bounded in expectation, i.e. E X k "ξ r}F pθ k , ω k , X k q ´F pθ k , ω k q} 2 s ď B.</p><p>In addition, the last term of ( <ref type="formula" coords="41,216.67,246.69,9.09,9.46" target="#formula_170">53</ref>) is zero in expectation since Er∆f J k pF pθ k , ω k q ´F pθ k , ω k , X k qq | H k´1 s " ∆f J k ErpF pθ k , ω k q ´F pθ k , ω k , X k qq | H k´1 s " 0. As a result, we have</p><formula xml:id="formula_171" coords="41,156.69,283.03,361.50,51.52">Er}∆f k`1 } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 2 ´λ2 k qEr}∆f k } 2 s `4 λ k Er}F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 s `2Bλ 2 k . (<label>54</label></formula><formula xml:id="formula_172" coords="41,518.19,316.64,4.54,9.46">)</formula><p>By Lemma 12 and the Lipschitz condition of F ,</p><formula xml:id="formula_173" coords="41,153.50,367.65,305.00,67.34">}F pθ k , ω k q ´F pθ k`1 , ω k`1 q} 2 ď 2L 2 }θ k`1 ´θk } 2 `2L 2 }ω k`1 ´ωk } 2 ď 6L 2 α 2 k p}∆f k } 2 `L2 y k `}∇hpθ k q} 2 q `4L 2 β 2 k p}∆g k } 2 `L2 y k q ď 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 .</formula><p>Plugging this bound into (54), we get</p><formula xml:id="formula_174" coords="41,110.45,470.31,390.39,127.21">Er}∆f k`1 } 2 s ď p1 ´λk qEr}∆f k } 2 s ´p λ k 2 ´λ2 k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 ‰ `2Bλ 2 k ď p1 ´λk qEr}∆f k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆f k } 2 s `4 λ k E " 6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 ‰ `2Bλ 2 k ,</formula><p>where the last inequality follows from λ k ď 1{4. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Proof of Lemma 14</head><p>By the update rule in (9), we have ∆g k`1 " g k`1 ´Gpθ k`1 , ω k`1 q " p1 ´λk qg k `λk Gpθ k , ω k , X k q ´Gpθ k`1 , ω k`1 q " p1 ´λk qg k `λk Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q `λk pGpθ k , ω k q ´Gpθ k , ω k , X k qq " p1 ´λk q∆g k `Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q `λk pGpθ k , ω k q ´Gpθ k , ω k , X k qq .</p><p>This leads to }∆g k`1 } 2 " p1 ´λk q 2 }∆g k } 2 `› › › pGpθ k , ω k q ´Gpθ k`1 , ω k`1 qq `λk pGpθ k , ω k q ´Gpθ k , ω k , X k qq › › › 2 `2p1 ´λk q∆g J k pGpθ k , ω k q ´Gpθ k`1 , ω k`1 qq `2p1 ´λk qλ k ∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq ď p1 ´λk q 2 }∆g k } 2 `2}Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 `2λ 2 k }Gpθ k , ω k q ´Gpθ k , ω k , X k q} 2 `λk</p><formula xml:id="formula_175" coords="42,103.28,276.94,419.45,101.96">2 }∆g k } 2 `2 λ k }Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq ď p1 ´λk q}∆g k } 2 `2λ 2 k }Gpθ k , ω k q ´Gpθ k , ω k , X k q} 2 `pλ 2 k ´λk 2 q}∆g k } 2 `4 λ k }Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 `2p1 ´λk qλ k ∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq .<label>(55)</label></formula><p>The second term on the right hand side of ( <ref type="formula" coords="42,296.00,392.73,9.09,9.46" target="#formula_175">55</ref>) is bounded in expectation, i.e. E X k "ξ r}Gpθ k , ω k , X k q ´Gpθ k , ω k q} 2 s ď B.</p><p>In addition, the last term of ( <ref type="formula" coords="42,217.08,442.86,9.09,9.46" target="#formula_170">53</ref>) is zero in expectation since Er∆g J k pGpθ k , ω k q ´Gpθ k , ω k , X k qq | H k´1 s " ∆g J k ErpGpθ k , ω k q ´Gpθ k , ω k , X k qq | H k´1 s " 0. As a result, we have</p><formula xml:id="formula_176" coords="42,156.71,479.47,366.02,51.52">Er}∆g k`1 } 2 s ď p1 ´λk qEr}∆g k } 2 s ´p λ k 2 ´λ2 k qEr}∆g k } 2 s `4 λ k Er}Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 s `2Bλ 2 k .<label>(56)</label></formula><p>By Lemma 12 and the Lipschitz condition of G,</p><formula xml:id="formula_177" coords="42,153.50,564.71,305.00,67.34">}Gpθ k , ω k q ´Gpθ k`1 , ω k`1 q} 2 ď 2L 2 }θ k`1 ´θk } 2 `2L 2 }ω k`1 ´ωk } 2 ď 6L 2 α 2 k p}∆f k } 2 `L2 y k `}∇hpθ k q} 2 q `4L 2 β 2 k p}∆g k } 2 `L2 y k q ď 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 .</formula><p>Plugging this bound into (56), we get</p><formula xml:id="formula_178" coords="42,110.45,667.90,208.63,42.55">Er}∆g k`1 } 2 s ď p1 ´λk qEr}∆g k } 2 s ´p λ k 2 ´λ2 k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `6L 2 β 2 k }∆g k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 ‰ `2Bλ 2 k ď p1 ´λk qEr}∆g k } 2 s ´p λ k 4 ´24L 2 β 2 k λ k qEr}∆g k } 2 s `4 λ k E " 6L 2 β 2 k }∆f k } 2 `10L 4 β 2 k y k `6L 2 α 2 k }∇hpθ k q} 2 ‰ `2Bλ 2 k ,</formula><p>where the last inequality follows from λ k ď 1{4. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Proof of Lemma 15</head><p>From the update rule (8) and the Lipschitz gradient condition in Assumption 1, we have hpθ k`1 q ď hpθ k q `x∇hpθ k q, θ k`1 ´θk y `L 2 }θ k`1 ´θk } 2</p><p>" hpθ k q ´αk x∇hpθ k q, f k y `Lα 2 k 2 }f k } 2</p><p>" hpθ k q ´αk x∇hpθ k q, ∆f k y ´αk x∇hpθ k q, F pθ k , ω ‹ pθ k qqy `αk x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy `Lα 2 k 2 }f k } 2</p><p>" hpθ k q ´αk }∇hpθ k q} 2 ´αk x∇hpθ k q, ∆f k y `αk x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy `Lα</p><formula xml:id="formula_179" coords="43,420.93,389.58,97.25,26.58">2 k 2 }f k } 2 . (<label>57</label></formula><formula xml:id="formula_180" coords="43,518.19,399.45,4.54,9.46">)</formula><p>To bound the second term on the right hand side of (57),</p><formula xml:id="formula_181" coords="43,143.29,447.16,374.90,24.43">´αk x∇hpθ k q, ∆f k y ď α k }∇hpθ k q}}∆f k } ď α k 4 }∇hpθ k q} 2 `αk }∆f k } 2 , (<label>58</label></formula><formula xml:id="formula_182" coords="43,518.19,454.89,4.54,9.46">)</formula><p>where the final inequality follows from the fact that 2}a}}b} ď c}a} 2 `1 c }b} 2 for any vector a, b and scalar c ą 0.</p><p>The third term on the right hand side of (57) can be treated in a similar manner</p><formula xml:id="formula_183" coords="43,116.39,535.40,406.34,55.54">α k x∇hpθ k q, F pθ k , ω ‹ pθ k qq ´F pθ k , ω k qy ď α k }∇hpθ k q}}F pθ k , ω ‹ pθ k qq ´F pθ k , ω k q} ď Lα k }∇hpθ k q}}ω ‹ pθ k q ´ωk } ď α k 4 }∇hpθ k q} 2 `L2 α k y k .<label>(59)</label></formula><p>Plugging ( <ref type="formula" coords="43,153.61,602.34,9.09,9.46" target="#formula_181">58</ref>) and ( <ref type="formula" coords="43,192.99,602.34,9.09,9.46" target="#formula_183">59</ref>) into (57), we get hpθ k`1 q ď hpθ k q ´αk }∇hpθ k q} 2 ´αk x∇hpθ k q, ∆f k y `αk x∇hpθ k q, F pθ k , ω By the update rule in (8), ω k`1 ´ω‹ pθ k`1 q " ω k`1 ´ω‹ pθ k`1 q " pω k ´ω‹ pθ k qq ´βk g k ´pω ‹ pθ k`1 q ´ω‹ pθ k qq " pω k ´ω‹ pθ k qq ´βk Gpθ k , ω k q ´βk ∆g k ´pω ‹ pθ k`1 q ´ω‹ pθ k qq .</p><p>Taking the norm yields y k`1 " }ω k`1 ´ω‹ pθ k`1 q} 2 " }ω k ´ω‹ pθ k q ´βk Gpθ k , ω k q} 2 loooooooooooooooooomoooooooooooooooooon T 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>`β2</head><p>k }∆g k } 2 `}ω ‹ pθ k`1 q ´ω‹ pθ k q} 2 loooooooooooomoooooooooooon T 2 ´2 pω k ´ω‹ pθ k q ´βk Gpθ k , ω k qq J pω ‹ pθ k`1 q ´ω‹ pθ k qq looooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooon We bound each term of (60) individually. First, since by definition Gpθ, ω ‹ pθqq " 0 for all θ, we have</p><formula xml:id="formula_184" coords="44,134.12,511.39,343.27,68.22">T 1 " }ω k ´ω‹ pθ k q} 2 ´2β k pω k ´ω‹ pθ k qq J Gpθ k , ω k q `β2 k }Gpθ k , ω k q} 2 " y k ´2β k pω k ´ω‹ pθ k qq J Gpθ k , ω k q `β2 k }Gpθ k , ω k q ´Gpθ k , ω ‹ pθ k qq} 2 ď y k ´2µ G β k }ω k ´ω‹ pθ k q} 2 `L2 β 2 k }ω k ´ω‹ pθ k q} 2 " p1 ´2µ G β k `L2 β 2 k qy k ,</formula><p>where the first inequality follows from Assumption 3 and the Lipschitz continuity of G. The term T 2 can be simply treated using the Lipschitz condition of ω ‹ T 2 " }ω ‹ pθ k`1 q ´ω‹ pθ k q} 2 ď L 2 }θ k`1 ´θk } 2 ď L 2 α 2 k p}∆f k } `L?</p><formula xml:id="formula_185" coords="44,125.46,626.53,374.72,32.48">y k `}∇hpθ k q}q 2 ď 3L 2 α 2 k `}∆f k } 2 `L2 y k `}∇hpθ k q} 2 ˘.</formula><p>By the Cauchy-Schwarz inequality, T 3</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,138.01,196.26,149.28,9.46;12,153.98,209.81,117.14,9.46;12,126.15,91.03,172.80,92.88"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Fast TDC Algorithm for Random Policy Evaluation</figDesc><graphic coords="12,126.15,91.03,172.80,92.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,410.72,542.36,111.28,9.46;12,90.00,555.91,433.73,9.46;12,90.00,569.80,432.00,9.88;12,90.00,583.78,334.78,9.46;12,143.36,625.69,19.69,10.18;12,166.09,607.51,8.18,3.05;12,166.09,626.82,8.18,3.05;12,166.09,633.69,8.18,3.05;12,181.98,611.04,46.02,9.57;12,244.93,611.04,5.45,9.57;12,179.25,625.95,78.11,9.57;12,186.22,640.85,5.45,9.57;12,208.61,640.85,46.03,9.57;12,262.34,607.51,8.18,3.05;12,262.34,626.82,8.18,3.05;12,262.34,633.69,8.18,3.05;12,272.34,625.69,36.09,10.18;12,311.46,607.51,8.18,3.05;12,311.46,626.82,8.18,3.05;12,311.46,633.69,8.18,3.05"><head></head><label></label><figDesc>numerically confirms the superiority of the proposed algorithm. Details of the experimental setup can be found in Appendix E. Policy optimization for LQR. We experiment with the small-dimensional environment considered inZeng et al. (2024)  where the transition and costs matrices are chosen to be A "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="44,270.62,422.09,8.58,7.71;44,149.55,436.78,160.09,10.77;44,309.63,434.43,6.21,6.77;44,318.16,436.78,18.70,10.77;44,149.55,447.60,188.03,3.05;44,239.02,456.29,8.58,7.71;44,339.39,436.78,41.34,10.77;44,381.12,434.95,6.21,6.77;44,380.73,437.88,19.95,10.67;44,401.07,434.95,4.23,6.77;44,405.80,437.88,77.75,9.67;44,349.69,447.65,133.87,3.05;44,412.08,456.34,8.58,7.71;44,485.37,436.78,3.03,9.57;44,504.56,437.13,18.17,9.46"><head>5 .</head><label>5</label><figDesc>k ´ω‹ pθ k q ´βk Gpθ k , ω k qq J ∆g k loooooooooooooooooooooooooomoooooooooooooooooooooooooon T 4 `2β k ∆g J k pω ‹ pθ k`1 q ´ω‹ pθ k qq loooooooooooooooooomoooooooooooooooooon T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,98.12,291.18,410.31,106.63"><head>Table 1 :</head><label>1</label><figDesc>Summary of finite-time complexity under various structural assumptions on h. s θ k is a weighted average of history iterates defined in Theorem 2.</figDesc><table coords="3,98.12,324.42,410.31,73.39"><row><cell>Structural property</cell><cell cols="2">Metric</cell><cell>Complexity in this paper</cell><cell>Order of step sizes</cell><cell>Best known complexity of standard two-time-scale SA</cell></row><row><cell>Strong Convexity</cell><cell>Er}θ k</cell><cell>´θ‹ } 2 s</cell><cell>Opk ´1q</cell><cell>k ´1</cell><cell>Opk ´2 3 q</cell></row><row><cell>Convexity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="15,90.00,108.07,433.91,597.15"><head></head><label></label><figDesc>Annals of Applied Probability, 14(2):796-819, 2004. Vijaymohan R Konda and Vivek S Borkar. Actor-critic-type learning algorithms for markov decision processes. SIAM Journal on control and Optimization, 38(1):94-123, 1999. Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two time-scale actor-critic methods. Advances in Neural Information Processing Systems, 33:17617-17628, 2020. Huaqing Xiong, Tengyu Xu, Yingbin Liang, and Wei Zhang. Non-asymptotic convergence of adam-type reinforcement learning algorithms under markovian sampling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10460-10468, 2021. Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. Provably global convergence of actor-critic: A case for linear quadratic regulator with ergodic cost. Advances in neural information processing systems, 32, 2019. Sihan Zeng, Thinh T Doan, and Justin Romberg. Finite-time convergence rates of decentralized stochastic approximation with applications in multi-agent and multi-task learning. IEEE Transactions on Automatic Control, 2022. Sihan Zeng, Thinh T Doan, and Justin Romberg. A two-time-scale stochastic optimization framework with applications in control and reinforcement learning. SIAM Journal on Optimization, 34(1): 946-976, 2024. Mo Zhou and Jianfeng Lu. Single timescale actor-critic method to solve the linear quadratic regulator with convergence guarantees. Journal of Machine Learning Research, 24(222):1-34, 2023.</figDesc><table coords="15,90.00,166.15,433.91,489.71"><row><cell>Guanghui Lan. First-order and stochastic optimization methods for machine learning, volume 1.</cell></row><row><cell>Springer, 2020.</cell></row><row><cell>Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Richard S</cell></row><row><cell>Sutton. Convergent temporal-difference learning with arbitrary smooth function approximation.</cell></row><row><cell>Advances in neural information processing systems, 22, 2009.</cell></row><row><cell>Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence</cell></row><row><cell>rates of softmax policy gradient methods. In International Conference on Machine Learning,</cell></row><row><cell>pages 6820-6829. PMLR, 2020.</cell></row><row><cell>Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba</cell></row><row><cell>Szepesvári, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning</cell></row><row><cell>with linear function approximation. In Proceedings of the 26th Annual International Conference</cell></row><row><cell>on Machine Learning, pages 993-1000, 2009.</cell></row><row><cell>Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms</cell></row><row><cell>for minimizing compositions of expected-value functions. Mathematical Programming, 161(1):</cell></row><row><cell>419-449, 2017.</cell></row></table><note>A. Mokkadem and M. Pelletier. Convergence rate and averaging of nonlinear two-time-scale stochastic approximation algorithms. The Annals of Applied Probability, 16(3):1671-1702, 2006. Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1571-1578, 2012. Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International conference on machine learning, pages 71-79. PMLR, 2013. Han Shen and Tianyi Chen. A single-timescale analysis for stochastic approximation with multiple coupled sequences. Advances in Neural Information Processing Systems, 35:17415-17429, 2022. Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and TD learning. In Conference on Learning Theory, pages 2803-2830. PMLR, 2019. Richard S Sutton, Csaba Szepesvári, and Hamid Reza Maei. A convergent o (n) algorithm for off-policy temporal-difference learning with linear function approximation. Advances in neural information processing systems, 21(21):1609-1616, 2008. Yue Wang, Shaofeng Zou, and Yi Zhou. Non-asymptotic analysis for two time-scale tdc with general smooth function approximation. Advances in Neural Information Processing Systems, 34: 9747-9758, 2021. Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function approximation. Advances in neural information processing systems, 32, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="43,106.41,660.33,319.66,50.12"><head></head><label></label><figDesc>‹ pθ k qq ´F pθ k , ω k qy `Lα 2 k 2 }f k } 2 ď hpθ k q ´αk }∇hpθ k q} 2 `αk 4 }∇hpθ k q} 2 `αk }∆f k } 2 `αk 4 }∇hpθ k q} 2 `L2 α k y k `Lα 2 k 2 }f k } 2 ď hpθ k q ´αk 2 }∇hpθ k q} 2 `αk }∆f k } 2 `L2 α k y k `3Lα 2 k 2 `}∆f k } 2 `L2 y k `}∇hpθ k q} 2 ď hpθ k q ´αk 4 }∇hpθ k q} 2 `5α k 4 }∆f k } 2 `5L 2 α k 4 y k ,where the third inequality plugs in the result of Lemma 12, and the final inequality uses α k ď 1 6L . ■ D.6. Proof of Lemma 16</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the National Science Foundation under awards ECCS-CAREER-2339509 and CCF-2343599.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ď 2}ω k ´ω‹ pθ k q ´βk Gpθ k , ω k q}}ω ‹ pθ k`1 q ´ω‹ pθ k q} ď 2}ω k ´ω‹ pθ k q}}ω ‹ pθ k`1 q ´ω‹ pθ k q} `2β k }Gpθ k , ω k q ´Gpθ k , ω ‹ pθ k qq}}ω ‹ pθ k`1 q ´ω‹ pθ k q} ď p2L `2L 2 β k q ? y k }θ k`1 ´θk } ď 4Lα k ? y k p}∆f k } `L? y k `}∇hpθ k q}q .</p><p>where the second inequality follows from Gpθ, ω ‹ pθqq " 0 for all θ, and the fourth inequality applies Lemma 12 and the step size rule β k ď 1{L.</p><p>We can bound T 4 following a similar line of analysis</p><p>Finally, we treat T 5 again by the Cauchy-Schwarz inequality</p><p>where the second inequality again uses Lemma 12.</p><p>Applying the bounds on T 1 -T 5 to (60), we get</p><p>where the second inequality follows from the fact that 2}a}}b} ď c}a} 2 `1 c }b} 2 for any vector a, b and scalar c ą 0. We have also simplified terms using the conditions α k ď 1 72L 2 and β k ď 1 72L . ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Simulations Details</head><p>We generate completely random MDPs with transition probability matrix P P R |S|ˆ|A|ˆ|S| drawn i.i.d. from Unifp0, 1q and then normalized such that ÿ s 1 PS Pps 1 | s, aq " 1, @s, a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZENG DOAN</head><p>The behavior policy π b is chosen to be uniform for all states. The policy π to be evaluated is generated randomly by first drawing ψ P R |S|ˆ|A| such that ψ s,a " N p0, 1q, which produces the policy π through the softmax function πpa | sq " exppψ s,a q ř a 1 exppψ s,a 1 q , @s, a.</p><p>We draw the feature matrix Φ P R |S|ˆdx entry-wise i.i.d. from N p0, 1q. The value function V P R |S| is the solution to the Bellman equation</p><p>where the matrix P π P R |S|ˆ|S| is the state transition matrix under policy π</p><p>We would like to design the experiments such that the value function V P R |S| lies in the span of the feature matrix. To achieve, we determine the optimal parameter θ ‹ first, by sampling it entry-wise i.i.d. from N p0, 1q. This produces the value function V " Φθ, from which we reverse engineer the reward function R " pI ´γP π qV.</p><p>The discount factor γ is 0.5. All algorithms start with zero initialization, i.e. θ 0 " ω 0 " f 0 " g 0 " 0. The step sizes α k and β k are set to constant values 5e ´4 and 2e ´3 for simplicity, while the step size λ k is selected as</p><p>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,90.00,349.26,432.00,9.66;13,100.91,363.00,69.39,9.46" xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic programming and optimal control: Volume I</title>
		<author>
			<persName coords=""><forename type="first">Dimitri</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena scientific</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,90.00,385.77,433.51,9.66;13,100.91,399.52,68.18,9.46" xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic approximation with two time scales</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; Control Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,90.00,422.48,432.00,9.46;13,100.91,435.83,422.45,9.66;13,100.91,449.58,24.55,9.46" xml:id="b2">
	<analytic>
		<title level="a" type="main">Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuejiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="4937" to="4948" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,90.00,472.54,432.01,9.46;13,100.91,485.90,421.09,9.66;13,100.91,499.64,119.09,9.46" xml:id="b3">
	<analytic>
		<title level="a" type="main">A single-timescale method for stochastic bilevel optimization</title>
		<author>
			<persName coords=""><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuejiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022a</date>
			<biblScope unit="page" from="2466" to="2488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,90.00,522.61,432.01,9.46;13,100.91,535.96,423.00,9.66" xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimal algorithms for stochastic bilevel optimization under relaxed smoothness conditions</title>
		<author>
			<persName coords=""><forename type="first">Xuxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tesi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishnakumar</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.12067</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,90.00,558.92,432.00,9.66;13,100.72,572.47,186.47,9.66" xml:id="b5">
	<analytic>
		<title level="a" type="main">Finite-time analysis of single-timescale actor-critic</title>
		<author>
			<persName coords=""><forename type="first">Xuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,90.00,595.63,433.81,9.46;13,100.91,609.18,423.00,9.46;13,100.24,622.54,143.12,9.66" xml:id="b6">
	<analytic>
		<title level="a" type="main">Finitesample analysis of nonlinear stochastic approximation with applications in reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Zaiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John-Paul</forename><surname>Thinh T Doan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siva</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maguluri</forename><surname>Theja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">110623</biblScope>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,90.00,645.50,432.00,9.66;13,100.91,659.05,192.18,9.66" xml:id="b7">
	<analytic>
		<title level="a" type="main">An overview of bilevel optimization</title>
		<author>
			<persName coords=""><forename type="first">Benoît</forename><surname>Colson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrice</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilles</forename><surname>Savard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,90.00,682.21,432.00,9.46;13,100.91,695.56,330.35,9.66" xml:id="b8">
	<analytic>
		<title level="a" type="main">Finite sample analysis of two-timescale stochastic approximation with applications to reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Szörényi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.00,94.72,432.00,9.46;14,100.91,108.07,421.10,9.66;14,100.55,121.82,122.12,9.46" xml:id="b9">
	<analytic>
		<title level="a" type="main">A tale of two-timescale reinforcement learning with the tightest finite-time bound</title>
		<author>
			<persName coords=""><forename type="first">Gal</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Balazs</forename><surname>Szorenyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gugan</forename><surname>Thoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04">Apr. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3701" to="3708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.00,144.81,432.01,9.46;14,100.91,158.16,271.52,9.66" xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonlinear two-time-scale stochastic approximation convergence and finite-time performance</title>
		<author>
			<persName coords=""><surname>Thinh T Doan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.00,181.34,432.01,9.46;14,100.91,194.70,421.09,9.66;14,100.18,208.24,173.31,9.66" xml:id="b11">
	<analytic>
		<title level="a" type="main">Linear two-time-scale stochastic approximation a finite-time analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Thinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Romberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.00,231.43,432.00,9.46;14,100.91,244.78,421.09,9.66;14,100.09,258.53,114.25,9.46" xml:id="b12">
	<analytic>
		<title level="a" type="main">Global convergence of policy gradient methods for the linear quadratic regulator</title>
		<author>
			<persName coords=""><forename type="first">Maryam</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mehran</forename><surname>Mesbahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1467" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.00,281.52,432.38,9.46;14,100.91,294.87,421.09,9.66;14,100.52,308.42,168.18,9.66" xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework</title>
		<author>
			<persName coords=""><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1469" to="1492" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.00,331.60,432.00,9.46;14,100.91,344.96,421.09,9.66;14,100.55,358.51,150.12,9.66" xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning optimal controllers for linear systems with multiplicative noise via policy gradient</title>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Gravell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peyman</forename><surname>Mohajerin Esfahani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tyler</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5283" to="5298" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.00,381.69,432.01,9.46;14,100.91,395.04,421.09,9.66;14,100.57,408.59,131.62,9.66" xml:id="b15">
	<analytic>
		<title level="a" type="main">Finite-time performance bounds and adaptive learning rate selection for two time-scale reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Harsh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rayadurgam</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.00,431.78,433.81,9.46;14,100.91,445.13,315.31,9.66" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yuze</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.03893</idno>
		<title level="m">Finite-time decoupled convergence in nonlinear two-timescale stochastic approximation</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,90.00,468.31,433.82,9.46;14,100.91,481.67,422.45,9.66;14,100.91,495.41,24.55,9.46;14,90.00,518.40,432.18,9.46;14,100.91,531.75,422.61,9.66;14,100.91,545.50,79.09,9.46" xml:id="b17">
	<analytic>
		<title level="a" type="main">Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization</title>
		<author>
			<persName coords=""><forename type="first">Shaan</forename><surname>Ul Haque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sajad</forename><surname>Khodadadian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siva</forename><surname>Theja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maguluri</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2401.00364</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2489" to="2512" />
			<date type="published" when="2014">2023. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Tight finite time bounds of two-timescale linear stochastic approximation with markovian noise</note>
</biblStruct>

<biblStruct coords="14,90.00,568.49,432.00,9.46;14,100.91,581.84,421.09,9.66;14,100.43,595.39,207.30,9.66" xml:id="b18">
	<analytic>
		<title level="a" type="main">A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic</title>
		<author>
			<persName coords=""><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hoi-To</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="180" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,90.00,618.58,432.00,9.46;14,100.91,631.93,368.16,9.66" xml:id="b19">
	<monogr>
		<title level="m" type="main">A model-free first-order method for linear quadratic regulator with o (1/ε) sampling complexity</title>
		<author>
			<persName coords=""><forename type="first">Caleb</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgios</forename><surname>Kotsalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00084</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,90.00,655.11,433.81,9.46;14,100.91,668.46,421.09,9.66;14,100.52,682.01,423.39,9.40;14,100.64,695.56,348.76,9.66" xml:id="b20">
	<analytic>
		<title level="a" type="main">Linear convergence of gradient and proximalgradient methods under the Polyak-łojasiewicz condition</title>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julie</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016</title>
				<meeting><address><addrLine>Riva del Garda, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">September 19-23, 2016. 2016</date>
			<biblScope unit="page" from="795" to="811" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 16</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
